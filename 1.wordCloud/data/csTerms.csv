phrase,sentances
data storage,Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval.
hash functions,"Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. !! Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision). !! Hash functions rely on generating favourable probability distributions for their effectiveness, reducing access time to nearly constant. !! High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. !! Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval."
function interaction,"Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision)."
statistical properties,"Use of hash functions relies on statistical properties of key and function interaction: worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision)."
randomization functions,"Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. !! In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed."
computer science,"It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). !! The field of information visualization has emerged ""from research in humancomputer interaction, computer science, graphics, visual design, psychology, and business methods. !! In computer science, an instruction set architecture (ISA), also called computer architecture, is an abstract model of a computer. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. !! In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set). !! In computer science, a randomized meldable heap (also Meldable Heap or Randomized Meldable Priority Queue) is a priority queue based data structure in which the underlying structure is also a heap-ordered binary tree. !! In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! In computer science, performance prediction means to estimate the execution time or other performance factors (such as cache misses) of a program on a given computer. !! In computer science, partial order reduction is a technique for reducing the size of the state-space to be searched by a model checking or Automated planning and scheduling algorithm. !! In mathematics and computer science, computable analysis is the study of mathematical analysis from the perspective of computability theory. !! However, data science is different from computer science and information science. !! The fundamental concern of computer science is determining what can and cannot be automated. !! In computer science, Prim's algorithm (also known as Jarnk's algorithm) is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. !! In computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. !! In computer science, amortized analysis is a method for analyzing a given algorithm's complexity, or how much of a resource, especially time or memory, it takes to execute. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. !! In computer science, polymorphic recursion (also referred to as MilnerMycroft typability or the MilnerMycroft calculus) refers to a recursive parametrically polymorphic function where the type parameter changes with each recursive invocation made, instead of staying constant. !! In computer science, termination analysis is program analysis which attempts to determine whether the evaluation of a given program halts for each input. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms. !! In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. !! In computer science, an evolving intelligent system is a fuzzy logic system which improves the own performance by evolving rules. !! In computer science, partial sorting is a relaxed variant of the sorting problem. !! These vector operations perform additions on blocks of elements from the arrays a, b and c. Automatic vectorization is a major research topic in computer science. !! Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory. !! In computer science, a loop invariant is a property of a program loop that is true before (and after) each iteration. !! In computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a tag section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to. !! In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. !! Document classification or document categorization is a problem in library science, information science and computer science. !! In computer science, a doubly linked list is a linked data structure that consists of a set of sequentially linked records called nodes. !! In computer science, anonymous recursion is recursion which does not explicitly call a function by name. !! In computer science, a syntax error is an error in the syntax of a sequence of characters or tokens that is intended to be written in a particular programming language. !! State space search is a process used in the field of computer science, including artificial intelligence (AI), in which successive configurations or states of an instance are considered, with the intention of finding a goal state with the desired property. !! In computer science, a radix tree (also radix trie or compact prefix tree) is a data structure that represents a space-optimized trie (prefix tree) in which each node that is the only child is merged with its parent. !! In the field of computer science, a pre-topological order or pre-topological ordering of a directed graph is a linear ordering of its vertices such that if there is a directed path from vertex u to vertex v and v comes before u in the ordering, then there is also a directed path from vertex v to vertex u. !! Algorithmic mechanism design (AMD) lies at the intersection of economic game theory, optimization, and computer science. !! Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In computer science, a search data structure is any data structure that allows the efficient retrieval of specific items from a set of items, such as a specific record from a database. !! Adaptive optimization is a technique in computer science that performs dynamic recompilation of portions of a program based on the current execution profile. !! Researchers at their Computer Science and Artificial Intelligence Laboratory (CSAIL) have developed a new system, called MapLite, which allows self-driving cars to drive on roads that they have never been on before, without using 3D maps. !! WADS, the Algorithms and Data Structures Symposium, is an international academic conference in the field of computer science, focusing on algorithms and data structures. !! In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. !! In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars. !! In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do. !! In computer science, a three-way comparison takes two values A and B belonging to a type with a total order and determines whether A < B, A = B, or A > B in a single operation, in accordance with the mathematical law of trichotomy. !! An AA tree in computer science is a form of balanced tree used for storing and retrieving ordered data efficiently. !! Chaos theory has applications in a variety of disciplines, including meteorology, anthropology, sociology, environmental science, computer science, engineering, economics, ecology, pandemic crisis management. !! In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. !! In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm. !! In computer science, graph traversal (also known as graph search) refers to the process of visiting (checking and/or updating) each vertex in a graph. !! In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence. !! They show how the results obtained with a triangulation of SIM and CEM point at new research avenues not only for semiotic engineering and HCI but also for other areas of computer science such as software engineering and programming. !! In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. !! Computer science is the study of computation, automation, and information. !! In computer science, an access control matrix or access matrix is an abstract, formal security model of protection state in computer systems, that characterizes the rights of each subject with respect to every object in the system. !! In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. !! In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. !! In computer science, the range searching problem consists of processing a set S of objects, in order to determine which objects from S intersect with a query object, called the range. !! In computer science, a linked list is a linear collection of data elements whose order is not given by their physical placement in memory. !! In computer science, software pipelining is a technique used to optimize loops, in a manner that parallels hardware pipelining. !! Craig Gentry, Certificate-Based Encryption and the Certificate Revocation Problem, Lecture Notes in Computer Science, pp. !! :911 The stochastic matrix was first developed by Andrey Markov at the beginning of the 20th century, and has found use throughout a wide variety of scientific fields, including probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. !! In computer science, a linear search or sequential search is a method for finding an element within a list. !! Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! Matthias Braun; Sebastian Buchwald; Sebastian Hack; Roland Leia; Christoph Mallon; Andreas Zwinkau (2013), ""Simple and Efficient Construction of Static Single Assignment Form"", Compiler Construction, Lecture Notes in Computer Science, vol. !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! In computer science, garbage collection (GC) is a form of automatic memory management. !! In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification. !! This article is a list of notable unsolved problems in computer science. !! In mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs. !! In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! In computer science, a lookup table (LUT) is an array that replaces runtime computation with a simpler array indexing operation. !! In mathematics and computer science, symbolic-numeric computation is the use of software that combines symbolic and numeric methods to solve problems. !! Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. !! In mathematics and computer science, trace theory aims to provide a concrete mathematical underpinning for the study of concurrent computation and process calculi. !! In computer science, a sorting algorithm is an algorithm that puts elements of a list into an order. !! In computer science, formal specifications are mathematically based techniques whose purpose are to help with the implementation of systems and software. !! The subset sum problem (SSP) is a decision problem in computer science. !! The languages of this class have great practical importance in computer science as they can be parsed much more efficiently than nondeterministic context-free languages. !! In computer science, an in-place algorithm is an algorithm which transforms input using no auxiliary data structure. !! In computer science, a Fibonacci heap is a data structure for priority queue operations, consisting of a collection of heap-ordered trees. !! In computer science, interactive computing refers to software which accepts input from the user as it runs. !! ACM Transactions on Computational Logic (ACM TOCL) is a scientific journal that aims to disseminate the latest findings of note in the field of logic in computer science. !! In computer science, strictness analysis refers to any algorithm used to prove that a function in a non-strict functional programming language is strict in one or more of its arguments. !! In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). !! A state diagram is a type of diagram used in computer science and related fields to describe the behavior of systems. !! Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! In computer science, the computational complexity or simply complexity of an algorithm is the amount of resources required to run it. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to practical disciplines (including the design and implementation of hardware and software). !! In computer science and computer programming, access level denotes the set of permissions or restrictions provided to a data type. !! In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. !! In computer science, program derivation is the derivation of a program from its specification, by mathematical means. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In computer science, imperialist competitive algorithms are a type of computational method used to solve optimization problems of different types. !! In computer science, storage virtualization is ""the process of presenting a logical view of the physical storage resources to"" a host computer system, ""treating all storage media (hard disk, optical disk, tape, etc. ) !! In mathematics, logic, and computer science, a type theory is a formal system in which every ""term"" has a ""type"". !! In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i. e. , in the order that the input is fed to the algorithm, without having the entire input available from the start. !! Computational statistics, or statistical computing, is the bond between statistics and computer science. !! In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. !! In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems. !! In computer science, array access analysis is a compiler analysis approach used to decide the read and write access patterns to elements or portions of arrays. !! In computer science, the longest common substring problem is to find the longest string that is a substring of two or more strings. !! In computer science, a succinct data structure is a data structure which uses an amount of space that is ""close"" to the information-theoretic lower bound, but (unlike other compressed representations) still allows for efficient query operations. !! The theory of computation can be considered the creation of models of all kinds in the field of computer science. !! In computer science, shadow paging is a technique for providing atomicity and durability (two of the ACID properties) in database systems. !! In computer science, lazy deletion refers to a method of deleting elements from a hash table that uses open addressing. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). !! In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. !! Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory. !! In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node. !! In computer science, merge-insertion sort or the FordJohnson algorithm is a comparison sorting algorithm published in 1959 by L. R. Ford Jr. and Selmer M. Johnson. !! In application of modal logic to computer science, the so-called possible worlds can be understood as representing possible states and the accessibility relation can be understood as a program. !! In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. !! In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. !! In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching). !! In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Dervi Karaboa (Erciyes University) in 2005. !! In computer science, radix sort is a non-comparative sorting algorithm. !! In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially once through, from start to finish, without other processing executing as opposed to concurrently or in parallel. !! In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. !! In computer science, state space enumeration are methods that consider each reachable program state to determine whether a program satisfies a given property. !! In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree. !! In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. !! In computer science and operations research, the bees algorithm is a population-based search algorithm which was developed by Pham, Ghanbarzadeh et al. !! In computer science, a search algorithm is an algorithm (typically involving a multitude of other, more specific algorithms ) which solves a search problem. !! In computer science, a min-max heap is a complete binary tree data structure which combines the usefulness of both a min-heap and a max-heap, that is, it provides constant time retrieval and logarithmic time removal of both the minimum and maximum elements in it. !! In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. !! In computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. !! Computer science is generally considered an area of academic research and distinct from computer programming. !! In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to allow the processes to manage shared data. !! In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). !! In computer science, shared memory is memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies. !! In electrical engineering and computer science, analog image processing is any image processing task conducted on two-dimensional analog signals by analog means (as opposed to digital image processing). !! In computer science, the predecessor problem involves maintaining a set of items to, given an element, efficiently query which element precedes or succeeds that element in an order. !! In computer science, an abstract syntax tree (AST), or just syntax tree, is a tree representation of the abstract syntactic structure of text (often source code) written in a formal language. !! In computer science, the Method of Four Russians is a technique for speeding up algorithms involving Boolean matrices, or more generally algorithms involving matrices in which each cell may take on only a bounded number of possible values. !! In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small positive integers; that is, it is an integer sorting algorithm. !! In computer science, a perfect hash function h for a set S is a hash function that maps distinct elements in S to a set of m integers, with no collisions. !! Logic in computer science covers the overlap between the field of logic and that of computer science. !! In computer science, a public interface is the logical point at which independent software entities interact. !! In computer science, string generation is the process of creating a set of strings from a collection of rules. !! In computer science, control-flow analysis (CFA) is a static-code-analysis technique for determining the control flow of a program. !! Maximum Variance Unfolding (MVU), also known as Semidefinite Embedding (SDE), is an algorithm in computer science that uses semidefinite programming to perform non-linear dimensionality reduction of high-dimensional vectorial input data. !! In computer science, the maximum weight matching problem is the problem of finding, in a weighted graph, a matching in which the sum of weights is maximized. !! The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. !! In computer science, a Range Query Tree, or RQT, is a term for referring to a data structure that is used for performing range queries and updates on an underlying array, which is treated as the leaves of the tree. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! Logic in Computer Science: Modelling and Reasoning about Systems (2nd ed. !! In computer science, a shadow heap is a mergeable heap data structure which supports efficient heap merging in the amortized sense. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! In computer science, terminal and nonterminal symbols are the lexical elements used in specifying the production rules constituting a formal grammar. !! In computer science, model checking or property checking is a method for checking whether a finite-state model of a system meets a given specification (also known as correctness). !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure. !! In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (""inverse similarity"") between two text strings for approximate string matching or comparison and in fuzzy string searching. !! In combinatorial mathematics, probability, and computer science, in the longest alternating subsequence problem, one wants to find a subsequence of a given sequence in which the elements are in alternating order, and in which the sequence is as long as possible. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithmsthe amount of time, storage, or other resources needed to execute them. !! Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. !! In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. !! In computer science, a self-balancing binary search tree (BST) is any node-based binary search tree that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions. !! In computer science, the longest palindromic substring or longest symmetric factor problem is the problem of finding a maximum-length contiguous substring of a given string that is also a palindrome. !! In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions. !! Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. !! In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems. !! In computer science, a maximal pair within a string is a pair of matching substrings that are maximal, where ""maximal"" means that it is not possible to make a longer matching pair by extending the range of both substrings to the left or right. !! In computer science, the ostrich algorithm is a strategy of ignoring potential problems on the basis that they may be exceedingly rare. !! In computer science, primitive data types are a set of basic data types from which all other data types are constructed. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory. !! First-order logicalso known as predicate logic, quantificational logic, and first-order predicate calculusis a collection of formal systems used in mathematics, philosophy, linguistics, and computer science. !! In computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. !! In computer science, binary search, also known as half-interval search, logarithmic search, or binary chop, is a search algori"
intelligent agent,"Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. !! In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence. !! They may be simple or complex a thermostat is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
artificial intelligence,"Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In artificial intelligence (AI) and philosophy, AI alignment and the AI control problem are aspects of how to build AI systems such that they will aid rather than harm their creators. !! Applied Artificial Intelligence is a peer-reviewed scientific journal covering applications of artificial intelligence in management, industry, engineering, administration, and education, as well as evaluations of existing AI systems and tools and their economic, social, and cultural impact. !! Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states. !! Textual case-based reasoning (TCBR) is a subtopic of case-based reasoning, in short CBR, a popular area in artificial intelligence. !! Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (not by definition) recurrent in its implementation. !! The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). !! Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. !! In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. !! Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! In artificial intelligence, model-based reasoning refers to an inference method used in expert systems based on a model of the physical world. !! The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. !! the problem of creating 'artificial intelligence' will substantially be solved"". !! Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. !! Robotic process automation (RPA) is a form of business process automation technology based on metaphorical software robots (bots) or on artificial intelligence (AI)/digital workers. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! Computational archaeology is also known as ""archaeological informatics"" (Burenhult 2002, Huggett and Ross 2004) or ""archaeoinformatics"" (sometimes abbreviated as ""AI"", but not to be confused with artificial intelligence). !! On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence, the White Houses Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications, which includes ten principles for United States agencies when deciding whether and how to regulate AI. !! Deep Learning Studio is a software tool that aims to simplify the creation of deep learning models used in artificial intelligence. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence. !! Knowledge representation and reasoning (KRR, KR&R, KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. !! Artificial intelligence in fiction often crosses the line to apparent artificial intuition, although it can't be shown if the intent of the fiction creator was to show a simulation of intuition or that real artificial intuition is part of the story's AI, because this depends on the internal structure of the programming of the AI, which is not usually shown in stories. !! Some popular accounts use the term ""artificial intelligence"" to describe machines that mimic ""cognitive"" functions that humans associate with the human mind, such as ""learning"" and ""problem solving"", however, this definition is rejected by major AI researchers. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Argument technology is a sub-field of artificial intelligence that focuses on applying computational techniques to the creation, identification, analysis, navigation, evaluation and visualisation of arguments and debates. !! The expression is used in the second paragraph with a footnote claiming that ""computational logic"" is ""surely a better phrase than 'theorem proving', for the branch of artificial intelligence which deals with how to make machines do deduction efficiently"". !! Ontology Learning and Population: Bridging the Gap between Text and Knowledge, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2008. !! Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. !! In artificial intelligence, an embodied agent, also sometimes referred to as an interface agent, is an intelligent agent that interacts with the environment through a physical body within that environment. !! Sources disagree about exactly what constitutes ""real"" intelligence as opposed to ""simulated"" intelligence and therefore whether there is a meaningful distinction between artificial intelligence and synthetic intelligence. !! In artificial intelligence, an intelligent agent (IA) is anything which perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or may use knowledge. !! Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics. !! The voice and style of the contentThe use of content intelligence is therefore connected to the science of big data and artificial intelligence. !! Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. !! The Artificial Intelligence of Things (AIoT) is the combination of Artificial intelligence (AI) technologies with the Internet of things (IoT) infrastructure to achieve more efficient IoT operations, improve human-machine interactions and enhance data management and analytics. !! In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! The concept of rational agents can be found in various disciplines such as artificial intelligence, cognitive science, decision theory, economics, ethics, game theory, and the study of practical reason. !! The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. !! Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. !! In artificial intelligence and related fields, an argumentation framework is a way to deal with contentious information and draw conclusions from it using formalized arguments. !! In 1972 the Metamathematics Unit at the University of Edinburgh was renamed The Department of Computational Logic in the School of Artificial Intelligence. !! Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Similarity learning is an area of supervised machine learning in artificial intelligence. !! Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Although no commercially successful general-purpose computer hardware has used a dataflow architecture, it has been successfully implemented in specialized hardware such as in digital signal processing, network routing, graphics processing, telemetry, and more recently in data warehousing, and artificial intelligence. !! Moravec's paradox is the observation by artificial intelligence and robotics researchers that, contrary to traditional assumptions, reasoning requires very little computation, but sensorimotor and perception skills require enormous computational resources. !! Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. !! and have been common in fiction, as in Mary Shelley's Frankenstein or Karel apek's R. U. R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence. !! In artificial intelligence, genetic programming (GP) is a technique of evolving programs, starting from a population of unfit (usually random) programs, fit for a particular task by applying operations analogous to natural genetic processes to the population of programs. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! Computer algebra systems began to appear in the 1960s and evolved out of two quite different sourcesthe requirements of theoretical physicists and research into artificial intelligence. !! Virtual intelligence is the term given to artificial intelligence that exists within a virtual world. !! The founding concepts behind learning classifier systems came from attempts to model complex adaptive systems, using rule-based agents to form an artificial cognitive system (i. e. artificial intelligence). !! A cognitive architecture refers to both a theory about the structure of the human mind and to a computational instantiation of such a theory used in the fields of artificial intelligence (AI) and computational cognitive science. !! Ontology Learning from Text: Methods, Evaluation and Applications, Series information for Frontiers in Artificial Intelligence and Applications, IOS Press, 2005. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
intelligent agents,"In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. !! Leading AI textbooks define ""artificial intelligence"" as the ""study and design of intelligent agents"", a definition that considers goal-directed behavior to be the essence of intelligence."
cognitive modeling,"Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
cognitive science,"Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! The concept of rational agents can be found in various disciplines such as artificial intelligence, cognitive science, decision theory, economics, ethics, game theory, and the study of practical reason. !! Embodied cognitive science borrows heavily from embodied philosophy and the related research fields of cognitive science, psychology, neuroscience and artificial intelligence. !! Network capable: ability to communicate and bundle (product bundling) with another product (business) or product setsThe vision of smart products poses questions relevant to various research areas, including marketing, product engineering, computer science, artificial intelligence, economics, communication science, media economics, cognitive science, consumer psychology, innovation management and many more. !! Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
intelligent agent paradigm,"Intelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations."
computer program,"Structured programming is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by making extensive use of the structured control flow constructs of selection (if/then/else) and repetition (while and for), block structures, and subroutines. !! Intelligent agents are often described schematically as an abstract functional system similar to a computer program. !! A program transformation is any operation that takes a computer program and generates another program. !! In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources. !! Data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a computer program. !! In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). !! A real data type is a data type used in a computer program to represent an approximation of a real number. !! Automatic vectorization, in parallel computing, is a special case of automatic parallelization, where a computer program is converted from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once. !! Structured concurrency is a programming paradigm aimed at improving the clarity, quality, and development time of a computer program by using a structured approach to concurrent programming. !! A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry that executes instructions comprising a computer program. !! In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer. !! Time travel debugging or time traveling debugging is the process of stepping back in time through source code to understand what is happening during execution of a computer program. !! An algorithmic paradigm is an abstraction higher than the notion of an algorithm, just as an algorithm is an abstraction higher than a computer program. !! In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. !! In computer science, a software agent is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. !! In computer performance, the instruction path length is the number of machine code instructions required to execute a section of a computer program."
computer programming,"In computer programming, the Schwartzian transform is a technique used to improve the efficiency of sorting a list of items. !! In computer programming, a runtime system, also called runtime environment, primarily implements portions of an execution model. !! In computer science and computer programming, access level denotes the set of permissions or restrictions provided to a data type. !! In computer programming, bidirectional transformations (bx) are programs in which a single piece of code can be run in several ways, such that the same data are sometimes considered as input, and sometimes as output. !! In computer programming, a code smell is any characteristic in the source code of a program that possibly indicates a deeper problem. !! In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. !! In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! In computer programming, DLL injection is a technique used for running code within the address space of another process by forcing it to load a dynamic-link library. !! In computer programming and software design, code refactoring is the process of restructuring existing computer codechanging the factoringwithout changing its external behavior. !! Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! In computer programming, an unrolled linked list is a variation on the linked list which stores multiple elements in each node. !! In computer programming, run-time type information or run-time type identification (RTTI) is a feature of some programming languages (such as C++, Object Pascal, and Ada) that exposes information about an object's data type at runtime. !! In computer programming, boilerplate code, or simply boilerplate, are sections of code that are repeated in multiple places with little to no variation. !! In computer programming, lazy initialization is the tactic of delaying the creation of an object, the calculation of a value, or some other expensive process until the first time it is needed. !! In computer programming, string interpolation (or variable interpolation, variable substitution, or variable expansion) is the process of evaluating a string literal containing one or more placeholders, yielding a result in which the placeholders are replaced with their corresponding values. !! The academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. !! In computer programming, symbolic programming is a programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data. !! The state pattern is used in computer programming to encapsulate varying behavior for the same object, based on its internal state. !! In computer programming, a forward declaration is a declaration of an identifier (denoting an entity such as a type, a variable, a constant, or a function) for which the programmer has not yet given a complete definition. !! In computer programming, addressing modes are primarily of interest to those who write in assembly languages and to compiler writers. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. !! In computer programming, a thread pool is a software design pattern for achieving concurrency of execution in a computer program. !! An XOR linked list is a type of data structure used in computer programming. !! In some areas of computer programming, dead code is a section in the source code of a program which is executed but whose result is never used in any other computation. !! In computer programming, feature-oriented programming (FOP) or feature-oriented software development (FOSD) is a programming paradigm for program generation in software product lines (SPLs) and for incremental development of programs. !! In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. !! Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. !! In computer programming, the proxy pattern is a software design pattern. !! Computer programming is the process of performing a particular computation (or more generally, accomplishing a specific computing result), usually by designing/building an executable computer program. !! The uniform access principle of computer programming was put forth by Bertrand Meyer (originally in Object-Oriented Software Construction). !! In computer programming, undefined behavior (UB) is the result of executing a program whose behavior is prescribed to be unpredictable, in the language specification to which the computer code adheres. !! Computer science is generally considered an area of academic research and distinct from computer programming. !! An object hierarchy is a concept from computer programming. !! In computer programming, a scientific programming language can refer to two degrees of the same concept. !! In computer programming, abstraction inversion is an anti-pattern arising when users of a construct need functions implemented within it but not exposed by its interface. !! In computer programming, an anonymous function (function literal, lambda abstraction, lambda function, lambda expression or block) is a function definition that is not bound to an identifier. !! In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits. !! In computer programming, bounds checking is any method of detecting whether a variable is within some bounds before it is used. !! Lazy loading (also known as asynchronous loading) is a design pattern commonly used in computer programming and mostly in web design and development to defer initialization of an object until the point at which it is needed. !! In computer programming, primary clustering is one of two major failure modes of open addressing based hash tables, especially those using linear probing."
scientific programming language,"In this sense, C/C++ and Python can be considered scientific programming languages. !! In a stronger sense, a scientific programming language is one that is designed and optimized for the use of mathematical formula and matrices. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! In computer programming, a scientific programming language can refer to two degrees of the same concept. !! Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
programming language,"It is possible to do structured programming in any programming language, though it is preferable to use something like a procedural programming language. !! A ""type"" in type theory has a role similar to a ""type"" in a programming language: it dictates the operations that can be performed on a term and, for variables, the possible values it might be replaced with. !! However, Smith argues that this model is not true structured concurrency as the programming language is unaware of the joining behavior, and is thus unable to enforce safety. !! The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm. !! A scripting language or script language is a programming language for a runtime system that automates the execution of tasks that would otherwise be performed individually by a human operator. !! A semantic resolution tree is a tree used for the definition of the semantics of a programming language. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! In any programming language that implements short-circuit evaluation, the expression x and y is equivalent to the conditional expression if x then y else x, and the expression x or y is equivalent to if x then x else y. !! Although this is a very useful property, it has a drawback: a programming language with the normalization property cannot be Turing complete, otherwise one could solve the halting problem by seeing if the program type-checks. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! A particularly interesting example of the use of partial evaluation, first described in the 1970s by Yoshihiko Futamura, is when prog is an interpreter for a programming language. !! In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! A lambda calculus system with the normalization property can be viewed as a programming language with the property that every program terminates. !! More generally ""primitive data types"" may refer to the standard data types built into a programming language."
computational mathematics,"Computational mathematics involves mathematical research in mathematics as well as in areas of science where computation plays a central and essential role, and emphasizes algorithms, numerical methods, and symbolic computations. !! Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. !! Foundations of Computational Mathematics: Special Volume. !! In a wide sense, a scientific programming language is a programming language that is used widely for computational science and computational mathematics. !! The essence of computational science is the application of numerical algorithms and computational mathematics. !! Computational Mathematics: An Introduction to Numerical Approximation. !! Computational mathematics may also refer to the use of computers for mathematics itself."
mathematical formula,"In a stronger sense, a scientific programming language is one that is designed and optimized for the use of mathematical formula and matrices."
scientific method,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
scientific programming languages,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
scientific language,"Scientific programming languages in the stronger sense include ALGOL, APL, Fortran, J, Julia, Maple, MATLAB and R. Scientific programming languages should not be confused with scientific language in general, which refers loosely to the higher standards in precision, correctness and concision expected from practitioners of the scientific method."
computer graphics,"Computer Graphics International (CGI) is one of the oldest annual international conferences on computer graphics. !! With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! In the geometry of computer graphics, a vertex normal at a vertex of a polyhedron is a directional vector associated with a vertex, intended as a replacement to the true geometric normal of the surface. !! Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. !! Vector graphics, as a form of computer graphics, is the set of mechanisms for creating visual images directly from geometric shapes defined on a Cartesian plane, such as points, lines, curves, and polygons. !! The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. !! In computer graphics, texture filtering or texture smoothing is the method used to determine the texture color for a texture mapped pixel, using the colors of nearby texels (pixels of the texture). !! The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization. !! An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics. !! The non-artistic aspects of computer graphics are the subject of computer science research. !! A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. !! Computer graphics deals with generating images with the aid of computers. !! The development of kinetic data structures was motivated by computational geometry problems involving physical objects in continuous motion, such as collision or visibility detection in robotics, animation or computer graphics."
polygonal approximations,"Vertex normals can also be computed for polygonal approximations to surfaces such as NURBS, or specified explicitly for artistic purposes."
error message,"End-users may see a stack trace displayed as part of an error message, which the user can then report to a programmer. !! Error messages are used when user intervention is required, to indicate that a desired operation has failed, or to relay important warnings (such as warning a computer user that they are almost out of hard disk space). !! An error message is information displayed when an unforeseen problem occurs, usually on a computer or other device. !! On modern operating systems with graphical user interfaces, error messages are often displayed using dialog boxes. !! Error messages are seen widely throughout computing, and are part of every operating system or computer hardware device. !! Proper design of error messages is an important topic in usability and other fields of humancomputer interaction."
sibling calls,Sibling calls do not appear in a stack trace.
hurwitz stable,Hadamard product: The Hadamard (coefficient-wise) product of two Hurwitz stable polynomials is again Hurwitz stable.
hadamard product,"If A and B are each real-valued matrices, the Frobenius inner product is the sum of the entries of the Hadamard product. !! Hadamard product: The Hadamard (coefficient-wise) product of two Hurwitz stable polynomials is again Hurwitz stable."
information technology,"System integration is defined in engineering as the process of bringing together the component sub-systems into one system (an aggregation of subsystems cooperating so that the system is able to deliver the overarching functionality) and ensuring that the subsystems function together as a system, and in information technology as the process of linking together different computing systems and software applications physically or functionally, to act as a coordinated whole. !! Enterprise modelling constructs can focus upon manufacturing operations and/or business operations; however, a common thread in enterprise modelling is an inclusion of assessment of information technology. !! In information technology, a notification system is a combination of software and hardware that provides a means of delivering a message to a set of recipients. !! In information technology, lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. !! In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. !! Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge. !! The field of Artificial Immune Systems (AIS) is concerned with abstracting the structure and function of the immune system to computational systems, and investigating the application of these systems towards solving computational problems from mathematics, engineering, and information technology. !! Solution architecture, term used in information technology with various definitions such as; ""A description of a discrete and focused business operation or activity and how IS/IT supports that operation"". !! The National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard ""information, such as personal information (of web users), financial and banking information and sovereign data""."
machine learning,"Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! In communication networks, cognitive network (CN) is a new type of data network that makes use of cutting edge technology from several research areas (i. e. machine learning, knowledge representation, computer network, network management) to solve some problems current networks are faced with. !! In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. !! Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. !! Boltzmann machines with unconstrained connectivity have not proven useful for practical problems in machine learning or inference, but if the connectivity is properly constrained, the learning can be made efficient enough to be useful for practical problems. !! In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! Among the most used adaptive algorithms is the Widrow-Hoffs least mean squares (LMS), which represents a class of stochastic gradient-descent algorithms used in adaptive filtering and machine learning. !! One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. !! Structural risk minimization (SRM) is an inductive principle of use in machine learning. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! Data science is related to data mining, machine learning and big data. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. !! Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e. g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. !! Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. !! Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. !! Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. !! In machine learning, weighted majority algorithm (WMA) is a meta learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts. !! Meta-heuristics and machine learning are used to address the complexity of program optimization. !! Agent mining is an interdisciplinary area that synergizes multiagent systems with data mining and machine learning. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. !! Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. !! In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. !! In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. !! Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery. !! Parity learning is a problem in machine learning. !! In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. !! Regulation of algorithms, or algorithmic regulation, is the creation of laws, rules and public sector policies for promotion and regulation of algorithms, particularly in artificial intelligence and machine learning. !! In machine learning, one-class classification (OCC), also known as unary classification or class-modelling, tries to identify objects of a specific class amongst all objects, by primarily learning from a training set containing only the objects of that class, although there exist variants of one-class classifiers where counter-examples are used to further refine the classification boundary. !! Beyond quantum computing, the term ""quantum machine learning"" is also associated with classical machine learning methods applied to data generated from quantum experiments (i. e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. !! The Quantum Artificial Intelligence Lab (also called the Quantum AI Lab or QuAIL) is a joint initiative of NASA, Universities Space Research Association, and Google (specifically, Google Research) whose goal is to pioneer research on how quantum computing might help with machine learning and other difficult computer science problems. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! While the basic idea behind stochastic approximation can be traced back to the RobbinsMonro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning. !! In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
categorical label,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
pattern recognition task,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
sequence labeling,"The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. !! Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. !! Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. !! A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. !! In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
algorithmic assignment,"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values."
input sentence,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document."
speech tagging,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. !! Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
sequence labeling task,"A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document."
statistical inference,"The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. !! Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. !! Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. !! Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available."
sequence labeling algorithms,"Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence."
markov assumption,"The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain."
markov chain,"A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). !! The most common statistical models in use for sequence labeling make a Markov assumption, i. e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. !! A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. !! Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics. !! The simplest Markov model is the Markov chain. !! A continuous-time process is called a continuous-time Markov chain (CTMC). !! In mathematics, a stochastic matrix is a square matrix used to describe the transitions of a Markov chain. !! A hidden Markov model is a Markov chain for which the state is only partially observable or noisily observable. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! In the mathematical theory of probability, an absorbing Markov chain is a Markov chain in which every state can reach an absorbing state."
matrix decompositions,"There are many different matrix decompositions; each finds use among a particular class of problems. !! Analogous scale-invariant decompositions can be derived from other matrix decompositions, e. g. , to obtain scale-invariant eigenvalues. !! Refers to variants of existing matrix decompositions, such as the SVD, that are invariant with respect to diagonal scaling."
state machines,Finite-state machines are of two typesdeterministic finite-state machines and non-deterministic finite-state machines. !! The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. !! State diagrams can be used to graphically represent finite-state machines (also called finite automata).
block lanczos algorithm,"In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. !! The block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application."
integer factorization algorithms,"Trial division is the most laborious but easiest to understand of the integer factorization algorithms. !! The block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application."
global view,"In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
system usability scale,"System Usability Scale (SUS) Score Calculator !! In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
systems engineering,"In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements. !! Meta-process modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable and useful to some predefined problems. !! In systems engineering, the system usability scale (SUS) is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability."
regression analysis,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis. !! In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis. !! Symbolic Regression (SR) is a type of regression analysis that searches the space of mathematical expressions to find the model that best fits a given dataset, both in terms of accuracy and simplicity. !! In regression analysis, ""mean squared error"", often referred to as mean squared prediction error or ""out-of-sample mean squared error"", can also refer to the mean value of the squared deviations of the predictions from the true values, over an out-of-sample test space, generated by a model estimated over a particular sample space."
supervised learning models,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis."
support-vector machine,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. !! It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support-vector machines, although given enough samples the algorithm still performs well. !! Support-vector machine weights have also been used to interpret SVM models in the past. !! More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. !! Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences."
dimensional space,"Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution. !! Sparse distributed memory is a mathematical representation of human memory, and uses high-dimensional space to help model the large amounts of memory that mimics that of the human neural network. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! Six degrees of freedom (6DOF) refers to the freedom of movement of a rigid body in three-dimensional space. !! More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. !! The metric structures in computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms."
vector machine models,Posthoc interpretation of support-vector machine models in order to identify features used by the model to make predictions is a relatively new area of research with special significance in the biological sciences.
genetic representation,"Genetic representation can encode appearance, behavior, physical qualities of individuals. !! In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. !! Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. !! The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. !! Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation."
physical qualities,"Genetic representation can encode appearance, behavior, physical qualities of individuals."
evolutionary computation,"In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. !! In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. !! Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation. !! Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes. !! Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation."
known classes,Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.
genetic representations,Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.
information theory,"Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to practical disciplines (including the design and implementation of hardware and software). !! In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions. !! Information theory is the scientific study of the quantification, storage, and communication of digital information. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In other words, it is shown within algorithmic information theory that computational incompressibility ""mimics"" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Gray, R. M. (2011), Entropy and Information Theory, Springer. !! In probability theory and information theory, adjusted mutual information, a variation of mutual information may be used for comparing clusterings. !! Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security. !! Additive white Gaussian noise (AWGN) is a basic noise model used in information theory to mimic the effect of many random processes that occur in nature. !! In information theory, units of information are also used to measure information contained in messages and the entropy of random variables. !! A key measure in information theory is entropy. !! In probability theory and in particular in information theory, total correlation (Watanabe 1960) is one of several generalizations of the mutual information. !! Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. !! The online textbook: Information Theory, Inference, and Learning Algorithms, by David J. C. MacKay, discusses the BCJR algorithm in chapter 25. !! In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! Information Theory has provided successful methods for alignment-free sequence analysis and comparison. !! In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points."
digital information,"Information theory is the scientific study of the quantification, storage, and communication of digital information."
scientific study,"Information theory is the scientific study of the quantification, storage, and communication of digital information."
algorithmic complexity theory,"Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security."
algorithmic information theory,"Algorithmic information theory principally studies complexity measures on strings (or other data structures). !! Unlike classical information theory, algorithmic information theory gives formal, rigorous definitions of a random string and a random infinite sequence that do not depend on physical or philosophical intuitions about nondeterminism or likelihood. !! Informally, from the point of view of algorithmic information theory, the information content of a string is equivalent to the length of the most-compressed possible self-contained representation of that string. !! Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security. !! Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. !! In other words, it is shown within algorithmic information theory that computational incompressibility ""mimics"" (except for a constant that only depends on the chosen universal programming language) the relations or inequalities found in information theory. !! In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation."
data compression,"In data compression and the theory of formal languages, the smallest grammar problem is the problem of finding the smallest context-free grammar that generates a given string of characters (but no other string). !! Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Practical streaming media was only made possible with advances in data compression, due to the impractically high bandwidth requirements of uncompressed media. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data."
zip files,"Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL)."
error detection,"Error detection is most commonly realized using a suitable hash function (or specifically, a checksum, cyclic redundancy check or other algorithm). !! Computer programming tasks that require bit manipulation include low-level device control, error detection and correction algorithms, data compression, encryption algorithms, and optimization. !! Applications of fundamental topics of information theory include source coding/data compression (e. g. for ZIP files), and channel coding/error detection and correction (e. g. for DSL). !! Error detection is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver. !! Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases. !! If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. !! In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! Error detection and correction codes are often used to improve the reliability of data storage media."
gaming systems,"Expanding outside of virtual reality and gaming systems, a newer segment of video game rehabilitation focuses on games that can be played on mobile phones or tablet computers - mobile apps."
virtual reality,"An immersive virtual musical instrument, or immersive virtual environment for music and sound, represents sound processes and their parameters as 3D entities of a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, using 3D interface metaphors consisting of interaction techniques such as navigation, selection and manipulation (NSM). !! Expanding outside of virtual reality and gaming systems, a newer segment of video game rehabilitation focuses on games that can be played on mobile phones or tablet computers - mobile apps."
computer skills,"Another challenge in video game rehabilitation, can result to lack of computer skills on the part of therapists, lack of support infrastructure, expensive equipment, inadequate communication infrastructure, and patient safety concerns."
temporal database,"A uni-temporal database has one axis of time, either the validity range or the system time range. !! Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time. !! A temporal database stores data relating to time instances. !! In order to handle changes in the information content anchor modeling emulates aspects of a temporal database in the resulting relational database schema. !! Temporal databases could be uni-temporal, bi-temporal or tri-temporal. !! Richard Snodgrass proposed in 1992 that temporal extensions to SQL be developed by the temporal database community."
temporal databases,"Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time."
current databases,"Temporal databases are in contrast to current databases (not to be confused with currently available databases), which store only facts which are believed to be true at the current time."
regularized logistic regression,Regularized logistic regression is specifically intended to be used in this situation.
global solutions,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance."
deterministic global optimization,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance. !! Deterministic global optimization methods require ways to rigorously bound function values over regions of space. !! The term ""deterministic global optimization"" typically refers to complete or rigorous (see below) optimization methods. !! Deterministic global optimization methods typically belong to the last two categories. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem."
numerical optimization,"Deterministic global optimization is a branch of numerical optimization which focuses on finding the global solutions of an optimization problem whilst providing theoretical guarantees that the reported solution is indeed the global one, within some predefined tolerance."
optimization methods,"Increasingly, operations research uses stochastic programming to model dynamic decisions that adapt to events; such problems can be solved with large-scale optimization and stochastic optimization methods. !! Stochastic optimization (SO) methods are optimization methods that generate and use random variables. !! Nonlinear optimization methods are widely used in conformational analysis. !! The term ""deterministic global optimization"" typically refers to complete or rigorous (see below) optimization methods. !! More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution."
global minimum,"Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem."
mathematical model,"Sparse distributed memory (SDM) is a mathematical model of human long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center. !! A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem. !! In computer science, an abstract data type (ADT) is a mathematical model for data types."
optimization problem,"In a genetic algorithm, a population of candidate solutions (called individuals, creatures, organisms, or phenotypes) to an optimization problem is evolved toward better solutions. !! Deterministic global optimization methods are typically used when locating the global solution is a necessity (i. e. when the only naturally occurring state described by a mathematical model is the global minimum of an optimization problem), when it is extremely difficult to find a feasible solution, or simply when the user desires to locate the best possible solution to a problem. !! In optimization theory, semi-infinite programming (SIP) is an optimization problem with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints."
lexicographic code,"Lexicographic codes or lexicodes are greedily generated error-correcting codes with remarkably good properties. !! In particular, the codewords in a binary lexicographic code of distance d encode the winning positions in a variant of Grundy's game, played on a collection of heaps of stones, in which each move consists of replacing any one heap by at most d 1 smaller heaps, and the goal is to take the last stone. !! The binary lexicographic codes are linear codes, and include the Hamming codes and the binary Golay codes. !! The theory of lexicographic codes is closely connected to combinatorial game theory. !! Following C generate lexicographic code and parameters are set for the Golay code (N=24, D=8)."
binary lexicographic codes,"The binary lexicographic codes are linear codes, and include the Hamming codes and the binary Golay codes."
golay code,"Following C generate lexicographic code and parameters are set for the Golay code (N=24, D=8)."
combinatorial game theory,The theory of lexicographic codes is closely connected to combinatorial game theory.
binary lexicographic code,"In particular, the codewords in a binary lexicographic code of distance d encode the winning positions in a variant of Grundy's game, played on a collection of heaps of stones, in which each move consists of replacing any one heap by at most d 1 smaller heaps, and the goal is to take the last stone."
minimum k-cut,"Inapproximability of Maximum Edge Biclique, Maximum Balanced Biclique and Minimum k-Cut from the Small Set Expansion Hypothesis. !! In mathematics, the minimum k-cut, is a combinatorial optimization problem that requires finding a set of edges whose removal would partition the graph to at least k connected components. !! Guttmann-Beck, N. ; Hassin, R. (1999), ""Approximation algorithms for minimum k-cut"" (PDF), Algorithmica, pp."
approximation algorithms,"The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. !! Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P NP conjecture. !! However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. !! Guttmann-Beck, N. ; Hassin, R. (1999), ""Approximation algorithms for minimum k-cut"" (PDF), Algorithmica, pp."
machine learning algorithms,"While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. !! Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. !! Deep learning is a class of machine learning algorithms that:199200 uses multiple layers to progressively extract higher-level features from the raw input. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms."
manifold alignment,"Manifold alignment assumes that disparate data sets produced by similar generating processes will share a similar underlying manifold representation. !! Perform linear manifold alignment on the embedded data, holding the first data set fixed, mapping each additional data set onto the first's manifold. !! Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. !! Manifold alignment can be used to find linear (feature-level) projections, or nonlinear (instance-level) embeddings. !! Most manifold alignment techniques consider only two data sets, but the concept extends to arbitrarily many initial data sets."
embedded data,"Perform linear manifold alignment on the embedded data, holding the first data set fixed, mapping each additional data set onto the first's manifold."
fuzzy classification,"A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function. !! A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~. !! Accordingly, fuzzy classification is the process of grouping individuals having the same characteristics into a fuzzy set. !! Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function. !! The fuzzy classification predicate ~ corresponds to a fuzzy restriction ""i is R"" of U, where R is a fuzzy set defined by a truth function."
fuzzy propositional function,Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function.
truth value,"In a temporal logic, a statement can have a truth value that varies in timein contrast with an atemporal logic, which applies only to statements whose truth values are constant in time. !! Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. !! Fuzzy classification is the process of grouping elements into a fuzzy set whose membership function is defined by the truth value of a fuzzy propositional function."
fuzzy classification predicate,"The fuzzy classification predicate ~ corresponds to a fuzzy restriction ""i is R"" of U, where R is a fuzzy set defined by a truth function. !! A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function."
fuzzy class,A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function.
fuzzy set,"Accordingly, fuzzy classification is the process of grouping individuals having the same characteristics into a fuzzy set. !! A fuzzy class ~C = { i | ~(i) } is defined as a fuzzy set ~C of individuals i satisfying a fuzzy classification predicate ~ which is a fuzzy propositional function."
membership function,"A fuzzy classification corresponds to a membership function that indicates whether an individual is a member of a class, given its fuzzy classification predicate ~."
mobile ad hoc networks,The Wireless Routing Protocol (WRP) is a proactive unicast routing protocol for mobile ad hoc networks (MANETs). !! The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks.
wireless routing protocol,The Wireless Routing Protocol (WRP) is a proactive unicast routing protocol for mobile ad hoc networks (MANETs).
immersive virtual environment,"An immersive virtual musical instrument, or immersive virtual environment for music and sound, represents sound processes and their parameters as 3D entities of a virtual reality so that they can be perceived not only through auditory feedback but also visually in 3D and possibly through tactile as well as haptic feedback, using 3D interface metaphors consisting of interaction techniques such as navigation, selection and manipulation (NSM)."
human-centered computing,"Human-centered computing is closely related to human-computer interaction and information science. !! Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! Human-centered computing (HCC) studies the design, development, and deployment of mixed-initiative human-computer systems. !! Human-centered systems (HCS) are systems designed for human-centered computing. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
computer systems,"In computer science, an access control matrix or access matrix is an abstract, formal security model of protection state in computer systems, that characterizes the rights of each subject with respect to every object in the system. !! Digital identity is now often used in ways that require data about persons stored in computer systems to be linked to their civil, or national, identities. !! The term ""digital identity"" also denotes certain aspects of civil and personal identity that have resulted from the widespread use of identity information to represent people in an acceptable and trusted digital format in computer systems. !! Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145, !! Human-centered computing (HCC) studies the design, development, and deployment of mixed-initiative human-computer systems. !! ACM Transactions on Computer Systems is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery. !! In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. !! A digital identity is information on an entity used by computer systems to represent an external agent. !! By the everyday usage definition of the phrase, all computer systems are reasoning systems in that they all automate some type of logic or decision."
information science,"Document classification or document categorization is a problem in library science, information science and computer science. !! In philosophy, the term formal ontology is used to refer to an ontology defined by axioms in a formal language with the goal to provide an unbiased (domain- and application-independent) view on reality, which can help the modeler of domain- or application-specific ontologies (information science) to avoid possibly erroneous ontological assumptions encountered in modeling large-scale ontologies. !! However, data science is different from computer science and information science. !! Human-centered computing is closely related to human-computer interaction and information science. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
computer interaction,"The nature of interactive computing as well as its impact on users, are studied extensively in the field of computer interaction. !! Human-centered computing is closely related to human-computer interaction and information science. !! Most self-identified persuasive technology research focuses on interactive, computational technologies, including desktop computers, Internet services, video games, and mobile devices, but this incorporates and builds on the results, theories, and methods of experimental psychology, rhetoric, and human-computer interaction. !! User experience design draws from design approaches like human-computer interaction and user-centered design, and includes elements from similar disciplines like interaction design, visual design, information architecture, user research, and others. !! Augmented cognition is an interdisciplinary area of psychology and engineering, attracting researchers from the more traditional fields of human-computer interaction, psychology, ergonomics and neuroscience. !! Graphically embodied agents aim to unite gesture, facial expression and speech to enable face-to-face communication with users, providing a powerful means of human-computer interaction. !! In computing, scratch input is an acoustic-based method of Human-Computer Interaction (HCI) that takes advantage of the characteristic sound produced when a finger nail or other object is dragged over a surface, such as a table or wall. !! In human-computer interaction, low-key feedback is a type of output that takes a background role by being very subtle, sometimes nearly imperceptible. !! In computer science, particularly in human-computer interaction, presentation semantics specify how a particular piece of a formal language is represented in a distinguished manner accessible to human senses, usually human vision. !! Human-centered computing is usually concerned with systems and practices of technology use while human-computer interaction is more focused on ergonomics and the usability of computing artifacts and information science is focused on practices surrounding the collection, manipulation, and use of information."
graphic design,"Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design. !! One definition is that it's information visualization when the spatial representation (e. g. , the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given. !! Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization."
industrial design,"Human-centered computing researchers and practitioners usually come from one or more of disciplines such as computer science, human factors, sociology, psychology, cognitive science, anthropology, communication studies, graphic design and industrial design."
computer network,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network. !! Network architecture is the design of a computer network. !! Computer network programming involves writing computer programs that enable processes to communicate with each other across a computer network. !! The nodes of a computer network may include personal computers, servers, networking hardware, or other specialised or general-purpose hosts. !! Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. !! Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications. !! In telecommunications, a protocol data unit (PDU) is a single unit of information transmitted among peer entities of a computer network. !! In communication networks, cognitive network (CN) is a new type of data network that makes use of cutting edge technology from several research areas (i. e. machine learning, knowledge representation, computer network, network management) to solve some problems current networks are faced with. !! Computer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent. !! A computer network is a set of computers sharing resources located on or provided by network nodes. !! A grid network is not the same as a grid computer or a computational grid, although the nodes in a grid network are usually computers, and grid computing requires some kind of computer network or ""universal coding"" to interconnect the computers. !! Software incompatibility is a characteristic of software components or systems which cannot operate satisfactorily together on the same computer, or on different computers linked by a computer network. !! For example, a notification system can send an e-mail announcing when a computer network will be down for a scheduled maintenance. !! A broadcast storm or broadcast radiation is the accumulation of broadcast and multicast traffic on a computer network. !! Computer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. !! A broadcast domain is a logical division of a computer network, in which all nodes can reach each other by broadcast at the data link layer."
network architecture,"Network architecture is the design of a computer network. !! In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network. !! In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated. !! The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture. !! The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
communications network,"In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated."
internet protocol suite,"In the Internet Protocol Suite (TCP/IP), the data link layer functionality is contained within the link layer, the lowest layer of the descriptive model, which is assumed to be independent of physical infrastructure. !! The Internet Protocol (IP) is the network layer communications protocol in the Internet protocol suite for relaying datagrams across network boundaries. !! The Internet protocol suite is therefore often referred to as TCP/IP. !! The Internet Control Message Protocol (ICMP) is a supporting protocol in the Internet protocol suite. !! The Common Open Policy Service (COPS) Protocol is part of the internet protocol suite as defined by the RFC 2748. !! The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
hardware links,"The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links."
layered network architecture,The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture.
osi model,"Cognitive network is different from cognitive radio (CR) as it covers all the layers of the OSI model (not only layers 1 and 2 as with CR ). !! The Open Systems Interconnection model (OSI model) defines and codifies the concept of layered network architecture. !! The application firewall can control communications up to the application layer of the OSI model, which is the highest operating layer, and where it gets its name."
distributed computing,"Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network. !! In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction."
distributed application architecture,"In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network."
distributed application,"In distributed computing, the network architecture often describes the structure and classification of a distributed application architecture, as the participating nodes in a distributed application are often referred to as a network."
lossless compression,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. !! By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data. !! Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
original data,"Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases. !! Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. !! Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data."
compressed data,Lossless compression is a class of data compression that allows the original data to be perfectly reconstructed from the compressed data.
lossless compression algorithm,"Some of the most common lossless compression algorithms are listed below. !! While, in principle, any general-purpose lossless compression algorithm (general-purpose meaning that they can accept any bitstring) can be used on any type of data, many are unable to achieve significant compression on data that are not of the form for which they were designed to compress. !! However, many ordinary lossless compression algorithms produce headers, wrappers, tables, or other predictable output that might instead make cryptanalysis easier. !! By operation of the pigeonhole principle, no lossless compression algorithm can efficiently compress all possible data. !! No lossless compression algorithm can efficiently compress all possible data (see the section Limitations below for details)."
decompressed data,"Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable."
lossy methods,"Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
image file formats,"Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods."
bit sequences,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data."
statistical model,"Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! Bayesian inference derives the posterior probability as a consequence of two antecedents: a prior probability and a ""likelihood function"" derived from a statistical model for the observed data."
input data,"Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Most lossless compression programs do two things in sequence: the first step generates a statistical model for the input data, and the second step uses this model to map input data to bit sequences in such a way that ""probable"" (e. g. frequently encountered) data will produce shorter output than ""improbable"" data. !! Here is a simple competitive learning algorithm to find three clusters within some input data. !! This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm."
state machine,"Finite-state machines are of two typesdeterministic finite-state machines and non-deterministic finite-state machines. !! The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. !! Finite-state machine-based programming is generally the same, but, formally speaking, does not cover all possible variants, as FSM stands for finite-state machine, and automata-based programming does not necessarily employ FSMs in the strict sense. !! A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. !! Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! The finite-state machine has less computational power than some other models of computation such as the Turing machine. !! A deterministic finite-state machine can be constructed equivalent to any non-deterministic one."
automata-based programming,"Automata-based programming indeed closely matches the programming needs found in the field of automation. !! Another reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc. !! Finite-state machine-based programming is generally the same, but, formally speaking, does not cover all possible variants, as FSM stands for finite-state machine, and automata-based programming does not necessarily employ FSMs in the strict sense. !! Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! Automata-based programming is widely used in lexical and syntactic analyses."
programming paradigm,"Automata-based programming is a programming paradigm in which the program or part of it is thought of as a model of a finite-state machine (FSM) or any other (often more complicated) formal automaton (see automata theory). !! In computer programming, symbolic programming is a programming paradigm in which the program can manipulate its own formulas and program components as if they were plain data. !! In computer programming, feature-oriented programming (FOP) or feature-oriented software development (FOSD) is a programming paradigm for program generation in software product lines (SPLs) and for incremental development of programs. !! Logic programming is a programming paradigm which is largely based on formal logic."
markov algorithms,"Another reason for using the notion of automata-based programming is that the programmer's style of thinking about the program in this technique is very similar to the style of thinking used to solve mathematical tasks using Turing machines, Markov algorithms, etc. !! Refal is a programming language based on Markov algorithms. !! Markov algorithms have been shown to be Turing-complete, which means that they are suitable as a general model of computation and can represent any mathematical expression from its simple notation."
web design,"Lazy loading (also known as asynchronous loading) is a design pattern commonly used in computer programming and mostly in web design and development to defer initialization of an object until the point at which it is needed. !! User experience evaluation has become common practice in web design, especially within organizations implementing user-centered design practices. !! Responsive web design (RWD) or responsive design is an approach to web design that aims to make web pages render well on a variety of devices and window or screen sizes from minimum to maximum display size to ensure usability and satisfaction."
statistical language model,A cache language model is a type of statistical language model. !! A statistical language model is a probability distribution over sequences of words.
language model,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! A statistical language model is a probability distribution over sequences of words. !! Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model. !! The language model provides context to distinguish between words and phrases that sound phonetically similar. !! Data sparsity is a major problem in building language models."
probability distribution,"A statistical language model is a probability distribution over sequences of words. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Monte Carlo methods are mainly used in three problem classes: optimization, numerical integration, and generating draws from a probability distribution. !! Then, the photonic implementation of the boson sampling task consists of generating a sample from the probability distribution of single-photon measurements at the output of the circuit. !! Maximal entropy random walk (MERW) is a popular type of biased random walk on a graph, in which transition probabilities are chosen accordingly to the principle of maximum entropy, which says that the probability distribution which best represents the current state of knowledge is the one with largest entropy. !! In statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. !! Since Bayesian statistics treats probability as a degree of belief, Bayes' theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters."
data sparsity,Data sparsity is a major problem in building language models.
optical character recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications."
handwriting recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Graph edit distance finds applications in handwriting recognition, fingerprint recognition and cheminformatics."
machine translation,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications."
speech recognition,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). !! Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure. !! Speech recognition applications include voice user interfaces such as voice dialing (e. g. ""call home""), call routing (e. g. ""I would like to make a collect call""), domotic appliance control, search key words (e. g. find a podcast where particular words were spoken), simple data entry (e. g. , entering a credit card number), preparation of structured documents (e. g. a radiology report), determining speaker characteristics, speech-to-text processing (e. g. , word processors or emails), and aircraft (usually termed direct voice input). !! From the technology perspective, speech recognition has a long history with several waves of major innovations. !! Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! The term voice recognition can refer to speaker recognition or speech recognition. !! Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition. !! Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics. !! Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics."
information retrieval,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Reverse image search is a content-based image retrieval (CBIR) query technique that involves providing the CBIR system with a sample image that it will then base its search upon; in terms of information retrieval, the sample image is what formulates a search query. !! Spreading activation can also be applied in information retrieval, by means of a network of nodes representing documents and terms contained in those documents. !! Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. !! Exploratory search is a topic that has grown from the fields of information retrieval and information seeking but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a Google-like keyword search)."
language modeling,"Language modeling is used in speech recognition, machine translation, part-of-speech tagging, parsing, Optical Character Recognition, handwriting recognition, information retrieval and other applications. !! Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers."
pronunciation model,Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.
acoustic model,Ambiguities are easier to resolve when evidence from the language model is integrated with a pronunciation model and an acoustic model.
source code,"Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code. !! Some programming languages allow a program to operate differently or even have a different control flow than the source code, as long as it exhibits the same user-visible side effects, if undefined behavior never happens during program execution. !! If Istatic is source code designed to run inside that interpreter, then partial evaluation of the interpreter with respect to this data/program produces prog*, a version of the interpreter that only runs that source code, is written in the implementation language of the interpreter, does not require the source code to be resupplied, and runs faster than the original combination of the interpreter and the source. !! In some areas of computer programming, dead code is a section in the source code of a program which is executed but whose result is never used in any other computation. !! Inline expansion is similar to macro expansion, but occurs during compilation, without changing the source code (the text), while macro expansion occurs prior to compilation, and results in different text that is then processed by the compiler. !! In computing, file comparison is the calculation and display of the differences and similarities between data objects, typically text files such as source code. !! A compilation error message often helps programmers debugging the source code. !! In computer programming, a code smell is any characteristic in the source code of a program that possibly indicates a deeper problem. !! Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! Time travel debugging or time traveling debugging is the process of stepping back in time through source code to understand what is happening during execution of a computer program. !! A software code audit is a comprehensive analysis of source code in a programming project with the intent of discovering bugs, security breaches or violations of programming conventions. !! In computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. !! Loop optimization can be viewed as the application of a sequence of specific loop transformations (listed below or in Compiler transformations for high-performance computing) to the source code or intermediate representation, with each transformation having an associated test for legality. !! Formal verification can be helpful in proving the correctness of systems such as: cryptographic protocols, combinational circuits, digital circuits with internal memory, and software expressed as source code."
data objects,"In computing, file comparison is the calculation and display of the differences and similarities between data objects, typically text files such as source code."
prototype-based supervised classification algorithm,"In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm."
learning vector quantization,"See the 'Bibliography on the Self-Organizing Map (SOM) and Learning Vector Quantization (LVQ)'. !! In computer science, learning vector quantization (LVQ) is a prototype-based supervised classification algorithm. !! Self-Organizing Maps and Learning Vector Quantization for Feature Sequences, Somervuo and Kohonen."
reduction systems,"Such methods may be achieved by rewriting systems (also known as rewrite systems, rewrite engines, or reduction systems)."
compute arithmetic operations,Term rewriting systems can be employed to compute arithmetic operations on natural numbers.
functional architecture,"Traceabilities also support change management as part of requirements management in understanding the impacts of changes through requirements or other related elements (e. g. , functional impacts through relations to functional architecture), and facilitating introducing these changes."
language specification,"In computer programming, undefined behavior (UB) is the result of executing a program whose behavior is prescribed to be unpredictable, in the language specification to which the computer code adheres. !! In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification."
compiler correctness,"In computing, compiler correctness is the branch of computer science that deals with trying to show that a compiler behaves according to its language specification."
relational semantics,An accessibility relation is a relation which plays a key role in assigning truth values to sentences in the relational semantics for modal logic.
mathematical study,"Queueing theory is the mathematical study of waiting lines, or queues. !! The mathematical study of how validities are tied to conditions on accessibility relations is known as modal correspondence theory. !! Switching circuit theory is the mathematical study of the properties of networks of idealized switches. !! Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family of curves, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations."
double-ended priority queue,"One example application of the double-ended priority queue is external sorting. !! Double-ended priority queues can be built from balanced binary search trees (where the minimum and maximum elements are the leftmost and rightmost leaves, respectively), or using specialized data structures like min-max heap and pairing heap. !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure."
priority queue,"In computer science, a binomial heap is a data structure that acts as a priority queue but also allows pairs of heaps to be merged. !! The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. !! In computer science, a double-ended priority queue (DEPQ) or double-ended heap is a data structure similar to a priority queue or heap, but allows for efficient removal of both the maximum and minimum, according to some ordering on the keys (items) stored in the structure."
balanced binary search trees,"Double-ended priority queues can be built from balanced binary search trees (where the minimum and maximum elements are the leftmost and rightmost leaves, respectively), or using specialized data structures like min-max heap and pairing heap."
connected wireless device,Mobile virtualization is hardware virtualization on a mobile phone or connected wireless device.
arnoldi iteration,"The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! The Arnoldi iteration was invented by W. E. Arnoldi in 1951. !! This happens when the minimal polynomial of A is of degree k. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and GMRES, the algorithm has converged at this point."
eigenvalue algorithm,"In numerical linear algebra, the QR algorithm or QR iteration is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. !! The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! The eigenvalue algorithm can then be applied to the restricted matrix. !! If an eigenvalue algorithm does not produce eigenvectors, a common practice is to use an inverse iteration based algorithm with set to a close approximation to the eigenvalue. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not. !! Hessenberg and tridiagonal matrices are the starting points for many eigenvalue algorithms because the zero entries reduce the complexity of the problem. !! Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates. !! These eigenvalue algorithms may also find eigenvectors. !! This happens when the minimal polynomial of A is of degree k. In most applications of the Arnoldi iteration, including the eigenvalue algorithm below and GMRES, the algorithm has converged at this point."
iterative method,"In mathematics, the generalized minimal residual method (GMRES) is an iterative method for the numerical solution of an indefinite nonsymmetric system of linear equations. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable). !! Rayleigh quotient iteration is an iterative method, that is, it delivers a sequence of approximate solutions that converges to a true solution in the limit. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. !! In mathematics, the folded spectrum method (FSM) is an iterative method for solving large eigenvalue problems."
numerical linear algebra,"Because many properties of matrices and vectors also apply to functions and operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms. !! In numerical linear algebra, the QR algorithm or QR iteration is an eigenvalue algorithm: that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. !! In the mathematical discipline of numerical linear algebra, a matrix splitting is an expression which represents a given matrix as a sum or difference of matrices. !! Householder transformations are widely used in numerical linear algebra, for example, to annihilate the entries below the main diagonal of a matrix, to perform QR decompositions and in the first step of the QR algorithm. !! In numerical linear algebra, the biconjugate gradient stabilized method, often abbreviated as BiCGSTAB, is an iterative method developed by H. A. van der Vorst for the numerical solution of nonsymmetric linear systems. !! In numerical linear algebra, a convergent matrix is a matrix that converges to the zero matrix under matrix exponentiation. !! Numerical linear algebra aims to solve problems of continuous mathematics using finite precision computers, so its applications to the natural and social sciences are as vast as the applications of continuous mathematics. !! Numerical linear algebra uses properties of vectors and matrices to develop computer algorithms that minimize the error introduced by the computer, and is also concerned with ensuring that the algorithm is as efficient as possible. !! Noting the broad applications of numerical linear algebra, Lloyd N. Trefethen and David Bau, III argue that it is ""as fundamental to the mathematical sciences as calculus and differential equations"",:x even though it is a comparatively small field. !! In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. !! In numerical linear algebra, a Givens rotation is a rotation in the plane spanned by two coordinates axes. !! The main use of Givens rotations in numerical linear algebra is to introduce zeros in vectors or matrices. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the GaussSeidel method for solving a linear system of equations, resulting in faster convergence. !! Numerical linear algebra, sometimes called applied linear algebra, is the study of how matrix operations can be used to create computer algorithms which efficiently and accurately provide approximate answers to questions in continuous mathematics."
optimality problem,The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
characteristic polynomial,Thus eigenvalue algorithms that work by finding the roots of the characteristic polynomial can be ill-conditioned even when the problem is not. !! The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
monic polynomials,The characteristic polynomial of Hn minimizes ||p(A)q1||2 among all monic polynomials of degree n. This optimality problem has a unique solution if and only if the Arnoldi iteration does not break down.
krylov subspace,"The idea of the Arnoldi iteration as an eigenvalue algorithm is to compute the eigenvalues in the Krylov subspace. !! Many linear dynamical system tests in control theory, especially those related to controllability and observability, involve checking the rank of the Krylov subspace. !! These tests are equivalent to finding the span of the Grammians associated with the system/output maps so the uncontrollable and unobservable subspaces are simply the orthogonal complement to the Krylov subspace. !! denotes the maximal dimension of a Krylov subspace. !! Krylov subspaces are used in algorithms for finding approximate solutions to high-dimensional linear algebra problems. !! can be decomposed as the direct sum of Krylov subspaces."
decision boundary,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous. !! In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has. !! In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. !! The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class. !! If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
margin classifier,"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. !! This extends the geometric interpretation of SVMfor linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier. !! One theoretical motivation behind margin classifiers is that their generalization error may be bound by parameters of the algorithm and a margin term."
linear equations,"In the finite element method for the numerical solution of elliptic partial differential equations, the stiffness matrix represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation. !! In mathematics, the generalized minimal residual method (GMRES) is an iterative method for the numerical solution of an indefinite nonsymmetric system of linear equations. !! In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. !! The conjugate residual method is an iterative numeric method used for solving systems of linear equations. !! One way to find the LU decomposition of this simple matrix would be to simply solve the linear equations by inspection. !! In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is positive-definite. !! There are several algorithms for solving a system of linear equations. !! When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations. !! The simplest method for solving a system of linear equations is to repeatedly eliminate variables. !! Iterative refinement is an iterative method proposed by James H. Wilkinson to improve the accuracy of numerical solutions to systems of linear equations. !! In the case of a system of linear equations, the two main classes of iterative methods are the stationary iterative methods, and the more general Krylov subspace methods. !! This is an example of equivalence in a system of linear equations."
system matrix,"The conjugate residual method differs from the closely related conjugate gradient method primarily in that it involves more numerical operations and requires more storage, but the system matrix is only required to be Hermitian, not symmetric positive definite."
numerical operations,"The conjugate residual method differs from the closely related conjugate gradient method primarily in that it involves more numerical operations and requires more storage, but the system matrix is only required to be Hermitian, not symmetric positive definite."
representation learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning."
feature learning,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! In supervised feature learning, features are learned using labeled input data. !! Feature learning can be either supervised or unsupervised. !! In unsupervised feature learning, features are learned with unlabeled input data. !! Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process."
feature detection,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data."
raw data,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. !! As well, raw data have not been subject to any other manipulation by a software program or a human researcher, analyst or technician. !! A cyclic redundancy check (CRC) is an error-detecting code commonly used in digital networks and storage devices to detect accidental changes to raw data. !! In the context of examinations, the raw data might be described as a raw score (after test scores). !! Raw data have not been subjected to processing, ""cleaning"" by researchers to remove outliers, obvious instrument reading errors or data entry errors, or any analysis (e. g. , determining central tendency aspects such as the average or median result). !! Raw data, also known as primary data, are data (e. g. , numbers, instrument readings, figures, etc. ) !! If a scientist sets up a computerized thermometer which records the temperature of a chemical mixture in a test tube every minute, the list of temperature readings for every minute, as printed out on a spreadsheet or viewed on a computer screen are ""raw data""."
machine learning tasks,Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process.
supervised feature learning,"In supervised feature learning, features are learned using labeled input data."
unlabeled input data,"In unsupervised feature learning, features are learned with unlabeled input data."
unsupervised feature learning,"In unsupervised feature learning, features are learned with unlabeled input data."
tree decomposition,"The concept of tree decompositions was originally introduced by Rudolf Halin (1976). !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! Thus, given a graph G = (V, E), a tree decomposition is a pair (X, T), where X = {X1, . !! Intuitively, a tree decomposition represents the vertices of a given graph G as subtrees of a tree, in such a way that vertices in the given graph are adjacent only when the corresponding subtrees intersect. !! In graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph."
graph theory,"a depth-first search starting at the node A, assuming that the left edges in the shown graph are chosen before right edges, and assuming the search remembers previously visited nodes and will not repeat them (since this is a small graph), will visit the nodes in the following order: A, B, D, F, E, C, G. The edges traversed in this search form a Trmaux tree, a structure with important applications in graph theory. !! In the mathematical field of graph theory, a spanning tree T of an undirected graph G is a subgraph that is a tree which includes all of the vertices of G. In general, a graph may have several spanning trees, but a graph that is not connected will not contain a spanning tree (see spanning forests below). !! Definitions in graph theory vary. !! In mathematics, and more specifically in graph theory, a directed graph (or digraph) is a graph that is made up of a set of vertices connected by directed edges often called arcs. !! Graph theory plays an important role in electrical modeling of electrical networks, here, weights are associated with resistance of the wire segments to obtain electrical properties of network structures. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! Chemical graph theory uses the molecular graph as a means to model molecules. !! In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. !! The power of trace theory stems from the fact that the algebra of dependency graphs (such as Petri nets) is isomorphic to that of trace monoids, and thus, one can apply both algebraic formal language tools, as well as tools from graph theory. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! Graph theory is also used to study molecules in chemistry and physics. !! In computing and graph theory, a dynamic connectivity structure is a data structure that dynamically maintains information about the connected components of a graph. !! In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. !! In graph theory, a tree decomposition is a mapping of a graph into a tree that can be used to define the treewidth of the graph and speed up solving certain computational problems on the graph. !! In graph theory, the metric k-center or metric facility location problem is a combinatorial optimization problem studied in theoretical computer science."
tree decompositions,"The concept of tree decompositions was originally introduced by Rudolf Halin (1976). !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! Many algorithmic problems that are NP-complete for arbitrary graphs may be solved efficiently for partial k-trees by dynamic programming, using the tree decompositions of these graphs."
join trees,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
clique trees,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
matrix decomposition,"Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition."
query optimization,"Query optimization is a feature of many relational database management systems and other databases such as graph databases. !! Tree decompositions are also called junction trees, clique trees, or join trees; they play an important role in problems like probabilistic inference, constraint satisfaction, query optimization, and matrix decomposition. !! The purpose of query optimization, which is an automated process, is to find the way to process a given query in minimum time. !! Thus query optimization typically tries to approximate the optimum by comparing several common-sense alternatives to provide in a reasonable time a ""good enough"" plan which typically does not deviate much from the best possible result. !! The large possible variance in time justifies performing query optimization, though finding the exact optimal query plan, among all possibilities, is typically very complex, time-consuming by itself, may be too costly, and often practically impossible. !! Query plans for nested SQL queries can also be chosen using the same dynamic programming algorithm as used for join ordering, but this can lead to an enormous escalation in query optimization time."
binary file,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
object code,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
object code optimizer,"An object code optimizer, sometimes also known as a post pass optimizer or, for small sections of code, peephole optimizer, takes the output from a source language compile step - the object code or binary file - and tries to replace identifiable sections of the code with replacement code that is more algorithmically efficient (usually improved speed)."
mathematical logic,"In mathematical logic, a universal quantification is a type of quantifier, a logical constant which is interpreted as ""given any"" or ""for all"". !! Combinatory logic is a notation to eliminate the need for quantified variables in mathematical logic. !! In mathematical logic, a formula of first-order logic is in Skolem normal form if it is in prenex normal form with only universal first-order quantifiers. !! In mathematical logic, Skolem arithmetic is the first-order theory of the natural numbers with multiplication, named in honor of Thoralf Skolem. !! In computer science, an abstract state machine (ASM) is a state machine operating on states that are arbitrary data structures (structure in the sense of mathematical logic, that is a nonempty set together with a number of functions (operations) and relations over the set). !! Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. !! The OR gate is a digital logic gate that implements logical disjunction () from mathematical logic it behaves according to the truth table above. !! Lambda calculus (also written as -calculus) is a formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. !! In universal algebra and mathematical logic, a term algebra is a freely generated algebraic structure over a given signature. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! Categorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic. !! In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. !! The AND gate is a basic digital logic gate that implements logical conjunction () from mathematical logic AND gate behaves according to the truth table above. !! Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. !! In mathematical logic and theoretical computer science, an abstract rewriting system (also (abstract) reduction system or abstract rewrite system; abbreviated ARS) is a formalism that captures the quintessential notion and properties of rewriting systems."
theoretical computer science,"Martin Davis, Ron Sigal, Elaine J. Weyuker, Computability, complexity, and languages: fundamentals of theoretical computer science, 2nd ed. !! The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. !! In theoretical computer science, anonymous recursion is important, as it shows that one can implement recursion without requiring named functions. !! In theoretical computer science, a Markov algorithm is a string rewriting system that uses grammar-like rules to operate on strings of symbols. !! It is a theory in theoretical computer science, under discrete mathematics (a section of mathematics and also of computer science). !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! The Annual ACM Symposium on Theory of Computing (STOC) is an academic conference in the field of theoretical computer science. !! In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e. g. , approximate solutions versus precise ones). !! Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory. !! Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P NP conjecture. !! In mathematical logic and theoretical computer science, an abstract rewriting system (also (abstract) reduction system or abstract rewrite system; abbreviated ARS) is a formalism that captures the quintessential notion and properties of rewriting systems. !! These developments have led to the modern study of logic and computability, and indeed the field of theoretical computer science as a whole. !! Algorithmic information theory (AIT) is a branch of theoretical computer science that concerns itself with the relationship between computation and information of computably generated objects (as opposed to stochastically generated), such as strings or any other data structure. !! In discrete mathematics and theoretical computer science, the rotation distance between two binary trees with the same number of nodes is the minimum number of tree rotations needed to reconfigure one tree into another. !! A counter machine is an abstract machine used in a formal logic and theoretical computer science to model computation. !! In theoretical computer science, communication complexity studies the amount of communication required to solve a problem when the input to the problem is distributed among two or more parties. !! In graph theory and theoretical computer science, the longest path problem is the problem of finding a simple path of maximum length in a given graph. !! In graph theory, the metric k-center or metric facility location problem is a combinatorial optimization problem studied in theoretical computer science."
uniform binary search,"Uniform binary search may be faster on systems where it is inefficient to calculate the midpoint, such as on decimal computers. !! The uniform binary search was developed by A. K. Chandra of Stanford University in 1971. !! Uniform binary search would store the value of 3 as both indices differ from 6 by this same amount. !! Uniform binary search stores, instead of the lower and upper bounds, the difference in the index of the middle element from the current iteration to the next iteration."
decimal computers,"Uniform binary search may be faster on systems where it is inefficient to calculate the midpoint, such as on decimal computers."
relational database,"A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970. !! The term ""relational database"" was invented by E. F. Codd at IBM in 1970. !! Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database. !! A system used to maintain relational databases is a relational database management system (RDBMS). !! Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. !! One well-known definition of what constitutes a relational database system is composed of Codd's 12 rules. !! A Data Mapper is a Data Access Layer that performs bidirectional transfer of data between a persistent data store (often a relational database) and an in-memory data representation (the domain layer). !! Armstrong's axioms are a set of references (or, more precisely, inference rules) used to infer all the functional dependencies on a relational database. !! Dimensional modeling does not necessarily involve a relational database. !! A spatial database is a general-purpose database (usually a relational database) that has been enhanced to include spatial data that represents objects defined in a geometric space, along with tools for querying and analyzing such data. !! A database organized in terms of the relational model is a relational database. !! First normal form (1NF) is a property of a relation in a relational database."
relational databases,"Object databases are different from relational databases which are table-oriented. !! Third normal form (3NF) is a database schema design approach for relational databases which uses normalizing principles to reduce the duplication of data, avoid data anomalies, ensure referential integrity, and simplify data management. !! SQL-92 does not support creating or using table-valued columns, which means that using only the ""traditional relational database features"" (excluding extensions even if they were later standardized) most relational databases will be in first normal form by necessity. !! The problem of database repair is a question about relational databases which has been studied in database theory, and which is a particular kind of data cleansing."
sql systems,Database systems which do not require first normal form are often called no sql systems.
database systems,"Database systems which do not require first normal form are often called no sql systems. !! In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems. !! In computer science, shadow paging is a technique for providing atomicity and durability (two of the ACID properties) in database systems. !! The main research conferences in the area are the ACM Symposium on Principles of Database Systems (PODS) and the International Conference on Database Theory (ICDT)."
artificial neural network,"A residual neural network (ResNet) is an artificial neural network (ANN). !! An artificial neural network consists of a collection of simulated neurons. !! The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. !! In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of Artificial Neural Network(ANN), most commonly applied to analyze visual imagery. !! A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. !! Connectionist expert systems are artificial neural network (ANN) based expert systems where the ANN generates inferencing rules e. g. , fuzzy-multi layer perceptron where linguistic and natural form of inputs are used. !! In 1958, psychologist Frank Rosenblatt invented the perceptron, the first artificial neural network, funded by the United States Office of Naval Research. !! A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. !! Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. !! Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. !! Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks that constitute animal brains. !! This provided more processing power for the development of practical artificial neural networks in the 1980s."
artificial neural networks,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models. !! Artificial imagination, also called synthetic imagination or machine imagination, is defined as the artificial simulation of human imagination by general or special purpose computers or artificial neural networks. !! Self-organizing maps, like most artificial neural networks, operate in two modes: training and mapping. !! Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks. !! This provided more processing power for the development of practical artificial neural networks in the 1980s. !! Spreading activation is a method for searching associative networks, biological and artificial neural networks, or semantic networks. !! The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. !! This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. !! In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. !! Most modern deep learning models are based on artificial neural networks, specifically convolutional neural networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines. !! The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e. g. weights of connections between neurons in artificial neural networks) of the model. !! With respect to other advanced machine learning approaches, such as artificial neural networks, random forests, or genetic programming, learning classifier systems are particularly well suited to problems that require interpretable solutions. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. !! Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. !! Artificial neural networks (ANNs), usually simply called neural networks (NNs), are computing systems inspired by the biological neural networks that constitute animal brains. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function. !! Using Artificial neural networks requires an understanding of their characteristics. !! A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. !! Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Biological neural networks have inspired the design of artificial neural networks, but artificial neural networks are usually not strict copies of their biological counterparts. !! Neurocomputational speech processing is speech processing by artificial neural networks."
processing power,"Computational archaeology may include the use of geographical information systems (GIS), especially when applied to spatial analyses such as viewshed analysis and least-cost path analysis as these approaches are sufficiently computationally complex that they are extremely difficult if not impossible to implement without the processing power of a computer. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! An ASIC based IPS may detect and block denial-of-service attacks because they have the processing power and the granularity to analyze the attacks and act like a circuit breaker in an automated way. !! Single core processors used to be widespread in desktop computers, but as applications demanded more processing power, the slower speed of single core systems became a detriment to performance. !! This provided more processing power for the development of practical artificial neural networks in the 1980s."
simulated neurons,An artificial neural network consists of a collection of simulated neurons.
artificial neurons stochastic transfer functions,"Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights."
stochastic weights,"Stochastic neural networks originating from SherringtonKirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights."
fuzzy sets,"In mathematics, vague sets are an extension of fuzzy sets."
vague set,"proposed the notion of vague sets, where each object is characterized by two different membership functions: a true membership function and a false membership function. !! In mathematics, vague sets are an extension of fuzzy sets. !! Vague Sets"
logic programming,"Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic. !! In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs. !! Logic programming is a programming paradigm which is largely based on formal logic. !! Computational logic has also come to be associated with logic programming, because much of the early work in logic programming in the early 1970s also took place in the Department of Computational Logic in Edinburgh. !! The stable model semantics, which is used to give a semantics to logic programming with negation as failure, can be seen as a simplified form of autoepistemic logic. !! Backward chaining is implemented in logic programming by SLD resolution. !! Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. !! Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain."
logic programming language,"Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain."
logic programming languages,"Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic. !! The first host languages used were logic programming languages, so the field was initially called constraint logic programming. !! In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs."
classification problem,"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. !! In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. !! Empirical risk minimization for a classification problem with a 0-1 loss function is known to be an NP-hard problem even for such a relatively simple class of functions as linear classifiers."
problem space,A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.
output label,A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.
hidden layers,"A convolutional neural network consists of an input layer, hidden layers and an output layer. !! In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has."
backpropagation based artificial neural networks,"In the case of backpropagation based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers the network has."
universal approximation theorem,"If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
continuous function,"If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary."
computational learning theory,"While its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. !! In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms. !! In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. !! computational learning theory, a computation is considered feasible if it can be done in polynomial time. !! Computational learning theory: Survey and selected bibliography. !! In computational learning theory, induction of regular languages refers to the task of learning a formal description (e. g. grammar) of a regular language from a given set of example strings."
learning theory,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms."
time complexity,"In both cases, the time complexity is generally expressed as a function of the size of the input. !! In fact, for showing that a computable function is primitive recursive, it suffices to show that its time complexity is bounded above by a primitive recursive function of the input size. !! In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. !! The time complexity of Prim's algorithm depends on the data structures used for the graph and for ordering the edges by weight, which can be done using a priority queue. !! In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. !! Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. !! An algorithm is said to take linear time, or O(n) time, if its time complexity is O(n). !! The time complexity of operations on the binary search tree is directly proportional to the height of the tree."
system performance,"Performance tuning is the improvement of system performance. !! Cognitive ergonomics studies cognition in work and operational settings, in order to optimize human well-being and system performance."
linear combination,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). !! In the above polynomial interpolations using a linear combination of the given values, the coefficients were determined using the Lagrange method. !! Standardized Coefficients: Each predictor's weight in the linear combination that is the discriminant function. !! In many scenarios, an efficient and convenient polynomial interpolation is a linear combination of the given values, using previously known coefficients."
numerical analysis,"In numerical analysis, inverse iteration (also known as the inverse power method) is an iterative eigenvalue algorithm. !! Techniques for series acceleration are often applied in numerical analysis, where they are used to improve the speed of numerical integration. !! George, J. Alan (1973), ""Nested dissection of a regular finite element mesh"", SIAM Journal on Numerical Analysis, 10 (2): 345363, !! In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset. !! In numerical analysis, an incomplete Cholesky factorization of a symmetric positive definite matrix is a sparse approximation of the Cholesky factorization. !! Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. !! Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. !! In numerical analysis, the Shanks transformation is a non-linear series acceleration method to increase the rate of convergence of a sequence. !! Numerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used. !! Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. !! In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! In numerical analysis the diffuse element method (DEM) or simply diffuse approximation is a meshfree method. !! In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value. !! First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc. !! Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). !! In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor. !! For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated scientific problems; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. !! In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! In numerical analysis, nested dissection is a divide and conquer heuristic for the solution of sparse symmetric systems of linear equations based on graph partitioning. !! In numerical analysis, finite differences are widely used for approximating derivatives, and the term ""finite difference"" is often used as an abbreviation of ""finite difference approximation of derivatives"". !! Rational functions are used in numerical analysis for interpolation and approximation of functions, for example the Pad approximations introduced by Henri Pad. !! Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology."
bernstein basis polynomials,"In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials."
bernstein polynomial,"With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! In the mathematical field of numerical analysis, a Bernstein polynomial is a polynomial that is a linear combination of Bernstein basis polynomials. !! Bernstein polynomials can be generalized to k dimensions the resulting polynomials have the form Bi1(x1) Bi2(x2) . !! A generalized parametric PR-QMF design technique based on Bernstein polynomial approximation. !! In the simplest case only products of the unit interval [0,1] are considered; but, using affine transformations of the line, Bernstein polynomials can also be defined for products [a1, b1] [a2, b2] ."
bernstein polynomials,"With the advent of computer graphics, Bernstein polynomials, restricted to the interval [0, 1], became important in the form of Bzier curves. !! Bernstein polynomials can be generalized to k dimensions the resulting polynomials have the form Bi1(x1) Bi2(x2) . !! In the simplest case only products of the unit interval [0,1] are considered; but, using affine transformations of the line, Bernstein polynomials can also be defined for products [a1, b1] [a2, b2] ."
confidence score,"Pool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner understands the data."
pool-based sampling,"Pool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner understands the data."
propositional logic,"Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. !! While propositional logic can only express facts, autoepistemic logic can express knowledge and lack of knowledge about facts. !! It is also called propositional logic, statement logic, sentential calculus, sentential logic, or sometimes zeroth-order logic. !! The semantics of autoepistemic logic is based on the expansions of a theory, which have a role similar to models in propositional logic. !! This distinguishes it from propositional logic, which does not use quantifiers or relations; in this sense, propositional logic is the foundation of first-order logic. !! Propositional logic may be studied through a formal system in which formulas of a formal language may be interpreted to represent propositions. !! In this sense, propositional logic is the foundation of first-order logic and higher-order logic. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. !! However, all the machinery of propositional logic is included in first-order logic and higher-order logics."
numerical computing programs,"UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account."
computer games,"UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics."
computer programs,"Mathematical induction is an inference rule used in formal proofs, and is the foundation of most correctness proofs for computer programs. !! An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs. !! Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs. !! UserLAnd Technologies is a free and open-source ad-free compatibility layer mobile app that allows Linux distributions, computer programs, computer games and numerical computing programs to run on mobile devices without requiring a root account. !! Trace scheduling is an optimization technique developed by Josh Fisher used in compilers for computer programs. !! In software engineering, version control (also known as revision control, source control, or source code management) is a class of systems responsible for managing changes to computer programs, documents, large web sites, or other collections of information. !! In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. !! In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures."
huffman coding,Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding.
adaptive huffman coding,"This article incorporates public domain material from the NIST document: Black, Paul E. ""adaptive Huffman coding"". !! Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding."
bitwise operation,"On simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. !! Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands. !! While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources. !! In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits. !! The bit shifts are sometimes considered bitwise operations, because they treat a value as a series of bits rather than as a numerical quantity."
bit array,"In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits."
bit string,"In computer programming, a bitwise operation operates on a bit string, a bit array or a binary numeral (considered as a bit string) at the level of its individual bits."
bitwise operations,"On simple low-cost processors, typically, bitwise operations are substantially faster than division, several times faster than multiplication, and sometimes significantly faster than addition. !! Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands. !! Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! In computing, an arithmetic logic unit (ALU) is a combinational digital circuit that performs arithmetic and bitwise operations on integer binary numbers. !! While modern processors usually perform addition and multiplication just as fast as bitwise operations due to their longer instruction pipelines and other architectural design choices, bitwise operations do commonly use less power because of the reduced use of resources."
input operands,Most bitwise operations are presented as two-operand instructions where the result replaces one of the input operands.
gradient descent,"The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. !! The model (e. g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. !! They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i. e. , downhill). !! Moreover, not requiring the computation or approximation of function derivatives makes successive parabolic interpolation a popular alternative to other methods that do require them (such as gradient descent and Newton's method). !! Gradient descent is generally attributed to Cauchy, who first suggested it in 1847. !! In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. !! is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution."
local minimum,In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
differentiable function,In mathematics gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.
local minima,"is convex, all local minima are also global minima, so in this case gradient descent can converge to the global solution."
software engineering,"Behavior trees are a formal, graphical modelling language used primarily in systems and software engineering. !! In software engineering, the mediator pattern defines an object that encapsulates how a set of objects interact. !! In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. !! In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. !! Single and composite or integrated behavior tree forms are both important in the application of behavior trees in systems and software engineering. !! Architecture description languages (ADLs) are used in several disciplines: system engineering, software engineering, and enterprise modelling and engineering. !! In software engineering, dependency injection is a technique in which an object receives other objects that it depends on, called dependencies. !! In software engineering, version control (also known as revision control, source control, or source code management) is a class of systems responsible for managing changes to computer programs, documents, large web sites, or other collections of information. !! In software engineering, the data mapper pattern is an architectural pattern. !! Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. !! In software engineering, the composite pattern is a partitioning design pattern. !! In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! In software engineering, structured analysis (SA) and structured design (SD) are methods for analyzing business requirements and developing specifications for converting practices into computer programs, hardware configurations, and related manual procedures. !! In software engineering, the active record pattern is considered an architectural pattern by some people and as an anti-pattern by some others recently. !! Beginning in the 1960s, software engineering was seen as its own type of engineering. !! They show how the results obtained with a triangulation of SIM and CEM point at new research avenues not only for semiotic engineering and HCI but also for other areas of computer science such as software engineering and programming. !! A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. !! Often one of the biggest problems in software engineering is that the requirements change quickly and the internet-speed development method was created to adapt to this situation. !! A model transformation language in systems and software engineering is a language intended specifically for model transformation. !! Software analysis patterns or analysis patterns in software engineering are conceptual models, which capture an abstraction of a situation that can often be encountered in modelling. !! In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance. !! Software engineering is an engineering approach on a software development of systematics application. !! In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established. !! In software engineering, software durability means the solution ability of serviceability of software and to meet user's needs for a relatively long time. !! Additionally, the development of software engineering was seen as a struggle. !! Software diagnosis supports all branches of software engineering, in particular project management, quality management, risk management as well as implementation and test. !! Meta-process modeling is a type of metamodeling used in software engineering and systems engineering for the analysis and construction of models applicable and useful to some predefined problems. !! In software engineering, a circular dependency is a relation between two or more modules which either directly or indirectly depend on each other to function properly."
software project,Requirements analysis is critical to the success or failure of a systems or software project. !! Coding conventions are only applicable to the human maintainers and peer reviewers of a software project.
functional analysis,"Because many properties of matrices and vectors also apply to functions and operators, numerical linear algebra can also be viewed as a type of functional analysis which has a particular emphasis on practical algorithms. !! Functional requirements analysis will be used as the toplevel functions for functional analysis. !! Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. !! In functional analysis and quantum measurement theory, a positive-operator-valued measure (POVM) is a measure whose values are positive semi-definite operators on a Hilbert space."
big data,"Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data. !! The voice and style of the contentThe use of content intelligence is therefore connected to the science of big data and artificial intelligence. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! Data science is related to data mining, machine learning and big data. !! Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable."
computation time,"Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined."
computationally intractable,"Approximate inference methods make it possible to learn realistic models from big data by trading off computation time for accuracy, when exact learning and inference are computationally intractable. !! The optimal computing budget allocation problem is formulated as a Bayesian Markov decision process(MDP) and is solved by using the dynamic programming (DP) algorithm where the Optimistic knowledge gradient policy is used to solve the computationally intractable of the dynamic programming (DP) algorithm."
deep reinforcement learning,"Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! Deep reinforcement learning has also been applied to many domains beyond games. !! Deep reinforcement learning reached another milestone in 2015 when AlphaGo, a computer program trained with deep RL to play Go, became the first computer Go program to beat a human professional Go player without handicap on a full-sized 1919 board. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning."
deep learning,"Deep learning is a class of machine learning algorithms that:199200 uses multiple layers to progressively extract higher-level features from the raw input. !! In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of Artificial Neural Network(ANN), most commonly applied to analyze visual imagery. !! Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. !! The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. !! With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. !! Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. !! Deep reinforcement learning (deep RL) is a subfield of machine learning that combines reinforcement learning (RL) and deep learning. !! Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. !! AI Bridging Cloud Infrastructure (ABCI) is a planned supercomputer being built at the University of Tokyo for use in artificial intelligence, machine learning, and deep learning. !! Since about 2016, deep learning has emerged as the dominant method for performing accurate articulated body pose estimation. !! In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part."
computer vision,"Object detection has applications in many areas of computer vision, including image retrieval and video surveillance. !! Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Applications of graphical models include causal inference, information extraction, speech recognition, computer vision, decoding of low-density parity-check codes, modeling of gene regulatory networks, gene finding and diagnosis of diseases, and graphical models for protein structure. !! Computational human modeling is an interdisciplinary computational science that links the diverse fields of artificial intelligence, cognitive science, and computer vision with machine learning, mathematics, and cognitive psychology. !! It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! Automated sign language translationGesture recognition can be conducted with techniques from computer vision and image processing. !! Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics. !! Articulated body pose estimation in computer vision is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations. !! Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. !! Geometric feature learning is a technique combining machine learning and computer vision to solve visual tasks. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing. !! In computer vision or natural language processing, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. !! Object recognition technology in the field of computer vision for finding and identifying objects in an image or video sequence."
natural language processing,"Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. !! Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing. !! 1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted ""blocks worlds"" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Structured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. !! Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. !! Recursive neural networks, sometimes abbreviated as RvNNs, have been successful, for instance, in learning sequence and tree structures in natural language processing, mainly phrase and sentence continuous representations based on word embedding. !! The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations. !! Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. !! Natural language processing has its roots in the 1950s. !! In computer vision or natural language processing, document layout analysis is the process of identifying and categorizing the regions of interest in the scanned image of a text document. !! Up to the 1980s, most natural language processing systems were based on complex sets of hand-written rules. !! Lexical analysis is also an important early stage in natural language processing, where text or sound waves are segmented into words and other units. !! Reasoning systems have a wide field of application that includes scheduling, business rule processing, problem solving, complex event processing, intrusion detection, predictive analytics, robotics, computer vision, and natural language processing. !! Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! In natural language processing (NLP), word embedding is a term used for the representation of words for text analysis, typically in the form of a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning."
reinforcement learning,"The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible. !! In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning. !! Interest in learning classifier systems was reinvigorated in the mid 1990s largely due to two events; the development of the Q-Learning algorithm for reinforcement learning, and the introduction of significantly simplified Michigan-style LCS architectures by Stewart Wilson. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. !! Intelligent control is a class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. !! Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e. g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). !! With respect to the field of reinforcement learning, learning automata are characterized as policy iterators. !! Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. !! Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. !! Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning."
neural network,"The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. !! Along with rising interest in neural networks beginning in the mid 1980s, interest grew in deep reinforcement learning, where a neural network is used in reinforcement learning to represent policies or value functions. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. !! In 1981, a report was given on the application of transfer learning in training a neural network on a dataset of images representing letters of computer terminals. !! A Hopfield network (or Ising model of a neural network or IsingLenzLittle model) is a form of recurrent artificial neural network and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974 based on Ernst Ising's work with Wilhelm Lenz on the Ising model. !! The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e. g. the number of hidden unitslayers and layer widthsin a neural network). !! Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. !! A biological neural network is composed of a groups of chemically connected or functionally associated neurons."
compilation error,"Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation. !! Compilation error refers to a state when a compiler fails to compile a piece of computer program source code, either due to errors in the code, or, more unusually, due to errors in the compiler itself. !! A compilation error message often helps programmers debugging the source code. !! Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time. !! However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors."
computer program source code,"Compilation error refers to a state when a compiler fails to compile a piece of computer program source code, either due to errors in the code, or, more unusually, due to errors in the compiler itself."
dynamic compilation,"However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors. !! Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation."
static compilation,"Although the definitions of compilation and interpretation can be vague, generally compilation errors only refer to static compilation and not dynamic compilation."
compilation errors,"Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time. !! However, dynamic compilation can still technically have compilation errors, although many programmers and sources may identify them as run-time errors."
run time,"Therefore, polymorphism is given by subtyping polymorphism as in other languages, and it is also extended in functionality by ad hoc polymorphism at run time. !! The use of optimization software requires that the function f is defined in a suitable programming language and connected at compile or run time to the optimization software. !! In computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows. !! Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time."
javascript v8 engine,"Most just-in-time compilers, such as the Javascript V8 engine, ambiguously refer to compilation errors as syntax errors since they check for them at run time."
program text,"In computer programming, a free-form language is a programming language in which the positioning of characters on the page in program text is insignificant. !! Program transformations may be specified as automated procedures that modify compiler data structures (e. g. abstract syntax trees) representing the program text, or may be specified more conveniently using patterns or templates representing parameterized source code fragments."
pattern calculus,Pattern calculus bases all computation on pattern matching of a very general kind. !! concurrent pattern calculus
pattern matching,"Parsing algorithms often rely on pattern matching to transform strings into syntax trees. !! Early programming languages with pattern matching constructs include COMIT (1957), SNOBOL (1962), Refal (1968) with tree-based pattern matching, Prolog (1972), SASL (1976), NPL (1977), and KRC (1981). !! In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. !! Important examples of compressed data structures include the compressed suffix array and the FM-index, both of which can represent an arbitrary text of characters T for pattern matching. !! Pattern calculus bases all computation on pattern matching of a very general kind. !! Pattern matching sometimes includes support for guards. !! Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i. e. , search and replace)."
concurrent pattern calculus,concurrent pattern calculus
scientific visualization,"One definition is that it's information visualization when the spatial representation (e. g. , the page layout of a graphic design) is chosen, whereas it's scientific visualization when the spatial representation is given. !! In scientific visualization, line integral convolution (LIC) is a technique proposed by Brian Cabral and Leith Leedom to visualize a vector field, such as fluid motion."
vector field,"In scientific visualization, line integral convolution (LIC) is a technique proposed by Brian Cabral and Leith Leedom to visualize a vector field, such as fluid motion."
web caching,"In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching)."
distributed data stores,"In computer science, consistency models are used in distributed systems like distributed shared memory systems or distributed data stores (such as filesystems, databases, optimistic replication systems or web caching)."
distributed information processing,"Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control."
scientific computing,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols. !! Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing field that uses advanced computing capabilities to understand and solve complex problems. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
distributed algorithms,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation. !! One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. !! Distributed algorithms are used in different application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. !! MIT Open Courseware - Distributed Algorithms !! Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing."
distributed search,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation."
resource allocation,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation. !! Analysis of Resource Allocation of Stochastic Diffusion Search."
spanning tree generation,"Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation."
parallel algorithm,"Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. !! The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption."
semantic predicate,A syntactic predicate specifies the syntactic validity of applying a production in a formal grammar and is analogous to a semantic predicate that specifies the semantic validity of applying a production.
semantic predicates,The term syntactic predicate was coined by Parr & Quong and differentiates this form of predicate from semantic predicates (also discussed).
computing machinery,ACM Transactions on Computer Systems is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery.
probability distributions,"In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions. !! In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions."
maximum entropy probability distribution,"Every probability distribution is trivially a maximum entropy probability distribution under the constraint that the distribution has its own entropy. !! In statistics and information theory, a maximum entropy probability distribution has entropy that is at least as great as that of all other members of a specified class of probability distributions."
signal processing,"In signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. !! k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. !! Digital signal processing and analog signal processing are subfields of signal processing. !! While multidimensional signal processing is a subset of signal processing, it is unique in the sense that it deals specifically with data that can only be adequately detailed using more than one dimension. !! Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing. !! Mathematical Morphology and its Application to Signal Processing, J. Serra and Ph. !! Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics. !! Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. !! Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! Analog signal processing is a type of signal processing conducted on continuous analog signals by some analog means (as opposed to the discrete digital signal processing where the signal processing is carried out by a digital process). !! In signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems."
digital filter,"In signal processing, a digital filter is a system that performs mathematical operations on a sampled, discrete-time signal to reduce or enhance certain aspects of that signal. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. !! Program Instructions (software) running on the microprocessor implement the digital filter by performing the necessary mathematical operations on the numbers received from the ADC. !! Digital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters. !! Digital filters can often be made very high order, and are often finite impulse response filters, which allows for linear phase response."
input signal,"The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc."
analog filters,"Digital filters may be more expensive than an equivalent analog filter due to their increased complexity, but they make practical many designs that are impractical or impossible as analog filters."
digital filters,"Digital filters can often be made very high order, and are often finite impulse response filters, which allows for linear phase response."
computational complexity theory,"More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. !! In computability theory and computational complexity theory, an undecidable problem is a decision problem for which it is proved to be impossible to construct an algorithm that always leads to a correct yes-or-no answer. !! In computational complexity theory, the linear search problem is an optimal search problem introduced by Richard E. Bellman and independently considered by Anatole Beck. !! In computational complexity theory, a pseudo-polynomial transformation is a function which maps instances of one strongly NP-complete problem into another and is computable in pseudo-polynomial time. !! This application of abstract machines is related to the subject of computational complexity theory. !! In computational complexity theory and game complexity, a parsimonious reduction is a transformation from one problem to another (a reduction) that preserves the number of solutions. !! In computational complexity theory, a problem refers to the abstract question to be solved. !! One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. !! In computational complexity theory, the maximum satisfiability problem (MAX-SAT) is the problem of determining the maximum number of clauses, of a given Boolean formula in conjunctive normal form, that can be made true by an assignment of truth values to the variables of the formula. !! In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem. !! In computer science, parameterized complexity is a branch of computational complexity theory that focuses on classifying computational problems according to their inherent difficulty with respect to multiple parameters of the input or output. !! In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory. !! A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. !! An algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, that is, T(n) = O(nk) for some positive constant k. Problems for which a deterministic polynomial time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. !! Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. !! Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. !! The set of primitive recursive functions is known as PR in computational complexity theory. !! Algorithmic topology, or computational topology, is a subfield of topology with an overlap with areas of computer science, in particular, computational geometry and computational complexity theory. !! In the computational complexity theory of counting problems, a polynomial-time counting reduction is a type of reduction (a transformation from one problem to another) used to define the notion of completeness for the complexity class P. !! In computational complexity theory, a polynomial-time reduction is a method for solving one problem using another. !! Note that, unlike in computational complexity theory, communication complexity is not concerned with the amount of computation performed by Alice or Bob, or the size of the memory used, as we generally assume nothing about the computational power of either Alice or Bob. !! In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is a polynomial in the numeric value of the input (the largest integer present in the input)but not necessarily in the length of the input (the number of bits required to represent it), which is the case for polynomial time algorithms."
computational models,"In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems."
computational resource,"Computational resources are useful because we can study which problems can be computed in a certain amount of each computational resource. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined. !! As the inputs get bigger, the amount of computational resources needed to solve a problem will increase. !! In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems."
computational problems,"In computational complexity theory, a computational resource is a resource used by some computational models in the solution of computational problems. !! Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
memory space,"The methodology of run-time analysis can also be utilized for predicting other growth rates, such as consumption of memory space. !! The simplest computational resources are computation time, the number of steps necessary to solve a problem, and memory space, the amount of storage needed while solving the problem, but many more complicated resources have been defined."
computational resources,"Graph states are useful in quantum error-correcting codes, entanglement measurement and purification and for characterization of computational resources in measurement based quantum computing models. !! Computational resources are useful because we can study which problems can be computed in a certain amount of each computational resource."
complexity theory,"The set cover problem is a classical question in combinatorics, computer science, operations research, and complexity theory. !! Polynomial-time reductions are frequently used in complexity theory for defining both complexity classes and complete problems for those classes. !! The set of all of the computational problems that can be solved using a certain amount of a certain computational resource is a complexity class, and relationships between different complexity classes are one of the most important topics in complexity theory."
nested loop,"In compiler theory, loop interchange is the process of exchanging the order of two iteration variables used by a nested loop."
compiler theory,"In compiler theory, partial redundancy elimination (PRE) is a compiler optimization that eliminates expressions that are redundant on some but not necessarily all paths through a program. !! In compiler theory, loop optimization is the process of increasing execution speed and reducing the overheads associated with loops. !! In compiler theory, loop interchange is the process of exchanging the order of two iteration variables used by a nested loop. !! In compiler theory, dependence analysis produces execution-order constraints between statements/instructions."
cpu cache,"However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches. !! In computing, data-oriented design is a program optimization approach motivated by efficient usage of the CPU cache, used in video game development. !! Memory part 2: CPU caches an article on lwn. !! The major purpose of loop interchange is to take advantage of the CPU cache when accessing array elements. !! In computer engineering, a tag RAM is used to specify which of the possible memory locations is currently stored in a CPU cache. !! A CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. !! A victim cache is a cache used to hold blocks evicted from a CPU cache upon replacement."
compiler optimization,"In compiler theory, partial redundancy elimination (PRE) is a compiler optimization that eliminates expressions that are redundant on some but not necessarily all paths through a program. !! Like any compiler optimization, loop interchange may lead to worse performance because cache performance is only part of the story. !! In compiler optimization, register allocation is the process of assigning local automatic variables and expression results to a limited number of processor registers. !! In computing, inline expansion, or inlining, is a manual or compiler optimization that replaces a function call site with the body of the called function. !! In computer science, loop inversion is a compiler optimization and loop transformation in which a while loop is replaced by an if block containing a do."
random indexing,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e. g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately. !! Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. !! The TopSig technique extends the random indexing model to produce bit vectors for comparison with the Hamming distance similarity function. !! Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection. !! It can be also verified that random indexing is a random projection technique for the construction of Euclidean spacesi."
computational framework,"Random indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e. g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately."
sparse distributed memory,"Sparse distributed memory (SDM) is a mathematical model of human long-term memory introduced by Pentti Kanerva in 1988 while he was at NASA Ames Research Center. !! Sparse distributed memory is a mathematical representation of human memory, and uses high-dimensional space to help model the large amounts of memory that mimics that of the human neural network. !! The sparse distributed memory system distributes each pattern into approximately one hundredth of the locations, so interference can have detrimental results. !! Genetic memory uses genetic algorithm and sparse distributed memory as a pseudo artificial neural network. !! At the University of Memphis, Uma Ramamurthy, Sidney K. D'Mello, and Stan Franklin created a modified version of the sparse distributed memory system that represents ""realizing forgetting. "" !! Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection."
digital data,"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data."
coding theory,"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. !! In telecommunication, information theory, and coding theory, forward error correction (FEC) or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels."
programming languages,"In contrast to manifold approaches and techniques in software engineering, software diagnosis does not depend on programming languages, modeling techniques, software development processes or the specific techniques used in the various stages of the software development process. !! repeat the cycle until a suitable level of validation is obtained: the computational scientist trusts that the simulation generates adequately realistic results for the system under the studied conditionsSubstantial effort in computational sciences has been devoted to developing algorithms, efficient implementation in programming languages, and validating computational results. !! In some programming languages, the maximum size of the call stack is much less than the space available in the heap, and recursive algorithms tend to require more stack space than iterative algorithms. !! Phase Distinction is a property of programming languages that observe a strict division between types and terms. !! The most common use of code signing is to provide security when deploying; in some programming languages, it can also be used to help prevent namespace conflicts. !! In the classification of programming languages, an applicative programming language is built out of functions applied to arguments. !! Most programming languages consist of instructions for computers. !! Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! In computer programming, run-time type information or run-time type identification (RTTI) is a feature of some programming languages (such as C++, Object Pascal, and Ada) that exposes information about an object's data type at runtime. !! Cognitive dimensions or cognitive dimensions of notations are design principles for notations, user interfaces and programming languages, described by researcher Thomas R. G. Green and furthered researched with Marian Petre. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! Thousands of different programming languages have been created, and more are being created every year. !! Since hardware description languages are not considered to be programming languages by most hardware engineers, hardware refactoring is to be considered a separate field from traditional code refactoring. !! Parsing algorithms for natural language cannot rely on the grammar having 'nice' properties as with manually designed grammars for programming languages. !! In programming languages, ad hoc polymorphism is a kind of polymorphism in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. !! Prior to the release of Deep Learning Studio in January 2017, proficiency in Python, among other programming languages, was essential in developing effective deep learning models. !! He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats. !! Most programming languages have some form of runtime system that provides an environment in which programs run. !! Although the logic has also been studied for its own sake, more broadly, ideas from linear logic have been influential in fields such as programming languages, game semantics, and quantum physics (because linear logic can be seen as the logic of quantum information theory), as well as linguistics, particularly because of its emphasis on resource-boundedness, duality, and interaction. !! There are programmable machines that use a set of specific instructions, rather than general programming languages. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! Short-circuit evaluation, minimal evaluation, or McCarthy evaluation (after John McCarthy) is the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true. !! Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
natural language,"Knowledge representation and reasoning (KRR, KR&R, KR) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. !! Sequence alignments are also used for non-biological sequences, such as calculating the distance cost between strings in a natural language or in financial data. !! Context-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose."
data set,"In density-based clustering, clusters are defined as areas of higher density than the remainder of the data set. !! In Ensemble of Classifier Chains (ECC) several CC classifiers can be trained with random order of chains (i. e. random order of labels) on a random subset of data set."
random subset,In Ensemble of Classifier Chains (ECC) several CC classifiers can be trained with random order of chains (i. e. random order of labels) on a random subset of data set.
binary tree,"In computer science, a ternary search tree is a type of trie (sometimes called a prefix tree) where nodes are arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's limit of two. !! A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root. !! A skew heap (or self-adjusting heap) is a heap data structure implemented as a binary tree."
empty set,"A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root. !! The only non-singleton set with this property is the empty set. !! In mathematics, the power set (or powerset) of a set S is the set of all subsets of S, including the empty set and S itself."
binary trees,"if T1 and T2 are extended binary trees, then denote by T1 T2 the extended binary tree obtained by adding a root r connected to the left to T1 and to the right to T2 by adding edges when these sub-trees are non-empty. !! Because of a combinatorial equivalence between binary trees and triangulations of convex polygons, rotation distance is equivalent to the flip distance for triangulations of convex polygons. !! A more informal way of making the distinction is to say, quoting the Encyclopedia of Mathematics, that ""every node has a left child, a right child, neither, or both"" and to specify that these ""are all different"" binary trees. !! The everyday division of documents into chapters, sections, paragraphs, and so on is an analogous example with n-ary rather than binary trees. !! In computer science and probability theory, a random binary tree is a binary tree selected at random from some probability distribution on binary trees. !! Binary trees labelled this way are used to implement binary search trees and binary heaps, and are used for efficient searching and sorting. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set containing the root."
computer language,"Programming languages are one kind of computer language, and are used in computer programming to implement algorithms. !! The software engineering community uses an architecture description language as a computer language to create a description of a software architecture. !! A general-purpose language is a computer language that is broadly applicable across application domains, and lacks specialized features for a particular domain."
general-purpose language,"A general-purpose language is a computer language that is broadly applicable across application domains, and lacks specialized features for a particular domain."
semi-structured model,"The semi-structured model is a database model where there is no separation between the data and the schema, and the amount of structure used depends on the purpose."
database model,"The most popular example of a database model is the relational model, which uses a table-based format. !! A relational database contains multiple tables, each similar to the one in the ""flat"" database model. !! Three key terms are used extensively in relational database models: relations, attributes, and domains. !! A database model is a type of data model that determines the logical structure of a database. !! The semi-structured model is a database model where there is no separation between the data and the schema, and the amount of structure used depends on the purpose. !! Hierarchical database modelIt is the oldest form of data base model. !! Database design is the organization of data according to a database model."
numerical method,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! In numerical analysis, a numerical method is a mathematical tool designed to solve numerical problems. !! The implementation of a numerical method with an appropriate convergence check in a programming language is called a numerical algorithm."
semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! Thus, a newer alternative is probabilistic latent semantic analysis, based on a multinomial model, which is reported to give better results than standard LSA. !! ""Introduction to Latent Semantic Analysis"" (PDF). !! The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. !! The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches."
latent semantic analysis,"Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. !! The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search. !! ""Introduction to Latent Semantic Analysis"" (PDF)."
memory search,"The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of free recall and memory search."
multinomial model,"Thus, a newer alternative is probabilistic latent semantic analysis, based on a multinomial model, which is reported to give better results than standard LSA."
user queries,"The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches."
pattern recognition,"It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. !! Pattern recognition systems are commonly trained from labeled ""training"" data. !! Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. !! Pattern recognition is the automated recognition of patterns and regularities in data. !! Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics. !! Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration."
structural pattern recognition,"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features."
statistical pattern recognition,Syntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns.
character sets,"Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points. !! Many of the changes were subtle, such as collatable character sets within certain numeric ranges. !! Coded Character Sets, History and Development. !! The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)"
statistical modeling,"Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis."
least-squares support-vector machine,"Least-squares support-vector machines (LS-SVM) for statistics and in statistical modeling, are least-squares versions of support-vector machines (SVM), which are a set of related supervised learning methods that analyze data and recognize patterns, and which are used for classification and regression analysis."
k nearest neighbors,"An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). !! This value is the average of the values of k nearest neighbors."
single nearest neighbor,"If k = 1, then the object is simply assigned to the class of that single nearest neighbor."
classification accuracy,"Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis."
distance metric,"Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis."
Object Oriented Programming,Object Oriented Programming puts the Nouns first and foremost. !! Object oriented programming with ANSI-C. Hanser.
object oriented programming,Object oriented programming with ANSI-C. Hanser.
supervised learning,"A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. !! Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). !! Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. !! A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. !! Transductive support-vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. !! In such situations, semi-supervised learning can be of great practical value. !! Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. !! Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. !! From the perspective of statistical learning theory, supervised learning is best understood. !! In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes. !! Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning. !! In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). !! Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. !! Some supervised learning algorithms require the user to determine certain control parameters. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning."
3d core graphics system,The 3D Core Graphics System (a. k. a. !! The 3D Core Graphics System (or Core) was the first graphical standard to be developed.
logical calculi,"For more, see Other logical calculi below. !! With the tools of first-order logic it is possible to formulate a number of theories, either with explicit axioms or by rules of inference, that can themselves be treated as logical calculi."
relational database theory,"Sixth normal form (6NF) is a term in relational database theory, used in two different ways. !! Kent, W. (1983) A Simple Guide to Five Normal Forms in Relational Database Theory, Communications of the ACM, vol."
relational algebra,"Date and others have defined sixth normal form as a normal form, based on an extension of the relational algebra."
optimal solution,"More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal. !! Stack search is not guaranteed to find the optimal solution to the search problem. !! In computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems."
data mining,"Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e. g. behavior modeling, classification, data mining, regression, function approximation, or game strategy). !! Decision tree learning is a method commonly used in data mining. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Data mining in general and rule induction in detail are trying to create algorithms without human programming but with analyzing existing data structures. !! Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. !! Agent mining is an interdisciplinary area that synergizes multiagent systems with data mining and machine learning. !! Data science is related to data mining, machine learning and big data. !! Data stream mining can be considered a subfield of data mining, machine learning, and knowledge discovery. !! Frequent pattern discovery (or FP discovery, FP mining, or Frequent itemset mining) is part of knowledge discovery in databases, Massive Online Analysis, and data mining; it describes the task of finding the most frequent and relevant patterns in large datasets."
string kernel,"String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be. !! In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length. !! (i. e. data are elements of a vector space), using a string kernel allows the extension of these methods to handle sequence data. !! String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis. !! Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors."
kernel function,"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i. e. finite sequences of symbols that need not be of the same length."
string kernels,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis. !! String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be."
kernelized learning algorithms,"Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors."
text mining,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
gene analysis,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
sequence data,"String kernels are used in domains where sequence data are to be clustered or classified, e. g. in text mining and gene analysis."
classification algorithms,"Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. !! More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms."
data sets,"Since no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. !! In statistics, combinatorial data analysis (CDA) is the study of data sets where the order in which objects are arranged is important. !! Phase correlation is an approach to estimate the relative translative offset between two similar images (digital image correlation) or other data sets."
semantic resolution tree,A semantic resolution tree is a tree used for the definition of the semantics of a programming language.
software design,"One of the main components of software design is the software requirements analysis (SRA). !! A software design description (a. k. a. software design document or SDD; just design document; also Software Design Specification) is a representation of a software design that is to be used for recording design information, addressing various design concerns, and communicating that information to the designs stakeholders. !! In computer programming and software design, code refactoring is the process of restructuring existing computer codechanging the factoringwithout changing its external behavior. !! The hexagonal architecture, or ports and adapters architecture, is an architectural pattern used in software design. !! The principle of least astonishment (POLA), aka principle of least surprise (alternatively a law or rule), applies to user interface and software design. !! In software engineering, a software design pattern is a general, reusable solution to a commonly occurring problem within a given context in software design. !! Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as . !! Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. !! Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints. !! In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design. !! ""Software design usually involves problem-solving and planning a software solution. !! Software design is the process of envisioning and defining software solutions to one or more sets of problems."
user interface,"The benefit of low-key feedback is that it can provide always available indication without cluttering the user interface with explicit indicators such as text labels or indicator lights. !! In the industrial design field of humancomputer interaction, a user interface (UI) is the space where interactions between humans and machines occur. !! The Office Assistant is a discontinued ""intelligent"" user interface for Microsoft Office that assisted users by way of an interactive animated character which interfaced with the Office help content. !! User interfaces are composed of one or more layers, including a human-machine interface (HMI) that interfaces machines with physical input hardware such as keyboards, mice, or game pads, and output hardware such as computer monitors, speakers, and printers. !! Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc. !! The principle of least astonishment (POLA), aka principle of least surprise (alternatively a law or rule), applies to user interface and software design. !! The design considerations applicable when creating user interfaces are related to, or involve such disciplines as, ergonomics and psychology. !! Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i. e. maximum usability). !! Don Norman cited principles of cognitive engineering in his 1981 article, ""The truth about Unix: The user interface is horrid. "" !! In humancomputer interaction, an organic user interface (OUI) is defined as a user interface with a non-flat display. !! The hexagonal architecture divides a system into several loosely-coupled interchangeable components, such as the application core, the database, the user interface, test scripts and interfaces with other systems. !! Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls."
operating systems,"Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers. !! ""Bad command or file name"" is a common and ambiguous error message in MS-DOS and some other operating systems. !! Security-focused operating systems also exist. !! Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. !! Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources. !! The cylinder count of 306 is not conveniently close to any power of 1024; operating systems and programs using the customary binary prefixes show this as 9. !! 2 percent, while other operating systems amount to just 0."
optimization theory,"Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! Robust optimization is a field of optimization theory that deals with optimization problems in which a certain measure of robustness is sought against uncertainty that can be represented as deterministic variability in the value of the parameters of the problem itself and/or its solution. !! In optimization theory, semi-infinite programming (SIP) is an optimization problem with a finite number of variables and an infinite number of constraints, or an infinite number of variables and a finite number of constraints."
max-flow min-cut theorem,"In computer science and optimization theory, the max-flow min-cut theorem states that in a flow network, the maximum amount of flow passing from the source to the sink is equal to the total weight of the edges in a minimum cut, i. e. , the smallest total weight of the edges which if removed would disconnect the source from the sink. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal. !! Max-flow min-cut theorem. !! The other half of the max-flow min-cut theorem refers to a different aspect of a network: the collection of cuts. !! In this new definition, the generalized max-flow min-cut theorem states that the maximum value of an s-t flow is equal to the minimum capacity of an s-t cut in the new sense."
linear programming,"In mathematical optimization, the criss-cross algorithm is any of a family of algorithms for linear programming. !! Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. !! Linear programming can be applied to various fields of study. !! More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. !! A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists. !! Linear programming is a special case of mathematical programming (also known as mathematical optimization). !! Like the simplex algorithm of George B. Dantzig, the criss-cross algorithm is not a polynomial-time algorithm for linear programming. !! The equality in the max-flow min-cut theorem follows from the strong duality theorem in linear programming, which states that if the primal program has an optimal solution, x*, then the dual program also has an optimal solution, y*, such that the optimal values formed by the two solutions are equal."
graph algorithms,"In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth."
vector optimization,"A multi-objective optimization problem is a special case of a vector optimization problem: The objective space is the finite dimensional Euclidean space partially ordered by the component-wise ""less than or equal to"" ordering. !! Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. !! Benson's algorithm for linear vector optimization problems. !! Thus the minimizer of this vector optimization problem are the Pareto efficient points. !! Vector optimization is a subarea of mathematical optimization where optimization problems with a vector-valued objective functions are optimized with respect to a given partial ordering and subject to certain constraints."
data warehouse,"The process of dimensional modeling builds on a 4-step design method that helps to ensure the usability of the dimensional model and the use of the data warehouse. !! The earliest installations using anchor modeling were made in Sweden with the first dating back to 2004, when a data warehouse for an insurance company was built using the technique."
cross-correlation matrix,The cross-correlation matrix is used in various digital signal processing algorithms. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors.
correlation matrix,The cross-correlation matrix is used in various digital signal processing algorithms. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors.
random vectors,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! The cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors."
binary splitting,"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. !! To take full advantage of the scheme, fast multiplication algorithms such as ToomCook and SchnhageStrassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower. !! Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. !! Binary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. !! Since all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing."
optimization mechanism,"determines the type of the network built by the optimization mechanism. !! In network science, the optimization mechanism is a network growth algorithm, which randomly places new nodes in the system, and connects them to the existing nodes based on a cost-benefit analysis. !! Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc. !! Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network. !! The optimization mechanism is a model with growth, in which preferential attachment is valid under certain assumptions."
star network,"Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network."
router networks,"Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc."
probabilistic tests,Many popular primality tests are probabilistic tests.
sample space,"These tests use, apart from the tested number n, some other numbers a which are chosen at random from some sample space; the usual randomized primality tests never report a prime number as composite, but it is possible for a composite number to be reported as prime."
unsupervised learning,"Unsupervised learning is a type of algorithm that learns patterns from untagged data. !! Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). !! The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. !! Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. !! Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e. g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning). !! For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. !! Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods. !! Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. !! Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning."
neural networks,"The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. !! Subsymbolic artificial intelligence is the set of alternative approaches which do not use explicit high level symbols, such as mathematical optimization, statistical classifiers and neural networks. !! It's been generally proven that using methods based on neural networks, vector support machines, statistics, and the nearest neighbors are a great way to do this traffic classification, but in some specific cases some methods are better than others, for example: neural networks work better when the whole observation set is taken into account. !! Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods. !! The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. !! Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as competitive layer. !! Some researchers have achieved ""near-human performance"" on the MNIST database, using a committee of neural networks; in the same paper, the authors achieve performance double that of humans on other recognition tasks."
probabilistic methods,Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods.
maximum likelihood,"In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. !! is called the maximum likelihood estimate. !! so defined is measurable, then it is called the maximum likelihood estimator. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. !! The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference. !! The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate."
supervised method,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
boltzmann learning rule,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
hopfield learning rule,"In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
gibbs sampling,"But the reversible-jump variant is useful when doing Markov chain Monte Carlo or Gibbs sampling over nonparametric Bayesian models such as those involving the Dirichlet process or Chinese restaurant process, where the number of mixing components/clusters/etc. !! In contrast to Supervised method's dominant use of Backpropagation, Unsupervised Learning also employ other methods including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations."
bayesian network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! Efficient algorithms can perform inference and learning in Bayesian networks. !! For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. !! Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! This type of graphical model is known as a directed graphical model, Bayesian network, or belief network. !! Probabilistic relational model a Probabilistic Relational Model (PRM) is the counterpart of a Bayesian network in statistical relational learning."
probabilistic relational model,Probabilistic relational model a Probabilistic Relational Model (PRM) is the counterpart of a Bayesian network in statistical relational learning.
hardware technologies,Software and hardware technologies are used for human presence detection.
sql statements,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
table structure,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
sql dump,A database dump (also: SQL dump) contains a record of the table structure and/or the data from a database and is usually in the form of a list of SQL statements.
database dumps,"Database dumps are often published by free content projects, to allow reuse or forking, as well as local searching of the database using tools such as grep."
optimization problems,"Robust optimization is a field of optimization theory that deals with optimization problems in which a certain measure of robustness is sought against uncertainty that can be represented as deterministic variability in the value of the parameters of the problem itself and/or its solution. !! Online optimization is a field of optimization theory, more popular in computer science and operations research, that deals with optimization problems having no or incomplete knowledge of the future (online). !! In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. !! In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure. !! Vector optimization is a subarea of mathematical optimization where optimization problems with a vector-valued objective functions are optimized with respect to a given partial ordering and subject to certain constraints."
multimedia communication,Universal communication format is a communication protocol developed by the IEEE for multimedia communication.
flux architecture,"To support React's concept of unidirectional data flow (which might be contrasted with AngularJS's bidirectional flow), the Flux architecture was developed as an alternative to the popular modelviewcontroller architecture."
hebbian learning,"A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network."
hidden layer,Competitive Learning is usually implemented with Neural Networks that contain a hidden layer which is commonly known as competitive layer.
database columns,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key.
database relation,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key.
database table,A database relation (e. g. a database table) is said to meet third normal form standards if all the attributes (e. g. database columns) are functionally dependent on solely the primary key. !! A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
input error,"The negative outcome of such a design is that a doctor's number will be duplicated in the database if they have multiple patients, thus increasing both the chance of input error and the cost and risk of updating that number should it change (compared to a third normal form-compliant data model that only stores a doctor's number once on a doctor table)."
database normalization,"Database normalization is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity. !! An Introduction to Database Normalization by Mike Hillyer. !! The third normal form (3NF) is a normal form used in database normalization. !! Database Normalization Basics by Mike Chapple (About. !! The process is progressive, and a higher level of database normalization cannot be achieved unless the previous levels have been satisfied."
supervised machine learning,Similarity learning is an area of supervised machine learning in artificial intelligence.
similarity learning,"For further information on this topic, see the surveys on metric and similarity learning by Bellet et al. !! For this reason, ranking-based similarity learning is easier to apply in real large-scale applications. !! Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. !! Similarity learning is an area of supervised machine learning in artificial intelligence. !! Similarity learning is closely related to distance metric learning."
distance metric learning,Similarity learning is closely related to distance metric learning.
recommendation systems,"Similarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems."
multivariate analysis,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. !! Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis."
linear algebra,"In the mathematical discipline of linear algebra, a triangular matrix is a special kind of square matrix. !! In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication. !! In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces. !! In mathematics, particularly in linear algebra and applications, matrix analysis is the study of matrices and their algebraic properties. !! In linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. !! :911 The stochastic matrix was first developed by Andrey Markov at the beginning of the 20th century, and has found use throughout a wide variety of scientific fields, including probability theory, statistics, mathematical finance and linear algebra, as well as computer science and population genetics. !! In linear algebra, a QR decomposition, also known as a QR factorization or QU factorization, is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm, the QR algorithm. !! M. T. Chu, R. E. Funderlic, R. J. Plemmons, Structured low-rank approximation, Linear Algebra and its Applications, Volume 366, 1 June 2003, Pages 157172 !! In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices. !! Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering. !! Historically, matrix multiplication has been introduced for facilitating and clarifying computations in linear algebra. !! In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. !! Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. !! In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced sh-LES-kee) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e. g. , Monte Carlo simulations. !! In linear algebra, a circulant matrix is a square matrix in which all row vectors are composed of the same elements and each row vector is rotated one element to the right relative to the preceding row vector."
non-negative matrix factorization,"There are different types of non-negative matrix factorizations. !! Also early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the 1990s under the name positive matrix factorization. !! In chemometrics non-negative matrix factorization has a long history under the name ""self modeling curve resolution"". !! In Learning the parts of objects by non-negative matrix factorization Lee and Seung proposed NMF mainly for parts-based decomposition of images. !! Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements."
linear system,"The inverse iteration algorithm requires solving a linear system or calculation of the inverse matrix. !! The Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse. !! In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the GaussSeidel method for solving a linear system of equations, resulting in faster convergence. !! In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables. !! Stationary iterative methods solve a linear system with an operator approximating the original one; and based on a measurement of the error in the result (the residual), form a ""correction equation"" for which this process is repeated."
inverse matrix,The inverse iteration algorithm requires solving a linear system or calculation of the inverse matrix.
artificial neurons,"A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes."
computer security,"In cryptanalysis and computer security, a dictionary attack is an attack using a restricted subset of a keyspace to defeat a cipher or authentication mechanism by trying to determine its decryption key or passphrase, sometimes trying thousands or millions of likely possibilities often obtained from lists of past security breaches. !! A DMA attack is a type of side channel attack in computer security, in which an attacker can penetrate a computer or other device, by exploiting the presence of high-speed expansion ports that permit direct memory access (DMA). !! In computer security, a sandbox is a security mechanism for separating running programs, usually in an effort to mitigate system failures and/or software vulnerabilities from spreading. !! In computer security, digital certificates are verified using a chain of trust. !! In computer security, a chain of trust is established by validating each component of hardware and software from the end entity up to the root certificate."
authentication mechanism,"In cryptanalysis and computer security, a dictionary attack is an attack using a restricted subset of a keyspace to defeat a cipher or authentication mechanism by trying to determine its decryption key or passphrase, sometimes trying thousands or millions of likely possibilities often obtained from lists of past security breaches. !! The efficacy of code signing as an authentication mechanism for software depends on the security of underpinning signing keys."
binary operation,"In mathematics, the Frobenius inner product is a binary operation that takes two matrices and returns a number. !! In mathematics, particularly in linear algebra, matrix multiplication is a binary operation that produces a matrix from two matrices."
tensor product,"Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product !! In linear algebra, the Schmidt decomposition (named after its originator Erhard Schmidt) refers to a particular way of expressing a vector in the tensor product of two inner product spaces."
complex vector spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
vector spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
hilbert spaces,Tensor product of Hilbert spaces the Frobenius inner product is the special case where the vector spaces are finite-dimensional real or complex vector spaces with the usual Euclidean inner product
parent class,"In class-based programming, inheritance is done by defining new classes as extensions of existing classes: the existing class is the parent class and the new class is the child class. !! Composition over inheritance (or composite reuse principle) in object-oriented programming (OOP) is the principle that classes should achieve polymorphic behavior and code reuse by their composition (by containing instances of other classes that implement the desired functionality) rather than inheritance from a base or parent class."
compiler design,"In compiler design, static single assignment form (often abbreviated as SSA form or simply SSA) is a property of an intermediate representation (IR), which requires that each variable be assigned exactly once, and every variable be defined before it is used."
recursive relation,"The master theorem allows many recurrence relations of this form to be converted to -notation directly, without doing an expansion of the recursive relation."
regularization methods,"Linear Regularization Methods""."
linear regularization methods,"Linear Regularization Methods""."
register allocation,"Register allocation consists therefore in choosing where to store the variables at runtime, i. e. inside or outside registers. !! Register allocation can happen over a basic block (local register allocation), over a whole function/procedure (global register allocation), or across function boundaries traversed via call-graph (interprocedural register allocation). !! In compiler optimization, register allocation is the process of assigning local automatic variables and expression results to a limited number of processor registers. !! Register allocation raises several problems that can be tackled (or avoided) by different register allocation approaches. !! Many register allocation approaches optimize for one or more specific categories of actions."
computational system,"The computational theory of mind holds that the mind is a computational system that is realized (i. e. physically implemented) by neural activity in the brain. !! When used in this manner, the counter machine is used to model the discrete time-steps of a computational system in relation to memory accesses. !! The SKI combinator calculus is a combinatory logic system and a computational system."
formal knowledge representation languages,Description logics (DL) are a family of formal knowledge representation languages.
description logics,"There are many varieties of description logics and there is an informal naming convention, roughly describing the operators allowed. !! Description logics (DL) are a family of formal knowledge representation languages."
mathematical constructors,"There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors."
fuzzy description logics,"There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors."
randomization function,"So, in practice one often uses randomization functions that are derived from pseudo-random number generators, preferably seeded with external ""random"" data such as the program's startup time. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! In theory, randomization functions are assumed to be truly random, and yield an unpredictably different function every time the algorithm is executed. !! The randomization technique would not work if, at every execution of the algorithm, the randomization function always performed the same mapping, or a mapping entirely determined by some externally observable parameter (such as the program's startup time)."
randomizing function,"In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm."
randomized algorithm,"In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator. !! In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm. !! A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. !! Computational complexity theory models randomized algorithms as probabilistic Turing machines. !! Randomized algorithms are particularly useful when faced with a malicious ""adversary"" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. !! The most basic randomized complexity class is RP, which is the class of decision problems for which there is an efficient (polynomial time) randomized algorithm (or probabilistic Turing machine) which recognizes NO-instances with absolute certainty and recognizes YES-instances with a probability of at least 1/2."
binary dependent variable,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In addition, linear regression may make nonsensical predictions for a binary dependent variable."
logistic function,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function."
logistic regression,"In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Outputs with more than two values are modeled by multinomial logistic regression and, if the multiple categories are ordered, by ordinal logistic regression (for example the proportional odds ordinal logistic model). !! The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier. !! Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. !! In a binary logistic regression model, the dependent variable has two levels (categorical)."
logistic model,"In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression). !! Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. !! In the logistic model, the log-odds (the logarithm of the odds) for the value labeled ""1"" is a linear combination of one or more independent variables (""predictors""); the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). !! Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass/fail which is represented by an indicator variable, where the two values are labeled ""0"" and ""1"". !! In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc."
independent variable,"Analogous models with a different sigmoid function instead of the logistic function can also be used, such as the probit model; the defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio."
learning automaton,"A learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. !! A learning automaton is one type of machine learning algorithm studied since 1970s. !! However, the term learning automaton was not used until Narendra and Thathachar introduced it in a survey paper in 1974. !! A visualised demo / Art Work of a single Learning Automaton had been developed by Systems (microSystems) Research Group at Newcastle University."
autonomic networking,"Generic Autonomic Networking Architecture (GANA) EFIPSANS Project http://www. !! Instead of a layering approach, autonomic networking targets a more flexible structure termed compartmentalization. !! Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001. !! Autonomic networking at the core of enterprise Wan governance blog !! A fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking."
autonomic computing,"Autonomic Networking follows the concept of Autonomic Computing, an initiative started by IBM in 2001."
control theory,"Many linear dynamical system tests in control theory, especially those related to controllability and observability, involve checking the rank of the Krylov subspace. !! Fuzzy logic has been applied to many fields, from control theory to artificial intelligence. !! Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. !! A fundamental concept of Control theory, the closed control loop, is among the fundamental principles of autonomic networking."
computer architecture,"In computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a tag section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to. !! In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation. !! In computer architecture, Amdahl's law (or Amdahl's argument) is a formula which gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved. !! In computer architecture, the memory hierarchy separates computer storage into a hierarchy based on response time. !! Dataflow architecture is a computer architecture that directly contrasts the traditional von Neumann architecture or control flow architecture. !! In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. !! The earliest computer architectures were designed on paper and then directly built into the final hardware form. !! Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints. !! In computer architecture, cache coherence is the uniformity of shared resource data that ends up stored in multiple local caches. !! The first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine."
data structures,"Algorithmic information theory principally studies complexity measures on strings (or other data structures). !! The term compressed data structure arises in the computer science subfields of algorithms, data structures, and theoretical computer science. !! Data structures serve as the basis for abstract data types (ADT). !! The predecessor problem is a simple case of the nearest neighbor problem, and data structures that solve it have applications in problems like integer sorting. !! The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra. !! When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. !! The range searching problem and the data structures that solve it are a fundamental topic of computational geometry. !! Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. !! Usually, efficient data structures are key to designing efficient algorithms. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Algorithms and data structures are central to computer science. !! Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. !! Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! WADS, the Algorithms and Data Structures Symposium, is an international academic conference in the field of computer science, focusing on algorithms and data structures. !! The term ""generic programming"" was originally coined by David Musser and Alexander Stepanov in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above. !! Forward declaration is used in languages that require declaration before use; it is necessary for mutual recursion in such languages, as it is impossible to define such functions (or data structures) without a forward reference in one definition: one of the functions (respectively, data structures) must be defined first."
objective function,"An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc. !! More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution. !! Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable)."
quadratic function,"More generally, if the objective function is not a quadratic function, then many optimization methods use other methods to ensure that some subsequence of iterations converges to an optimal solution."
conformational analysis,Nonlinear optimization methods are widely used in conformational analysis.
statistical mechanics,"Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing. !! In statistical mechanics, the two-dimensional square lattice Ising model is a simple lattice model of interacting magnetic spins. !! In physics, maximum entropy thermodynamics (colloquially, MaxEnt thermodynamics) views equilibrium thermodynamics and statistical mechanics as inference processes. !! Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."
physical memory,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
virtual address space,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
microsoft windows application programming interface,"Address Windowing Extensions (AWE) is a Microsoft Windows application programming interface that allows a 32-bit software application to access more physical memory than it has virtual address space, even in excess of the 4 GB limit."
encryption keys,"An article published in Dr. Dobb's Journal in 2004 noted that memory allocated using Address Windowing Extensions will not be written to the pagefile, and suggested that AWE regions could therefore be used as a way of protecting sensitive application data such as encryption keys."
multimodal architecture,"The Multimodal Architecture and Interfaces recommendation introduces a generic structure and a communication protocol to allow the modules in a multimodal system to communicate with each other. !! Multimodal Architecture and Interfaces is an open standard developed by the World Wide Web Consortium since 2005. !! Papers presented to W3C's Multimodal Architecture and Interfaces Workshop, 1920 July 2004. !! The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller. !! Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need."
multimodal system,The Multimodal Architecture and Interfaces recommendation introduces a generic structure and a communication protocol to allow the modules in a multimodal system to communicate with each other. !! Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
runtime framework,Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
main functions,Multimodal Architecture and Interfaces is the specified description of a larger services infrastructure called The Runtime Framework which provides the main functions that a multimodal system can need.
user interface structure,"The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller."
mvc design pattern,"The Multimodal Architecture and Interfaces specification is based on the MVC design pattern, that proposes to organize the user interface structure in three parts: the Model, the View and the Controller."
orthogonal decomposition,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! Weiss, Julien: A Tutorial on the Proper Orthogonal Decomposition. !! Applications of the Proper Orthogonal Decomposition Method http://www. !! As its name hints, it's operating an Orthogonal Decomposition along with the Principal Components of the field."
computational fluid dynamics,"The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations). !! In some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e. g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used."
structural analysis,"Structural analysis is thus a key part of the engineering design of structures. !! Advanced structural analysis may examine dynamic response, stability and non-linear behavior. !! Structural analysis is the determination of the effects of loads on physical structures and their components. !! Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure's deformations, internal forces, stresses, support reactions, accelerations, and stability. !! It is common practice to use approximate solutions of differential equations as the basis for structural analysis. !! The proper orthogonal decomposition is a numerical method that enables a reduction in the complexity of computer intensive simulations such as computational fluid dynamics and structural analysis (like crash simulations)."
scalar processor,"The Cortex-M7, like many consumer CPUs today, is a superscalar processor. !! A scalar processor is classified as a single instruction, single data (SISD) processor in Flynn's taxonomy. !! Scalar processors are a class of computer processors that process only one data item at a time. !! A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor. !! The Intel 486 is an example of a scalar processor."
scalar processors,Scalar processors are a class of computer processors that process only one data item at a time.
computer processors,Scalar processors are a class of computer processors that process only one data item at a time. !! In computer processors the carry flag (usually indicated as the C flag) is a single bit in a system status register/flag register used to indicate when an arithmetic carry or borrow has been generated out of the most significant arithmetic logic unit (ALU) bit position.
superscalar processor,"A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor. !! The Cortex-M7, like many consumer CPUs today, is a superscalar processor."
clock cycle,A superscalar processor (such as the Intel P5) may execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to redundant functional units on the processor.
singleton pattern,"In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance."
Singleton pattern,"In essence, the singleton pattern forces it to be responsible for ensuring that it is only instantiated once. !! In software engineering, the singleton pattern is a software design pattern that restricts the instantiation of a class to one ""single"" instance. !! How to navigate the deceptively simple Singleton pattern."
source programs,Semantic dictionary encoding (SDE) preserves the full semantic context of source programs while adding further information that can be used for accelerating the speed of code generation.
linear function,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
multilevel sampling system,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
adaptive predictive coding,Adaptive predictive coding (APC) is a narrowband analog-to-digital conversion that uses a one-level or multilevel sampling system in which the value of the signal at each sampling instant is predicted according to a linear function of the past values of the quantized signals.
telecommunications network,Active networking is a communication pattern that allows packets flowing through a telecommunications network to dynamically modify the operation of the network.
active networking,"Active networking is a communication pattern that allows packets flowing through a telecommunications network to dynamically modify the operation of the network. !! Active networking relates to other networking paradigms primarily based upon how computing and communication are partitioned in the architecture. !! The use of real-time genetic algorithms within the network to compose network services is also enabled by active networking. !! Active networking allows the possibility of highly tailored and rapid ""real-time"" changes to the underlying network operation. !! Network processors are one means of implementing active networking concepts."
network processors,Network processors are one means of implementing active networking concepts.
medical imaging,"This makes computational electromagnetics (CEM) important to the design, and modeling of antenna, radar, satellite and other communication systems, nanophotonic devices and high speed silicon electronics, medical imaging, cell-phone antenna design, among other applications. !! Medical imaging seeks to reveal internal structures hidden by the skin and bones, as well as to diagnose and treat disease. !! Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! Medical imaging also establishes a database of normal anatomy and physiology to make it possible to identify abnormalities. !! Lossless JPEG has some popularity in medical imaging, and is used in DNG and some digital cameras to compress raw images, but otherwise was never widely adopted. !! Although imaging of removed organs and tissues can be performed for medical reasons, such procedures are usually considered part of pathology instead of medical imaging. !! Medical imaging is the technique and process of imaging the interior of a body for clinical analysis and medical intervention, as well as visual representation of the function of some organs or tissues (physiology). !! In a limited comparison, these technologies can be considered forms of medical imaging in another discipline."
interpolative decomposition,"In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value."
identity matrix,"In numerical analysis, interpolative decomposition (ID) factors a matrix as the product of two matrices, one of which contains selected columns from the original matrix, and the other of which has a subset of columns consisting of the identity matrix and all its values are no greater than 2 in absolute value."
multimedia data,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc."
data stream clustering,"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. !! Data stream clustering has recently attracted attention for emerging applications that involve large amounts of streaming data. !! Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time."
streaming algorithm,"In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). !! There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing. !! In computing, a one-pass algorithm or single-pass algorithm is a streaming algorithm which reads its input exactly once. !! Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145, !! Though streaming algorithms had already been studied by Munro and Paterson as early as 1978, as well as Philippe Flajolet and G. Nigel Martin in 1982/83, the field of streaming algorithms was first formalized and popularized in a 1996 paper by Noga Alon, Yossi Matias, and Mario Szegedy. !! Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time. !! For this paper, the authors later won the Gdel Prize in 2005 ""for their foundational contribution to streaming algorithms. """
streaming data,Data stream clustering has recently attracted attention for emerging applications that involve large amounts of streaming data.
processor time,"Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources."
cost allocation,"Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources."
web servers,"Operating systems are found on many devices that contain a computer from cellular phones and video game consoles to web servers and supercomputers. !! Key aspects of service-oriented infrastructure include industrialisation and virtualisation, providing IT infrastructure services via a pool of resources (web servers, application servers, database servers, servers, storage instances) instead of through discrete instances."
binary search trees,"Binary search trees are also efficacious in sorting algorithms and search algorithms. !! Binary search trees are used in implementing priority queues, using the element or node's key as priorities. !! The geometry of binary search trees has been used to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal."
binary search tree algorithm,"The geometry of binary search trees has been used to provide an algorithm which is dynamically optimal if any binary search tree algorithm is dynamically optimal. !! The binary search tree algorithm was discovered independently by several researchers, including P. F. Windley, Andrew Donald Booth, Andrew Colin, Thomas N. Hibbard, and attributed to Conway Berners-Lee and David Wheeler, in 1960 for storing labeled data in magnetic tapes."
data modeling,"Data modeling is a process used to define and analyze data requirements needed to support the business processes within the scope of corresponding information systems in organizations. !! Data modeling defines not just data elements, but also their structures and the relationships between them. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. !! The data-flow diagram is a tool that is part of structured analysis and data modeling. !! The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details."
information system,"Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! Therefore, the process of data modeling involves professional data modelers working closely with business stakeholders, as well as potential users of the information system. !! A data-flow diagram is a way of representing a flow of data through a process or a system (usually an information system)."
data model,"A hierarchical database model is a data model in which the data are organized into a tree-like structure. !! Data modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques. !! A database model is a type of data model that determines the logical structure of a database. !! A data model (or datamodel) is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. !! A data model explicitly determines the structure of data. !! The term data model can refer to two distinct but closely related concepts. !! So the ""data model"" of a banking application may be defined using the entity-relationship ""data model"". !! For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner."
logical data model,"The logical data model of vector graphics is based on the mathematics of coordinate geometry, in which shapes are defined as a set of points in a two- or three-dimensional cartesian coordinate system, as p = (x, y) or p = (x, y, z). !! The last step in data modeling is transforming the logical data model to a physical data model that organizes the data into tables, and accounts for access, performance and storage details."
data elements,"Data Structure Diagram is a diagram type that is used to depict the structure of data elements in the data dictionary. !! Data modeling defines not just data elements, but also their structures and the relationships between them."
graph embedding,"This article deals only with the strict definition of graph embedding. !! Some authors define a weaker version of the definition of ""graph embedding"" by omitting the non-intersection condition for edges. !! In such contexts the stricter definition is described as ""non-crossing graph embedding""."
application programming interfaces,"Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system. !! :815Both types of automated personal assistant technology are enabled by the combination of mobile computing devices, application programming interfaces (APIs), and the proliferation of mobile apps."
code libraries,"Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system."
software frameworks,"The designers of software frameworks aim to facilitate software developments by allowing designers and programmers to devote their time to meeting software requirements rather than dealing with the more standard low-level details of providing a working system, thereby reducing overall development time. !! According to Pree, software frameworks consist of frozen spots and hot spots. !! Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system. !! The elegance issue is why relatively few software frameworks have stood the test of time: the best frameworks have been able to evolve gracefully as the underlying technology on which they were built advanced. !! Software frameworks rely on the Hollywood Principle: ""Don't call us, we'll call you. """
mutable array,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
resizable array,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
dynamic table,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
random access,"Elements in a sorted array can be looked up by their index (random access) at O(1) time, an operation taking O(log n) or O(n) time for more complex data structures. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
array list,"In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed."
dynamic array,"The elements of the dynamic array are stored contiguously at the start of the underlying array, and the remaining positions towards the end of the underlying array are reserved, or unused. !! In computer science, a dynamic array, growable array, resizable array, dynamic table, mutable array, or array list is a random access, variable-size list data structure that allows elements to be added or removed. !! Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation. !! A simple dynamic array can be constructed by allocating an array of fixed-size, typically larger than the number of elements immediately required. !! A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
static arrays,"Dynamic arrays overcome a limit of static arrays, which have a fixed capacity that needs to be specified at allocation."
back end,"A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
dynamically allocated array,"A dynamic array is not the same thing as a dynamically allocated array, which is an array whose size is fixed when the array is allocated, although a dynamic array may use such a fixed-size array as a back end."
swarm intelligence algorithms,This is a chronologically ordered list of metaphor-based metaheuristics and swarm intelligence algorithms. !! Recent work has involved merging the global search properties of SDS with other swarm intelligence algorithms.
bcjr algorithm,"The BCJR algorithm is an algorithm for maximum a posteriori decoding of error correcting codes defined on trellises (principally convolutional codes). !! The online textbook: Information Theory, Inference, and Learning Algorithms, by David J. C. MacKay, discusses the BCJR algorithm in chapter 25. !! Susa framework implements BCJR algorithm for forward error correction codes and channel equalization in C++. !! The implementation of BCJR algorithm in Susa signal processing framework"
recurrent neural network,"Recurrent neural networks were based on David Rumelhart's work in 1986. !! The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. !! A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. !! Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. !! The echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). !! Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. !! The Echo State Network (ESN) belongs to the Recurrent Neural Network (RNN) family and provide their architecture and supervised learning principle."
numpy bindings,"Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing."
matlab code,"Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing."
autonomous operation,"Another feature of the ESN is the autonomous operation in prediction: if the Echo State Network is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input."
signal generation,"Another feature of the ESN is the autonomous operation in prediction: if the Echo State Network is trained with an input that is a backshifted version of the output, then it can be used for signal generation/prediction by using the previous output as input."
recurrent neural networks,"Recurrent neural networks were based on David Rumelhart's work in 1986. !! Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. !! ""Long Short-Term Memory in Recurrent Neural Networks"" (PDF). !! Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. !! Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. !! Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014."
fully recurrent neural networks,Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.
gated recurrent units,"Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. !! Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014."
discrete time recurrent neural networks,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations."
differential equations,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. !! Bifurcation theory is the mathematical study of changes in the qualitative or topological structure of a given family of curves, such as the integral curves of a family of vector fields, and the solutions of a family of differential equations. !! In continuous simulation, continuously changing state variables of a system are modeled by differential equations. !! Continuous Simulation refers to simulation approaches where a system is modeled with the help of variables that change continuously according to a set of differential equations. !! The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems. !! It is common practice to use approximate solutions of differential equations as the basis for structural analysis. !! In both differential equations in continuous time and difference equations in discrete time, initial conditions affect the value of the dynamic variables (state variables) at any future time."
shannon sampling theorem,"Note that, by the Shannon sampling theorem, discrete time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations."
leaf node,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes. !! Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only to visit the root last (i. e. , they first access the children of the root, but only access the value of the root last). !! of the latter case form the relation (L) (<H) which is a partial map that assigns each non-leaf node its first child node. !! Similarly, (L+) (>H) assigns each non-leaf node with finitely many children its last child node."
external node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
terminal node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
outer node,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
child nodes,"Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes."
leaf nodes,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only to visit the root last (i. e. , they first access the children of the root, but only access the value of the root last)."
root node,"The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. !! Possibly the easiest operation for the randomized meldable heap, FindMin() simply returns the element currently stored in the heap's root node. !! In computer science, a heap is a specialized tree-based data structure which is essentially an almost complete tree that satisfies the heap property: in a max heap, for any given node C, if P is a parent node of C, then the key (the value) of P is greater than or equal to the key of C. In a min heap, the key of P is less than or equal to the key of C. The node at the ""top"" of the heap (with no parents) is called the root node."
cognitive architecture,"The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990. !! He included more aspects of his research on long-term memory and thinking processes into this research and eventually designed a cognitive architecture he eventually called ACT. !! The Institute for Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together in conjunction with knowledge and skills embodied within the architecture to yield intelligent behavior in a diversity of complex environments. "" !! Successful cognitive architectures include ACT-R (Adaptive Control of Thought - Rational) and SOAR. !! A cognitive architecture refers to both a theory about the structure of the human mind and to a computational instantiation of such a theory used in the fields of artificial intelligence (AI) and computational cognitive science."
cognitive architectures,"Cognitive architectures form a subset of general agent architectures. !! The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990. !! Cognitive architectures can be symbolic, connectionist, or hybrid. !! Successful cognitive architectures include ACT-R (Adaptive Control of Thought - Rational) and SOAR. !! The software used to implement the cognitive architectures were also ""cognitive architectures""."
cognitive theories,The research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990.
artificial systems,"The Institute for Creative Technologies defines cognitive architecture as: ""hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together in conjunction with knowledge and skills embodied within the architecture to yield intelligent behavior in a diversity of complex environments. """
rayleigh quotient,Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates.
computational neural network models,Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.
quantum neural networks,"Most Quantum neural networks are developed as feed-forward networks. !! Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. !! Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. !! Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics."
quantum mechanics,"""Interpretations of Quantum Mechanics"" by Peter J. Lewis. !! A uniqueness theorem for interpretations of quantum mechanics. !! Others, like Nico van Kampen and Willis Lamb, have openly criticized non-orthodox interpretations of quantum mechanics. !! Relational quantum mechanics (RQM) is an interpretation of quantum mechanics which treats the state of a quantum system as being observer-dependent, that is, the state is the relation between the observer and the system. !! Relational quantum mechanics arose from a comparison of the quandaries posed by the interpretations of quantum mechanics with those resulting from Lorentz transformations prior to the development of special relativity. !! Stochastic quantum mechanics (or the stochastic interpretation) is an interpretation of quantum mechanics. !! In quantum mechanics, and especially quantum information and the study of open quantum systems, the trace distance T is a metric on the space of density matrices and gives a measure of the distinguishability between two states. !! Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. !! Time-symmetric interpretations of quantum mechanics were first suggested by Walter Schottky in 1921. !! In quantum mechanics, a density matrix is a matrix that describes the quantum state of a physical system. !! Modal interpretations of quantum mechanics were first conceived of in 1972 by Bas van Fraassen, in his paper ""A formal approach to the philosophy of science. """
quantum information,"The term quantum information theory is also used, but it fails to encompass experimental research, and can be confused with a subfield of quantum information science that addresses the processing of quantum information. !! However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms."
classical computer,"Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data."
quantum computer,"Superconducting quantum computing is an implementation of a quantum computer in superconducting electronic circuits. !! Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data. !! A quantum sort is any sorting algorithm that runs on a quantum computer. !! Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. !! Quantum programming is the process of assembling sequences of instructions, called quantum programs, that are capable of running on a quantum computer."
quantum data,"Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data."
algorithmic design,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior."
classical backpropagation rule,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior."
training set,"Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. !! Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. !! The point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes."
model predictive control,High-level controllers such as model predictive control (MPC) or real-time optimization (RTO) employ mathematical optimization.
axiomatic set theory,"In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set."
constraint programming,"Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. !! In constraint programming, users declaratively state the constraints on the feasible solutions for a set of decision variables. !! Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. !! Constraint programming is an embedding of constraints in a host language. !! The constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time."
logic program,"Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program."
constraint logic programming,"Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. !! Today most Prolog implementations include one or more libraries for constraint logic programming. !! The first host languages used were logic programming languages, so the field was initially called constraint logic programming. !! The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP."
stochastic control,"Robert Merton used stochastic control to study optimal portfolios of safe and risky assets. !! Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system. !! Stochastic control aims to design the time path of the controlled variables that performs the desired control task with minimum cost, somehow defined, despite the presence of this noise. !! An extremely well-studied formulation in stochastic control is that of linear quadratic Gaussian control. !! The field of stochastic control has developed greatly since the 1970s, particularly in its applications to finance."
stochastic optimal control,Stochastic control or stochastic optimal control is a sub field of control theory that deals with the existence of uncertainty either in observations or in the noise that drives the evolution of the system.
linear quadratic gaussian control,An extremely well-studied formulation in stochastic control is that of linear quadratic Gaussian control.
convolutional neural networks,"Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. !! Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling. !! On the other hand, neural techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN). !! The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. !! Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition. !! The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources."
spatial relations,"Furthermore, convolutional neural networks are ideal for data with a grid-like topology (such as images) as spatial relations between separate features are taken into account during convolution and/or pooling."
computing resources,"The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources."
feedback connections,The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections.
neural abstraction pyramid,The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections.
generative model,The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data.
helmholtz machine,"Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field). !! The Helmholtz machine (named after Hermann von Helmholtz and his concept of Helmholtz free energy) is a type of artificial neural network that can account for the hidden structure of a set of data by being trained to create a generative model of the original set of data. !! A Helmholtz machine contains two networks, a bottom-up recognition network that takes the data as input and produces a distribution over hidden variables, and a top-down ""generative"" network that generates values of the hidden variables and the data itself. !! Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm. !! At the time, Helmholtz machines were one of a handful of learning architectures that used feedback as well as feedforward to ensure quality of learned models."
helmholtz machines,"Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm. !! At the time, Helmholtz machines were one of a handful of learning architectures that used feedback as well as feedforward to ensure quality of learned models."
unsupervised learning algorithm,"Helmholtz machines are usually trained using an unsupervised learning algorithm, such as the wake-sleep algorithm."
character recognition,"Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field)."
supervised learning algorithm,"A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. !! Helmholtz machines may also be used in applications requiring a supervised learning algorithm (e. g. character recognition, or position-invariant recognition of an object within a field). !! This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm. !! In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. !! Some supervised learning algorithms require the user to determine certain control parameters. !! A wide range of supervised learning algorithms are available, each with its strengths and weaknesses."
electronic signature,"While an electronic signature can be as simple as a name entered in an electronic document, digital signatures are increasingly used in e-commerce and in regulatory filings to implement electronic signatures in a cryptographically protected way. !! An electronic signature is intended to provide a secure and accurate identification method for the signatory to provide a seamless transaction. !! Definitions of electronic signatures vary depending on the applicable jurisdiction. !! Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures. !! An electronic signature, or e-signature, is data that is logically associated with other data and which is used by the signatory to sign the associated data."
electronic signatures,"Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures."
digital signatures,"Electronic signatures are a legal concept distinct from digital signatures, a cryptographic mechanism often used to implement electronic signatures. !! a small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)Cryptographic hash functions have many information-security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. !! While an electronic signature can be as simple as a name entered in an electronic document, digital signatures are increasingly used in e-commerce and in regulatory filings to implement electronic signatures in a cryptographically protected way."
implicit euler method,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations."
backward euler method,"In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) is one of the most basic numerical methods for the solution of ordinary differential equations. !! The backward Euler method has order one. !! and the formula for the backward Euler method follows. !! The region of absolute stability for the backward Euler method is the complement in the complex plane of the disk with radius 1 centered at 1, depicted in the figure. !! The backward Euler method has error of order one in time."
computational statistics,"), ""Special Section: Teaching Computational Statistics"", The American Statistician, 58: 1, !! The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models. !! This marks the beginning of the era of mechanized computational statistics and semiautomatic data processing systems. !! Algorithms for calculating variance play a major role in computational statistics. !! Though computational statistics is widely used today, it actually has a relatively short history of acceptance in the statistics community. !! Computational statistics, or statistical computing, is the bond between statistics and computer science. !! Computational Statistics & Data Analysis is a monthly peer-reviewed scientific journal covering research on and applications of computational statistics and data analysis."
statistical computing,"Computational statistics, or statistical computing, is the bond between statistics and computer science."
generalized additive models,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
kernel density estimation,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
local regression,"The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models."
semiautomatic data processing systems,This marks the beginning of the era of mechanized computational statistics and semiautomatic data processing systems.
single core cpu,A computer using a single core CPU is generally slower than a multi-core system.
desktop computers,"Single core processors used to be widespread in desktop computers, but as applications demanded more processing power, the slower speed of single core systems became a detriment to performance."
single core processors,Single core processors are still in use in some niche circumstances.
raspberry pi,Single core processors also used in hobbyist computers like the Raspberry Pi and Single-board microcontrollers.
nm matrix,"In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
computer memory,"Similarly, since objects in computer memory are not inherently sequential, and may include links to other objects (including self-referential links), XML data binding mappings often have difficulty preserving all the information about an object when it is marshalled to XML. !! A sorted array is an array data structure in which each element is sorted in numerical, alphabetical, or some other order, and placed at equally spaced addresses in computer memory. !! In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM. !! Random-access memory (RAM; ) is a form of computer memory that can be read and changed in any order, typically used to store working data and machine code. !! Memory management is a form of resource management applied to computer memory. !! XML data binding refers to a means of representing information in an XML document as a business object in computer memory."
in-place matrix transposition,"The following briefly summarizes the published algorithms to perform in-place matrix transposition. !! In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
situ matrix transposition,"In-place matrix transposition, also called in-situ matrix transposition, is the problem of transposing an NM matrix in-place in computer memory, ideally with O(1) (bounded) additional storage, or at most with additional storage much less than NM."
parity function,"Parity function: their value is 1 if the input vector has odd number of onesThe n-ary versions of AND, OR, XOR, NAND, NOR and XNOR are also symmetric Boolean functions."
distributed artificial intelligence,"The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents). !! Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition. !! In 1975 distributed artificial intelligence emerged as a subfield of artificial intelligence that dealt with interactions of intelligent agents[2]. !! Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. !! Distributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems."
decision making problems,"Distributed Artificial Intelligence (DAI) is an approach to solving complex learning, planning, and decision making problems."
autonomous processing nodes,"The objectives of Distributed Artificial Intelligence are to solve the reasoning, planning, learning and perception problems of artificial intelligence, especially if they require large data, by distributing the problem to autonomous processing nodes (agents)."
distributed artificial intelligence systems,"Distributed artificial intelligence systems were conceived as a group of intelligent entities, called agents, that interacted by cooperation, by coexistence or by competition."
network elements,"Streaming media is multimedia that is delivered and consumed in a continuous manner from a source, with little or no intermediate storage in network elements."
streaming media,"Streaming media is multimedia that is delivered and consumed in a continuous manner from a source, with little or no intermediate storage in network elements. !! The term ""streaming media"" can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered ""streaming text"". !! Other early companies that created streaming media technology include RealNetworks (originally known as Progressive Networks) and Protocomm both prior to widespread World Wide Web usage. !! Practical streaming media was only made possible with advances in data compression, due to the impractically high bandwidth requirements of uncompressed media. !! Microsoft developed a media player known as ActiveMovie in 1995 that supported streaming media and included a proprietary streaming format, which was the precursor to the streaming feature later in Windows Media Player 6."
streaming text,"The term ""streaming media"" can apply to media other than video and audio, such as live closed captioning, ticker tape, and real-time text, which are all considered ""streaming text""."
progressive networks,Other early companies that created streaming media technology include RealNetworks (originally known as Progressive Networks) and Protocomm both prior to widespread World Wide Web usage.
computer science fields,"There has since been a large body of work centered around data streaming algorithms that spans a diverse spectrum of computer science fields such as theory, databases, networking, and natural language processing."
data streaming algorithms,"Lall, Ashwin; Sekar, Vyas; Ogihara, Mitsunori; Xu, Jun; Zhang, Hui (2006), ""Data streaming algorithms for estimating entropy of network traffic"", Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS 2006) (PDF), p. 145,"
subject-oriented programming,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches. !! The introduction of aspect-oriented programming in 1997 raised questions about its relationship to subject-oriented programming, and about the difference between subjects and aspects. !! Subject-oriented programming advocates the organization of the classes that describe objects into ""subjects"", which may be composed to form larger subjects. !! In the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension. !! In computing, subject-oriented programming is an object-oriented software paradigm in which the state (fields) and behavior (methods) of objects are not seen as intrinsic to the objects themselves, but are provided by various subjective perceptions (""subjects"") of the objects."
method call,"In the presentation of subject-oriented programming, the join-points were deliberately restricted to field access and method call on the grounds that those were the points at which well-designed frameworks were designed to admit functional extension."
composition filters,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches."
feature oriented programming,"Like aspect-oriented programming, subject-oriented programming, composition filters, feature oriented programming and adaptive methods are considered to be aspect-oriented software development approaches."
boolean algebra,"Boolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854). !! Instead of elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction (and) denoted as , the disjunction (or) denoted as , and the negation (not) denoted as . !! According to Huntington, the term ""Boolean algebra"" was first suggested by Sheffer in 1913, although Charles Sanders Peirce gave the title ""A Boolean Algebra with One Constant"" to the first chapter of his ""The Simplest Mathematics"" in 1880. !! In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference. !! Boolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. !! A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
de morgan's law,"De Morgan's laws are an example of a more general concept of mathematical duality. !! De Morgan's laws commonly apply to text searching using Boolean operators AND, OR, and NOT. !! In set notation, De Morgan's laws can be remembered using the mnemonic ""break the line, change the sign"". !! De Morgan's laws are normally shown in the compact form above, with the negation of the output on the left and negation of the inputs on the right. !! In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference."
transformation rules,"In propositional logic and Boolean algebra, De Morgan's laws are a pair of transformation rules that are both valid rules of inference."
mathematical duality,De Morgan's laws are an example of a more general concept of mathematical duality.
instance-based learning,"One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. !! In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. !! Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
kernel machines,"Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
rbf networks,"Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks."
systems architecture,"Systems architecture depends heavily on practices and techniques which were developed over thousands of years in many other fields, perhaps the most important being civil architecture. !! A systems architecture makes use of elements of both software and hardware and is used to enable design of such a composite system. !! What is Systems Architecture !! Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. !! Systems Architecture: Canaxia Brings an Architect on Board, Article !! Journal of Systems Architecture"
reactive programming,"On the other hand, in reactive programming, the value of a is automatically updated whenever the values of b or c change, without the program having to explicit re-execute the statement a := b + c to determine the presently assigned value of a. !! Reactive programming has been proposed as a way to simplify the creation of interactive user interfaces and near-real-time system animation. !! Another example is a hardware description language such as Verilog, where reactive programming enables changes to be modeled as they propagate through circuits. !! In computing, reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change. !! For example, in a modelviewcontroller (MVC) architecture, reactive programming can facilitate changes in an underlying model that are reflected automatically in an associated view."
hardware description language,"Another example is a hardware description language such as Verilog, where reactive programming enables changes to be modeled as they propagate through circuits."
doubly linked list,"In computer science, a doubly linked list is a linked data structure that consists of a set of sequentially linked records called nodes. !! In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. !! Any node of a doubly linked list, once obtained, can be used to begin a new traversal of the list, in either direction (towards beginning or end), from the given node. !! The link fields of a doubly linked list node are often called next and previous or forward and backward. !! While adding or removing a node in a doubly linked list requires changing more links than the same operations on a singly linked list, the operations are simpler and potentially more efficient (for nodes other than first nodes) because there is no need to keep track of the previous node during traversal or no need to traverse the list to find the previous node, so that its link can be modified. !! The first and last nodes of a doubly linked list are immediately accessible (i. e. , accessible without traversal, and usually called head and tail) and therefore allow traversal of the list from the beginning or end of the list, respectively: e. g. , traversing the list from beginning to end, or from end to beginning, in a search of the list for a node with specific data value."
Node Link,"In a 'doubly linked list', each node contains, besides the next-node link, a second link field pointing to the 'previous' node in the sequence. !! With this convention, an empty list consists of the sentinel node alone, pointing to itself via the next-node link."
synchronous optical networking,Remote error indication (REI) or formerly far end block error (FEBE) is an alarm signal used in synchronous optical networking (SONET).
matrix operations,"Numerical linear algebra, sometimes called applied linear algebra, is the study of how matrix operations can be used to create computer algorithms which efficiently and accurately provide approximate answers to questions in continuous mathematics."
bayes network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
probabilistic graphical model,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables."
bayes net,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
decision network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG)."
belief network,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! This type of graphical model is known as a directed graphical model, Bayesian network, or belief network."
directed acyclic graph,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). !! If the graph is a directed acyclic graph (DAG), topological orderings are pre-topological orderings and vice versa."
bayesian networks,"Efficient algorithms can perform inference and learning in Bayesian networks. !! A generalization of the Viterbi algorithm, termed the max-sum algorithm (or max-product algorithm) can be used to find the most likely assignment of all or some subset of latent variables in a large number of graphical models, e. g. Bayesian networks, Markov random fields and conditional random fields. !! Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. !! Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams."
probabilistic relationships,"For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms."
speech signals,"This article is about Compressed sensing in speech signals. !! Bayesian networks that model sequences of variables (e. g. speech signals or protein sequences) are called dynamic Bayesian networks. !! The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. !! Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. !! Speech processing is the study of speech signals and the processing methods of signals."
proxy pattern,"Proxy pattern description from the Portland Pattern Repository !! In computer programming, the proxy pattern is a software design pattern. !! The lazy loading demonstrated in this example is not part of the proxy pattern, but is merely an advantage made possible by the use of the proxy. !! Using the proxy pattern, the code of the ProxyImage avoids multiple loading of the image, accessing it from the other system in a memory-saving manner."
artificial bee colony algorithm,"In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Dervi Karaboa (Erciyes University) in 2005."
sorting algorithm,"Among the authors of early sorting algorithms around 1951 was Betty Holberton, who worked on ENIAC and UNIVAC. !! For 20 years, merge-insertion sort was the sorting algorithm with the fewest comparisons known for all input lengths. !! For typical serial sorting algorithms, good behavior is O(n log n), with parallel sort in O(log2 n), and bad behavior is O(n2). !! In computer science, a sorting algorithm is an algorithm that puts elements of a list into an order. !! A quantum sort is any sorting algorithm that runs on a quantum computer. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted). !! Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the length of the range of possible key values (N) are approximately the same. !! A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a ""less than or equal to"" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list."
computer code,Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code.
code stylometry,"Code stylometry (also known as program authorship attribution or source code authorship analysis) is the application of stylometry to computer code to attribute authorship to anonymous binary or source code. !! Unlike software forensics, code stylometry attributes authorship for purposes other than intellectual property infringement, including plagiarism detection, copyright investigation, and authorship verification."
copyright investigation,"Unlike software forensics, code stylometry attributes authorship for purposes other than intellectual property infringement, including plagiarism detection, copyright investigation, and authorship verification."
pseudorandom number generators,"However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators. !! Other higher-quality PRNGs, both in terms of computational and statistical performance, were developed before and after this date; these can be identified in the List of pseudorandom number generators. !! Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility. !! Random number are generated by Javascript pseudorandom number generators (PRNGs) algorithms !! Uses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling."
javascript pseudorandom number generators,Random number are generated by Javascript pseudorandom number generators (PRNGs) algorithms
hash tables,"There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. !! Double hashing is a computer programming technique used in conjunction with open addressing in hash tables to resolve hash collisions, by using a secondary hash of the key as an offset when a collision occurs. !! Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, allow significantly faster searching for all but short lists. !! Linear probing is a scheme in computer programming for resolving collisions in hash tables, data structures for maintaining a collection of keyvalue pairs and looking up the value associated with a given key. !! As Thorup & Zhang (2012) write, ""Hash tables are the most commonly used nontrivial data structures, and the most popular implementation on standard hardware uses linear probing, which is both fast and simple. """
enhanced double hashing,"(a tetrahedral number), does solve the problem, a technique known as enhanced double hashing."
euclidean distance,"For qubits, the trace distance is equal to half the Euclidean distance in the Bloch representation."
abstract data type,"An abstract data type is defined by its behavior (semantics) from the point of view of a user, of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. !! The term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings. !! The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development. !! In computer science, a mergeable heap (also called a meldable heap) is an abstract data type, which is a heap supporting a merge operation. !! In computer science, an abstract data type (ADT) is a mathematical model for data types. !! An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics. !! Abstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages."
abstract graphical data type,An abstract graphical data type (AGDT) is an extension of an abstract data type for computer graphics.
program errors,Unanticipated arithmetic overflow is a fairly common cause of program errors.
maximum-margin hyperplane,"In the context of support-vector machines, the optimally separating hyperplane or maximum-margin hyperplane is a hyperplane which separates two convex hulls of points and is equidistant from the two."
distributed computing environment,"The Distributed Computing Environment is a component of the OSF offerings, along with Motif, OSF/1 and the Distributed Management Environment (DME). !! In a distributed computing environment, distributed object communication realizes communication between distributed objects. !! In computing, the Distributed Computing Environment (DCE) software system was developed in the early 1990s from the work of the Open Software Foundation (OSF), a consortium (founded in 1988) that included Apollo Computer (part of Hewlett-Packard from 1989), IBM, Digital Equipment Corporation, and others. !! By integrating security, RPC and other distributed services on a single ""official"" distributed computing environment, OSF could offer a major advantage over SVR4, allowing any DCE-supporting system (namely OSF/1) to interoperate in a larger network."
distributed object communication,"The server side object participating in distributed object communication is known as a skeleton (or stub; term avoided here). !! In a distributed computing environment, distributed object communication realizes communication between distributed objects. !! The client side object participating in distributed object communication is known as a stub or proxy, and is an example of a proxy object."
proxy object,"The client side object participating in distributed object communication is known as a stub or proxy, and is an example of a proxy object."
structural induction,"The structural induction proof is a proof that the proposition holds for all the minimal structures and that if it holds for the immediate substructures of a certain structure S, then it must hold for S also. !! Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out. !! A structural induction proof of some proposition P(L) then consists of two parts: A proof that P([]) is true and a proof that if P(L) is true for some list L, and if L is the tail of list M, then P(M) must also be true. !! Structural induction is a proof method that is used in mathematical logic (e. g. , in the proof of o' theorem), computer science, graph theory, and some other mathematical fields. !! Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction."
structural recursion,"Structural recursion is usually proved correct by structural induction; in particularly easy cases, the inductive step is often left out. !! Structural recursion is a recursion method bearing the same relationship to structural induction as ordinary recursion bears to ordinary mathematical induction."
internet service providers,"An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
internet routing registry,"The Internet routing registry works by providing an interlinked hierarchy of objects designed to facilitate the organization of IP routing between organizations, and also to provide data in an appropriate format for automatic programming of routers. !! An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
internet route objects,"An Internet Routing Registry (IRR) is a database of Internet route objects for determining, and sharing route and related information used for configuring routers, with a view to avoiding problematic issues between Internet service providers."
ip routing,"The Internet routing registry works by providing an interlinked hierarchy of objects designed to facilitate the organization of IP routing between organizations, and also to provide data in an appropriate format for automatic programming of routers."
microsoft sql server,"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems."
information dimension,"It is shown that information dimension and differential entropy are tightly connected. !! then we are doing exactly the same quantization as the definition of information dimension. !! The information dimension of a distribution gives a theoretical upper bound on the compression rate, if one wants to compress a variable coming from this distribution. !! In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! In 2010, Wu and Verd gave an operational characterization of Rnyi information dimension as the fundamental limit of almost lossless data compression for analog sources under various regularity constraints of the encoder/decoder."
euclidean space,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors. !! In computational geometry, a sweep line algorithm or plane sweep algorithm is an algorithmic paradigm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space."
normalized entropy,"In information theory, information dimension is an information measure for random vectors in Euclidean space, based on the normalized entropy of finely quantized versions of the random vectors."
distribution-based clustering,Distribution-based clustering produces complex models for clusters that can capture correlation and dependence between attributes.
meldable heap,"In computer science, a mergeable heap (also called a meldable heap) is an abstract data type, which is a heap supporting a merge operation."
computational linguistics,"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. !! Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. !! The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics."
automatic speech recognition,"It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT)."
text processing,"In text processing, a proximity search looks for documents where two or more separately matching term occurrences are within a specified distance, where distance is the number of intermediate words or characters. !! Speech recognition applications include voice user interfaces such as voice dialing (e. g. ""call home""), call routing (e. g. ""I would like to make a collect call""), domotic appliance control, search key words (e. g. find a podcast where particular words were spoken), simple data entry (e. g. , entering a credit card number), preparation of structured documents (e. g. a radiology report), determining speaker characteristics, speech-to-text processing (e. g. , word processors or emails), and aircraft (usually termed direct voice input)."
robbinsmonro optimization algorithm,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models."
stochastic gradient descent,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models. !! This can perform significantly better than ""true"" stochastic gradient descent described, because the code can make use of vectorization libraries rather than computing each step separately as was first shown in where it was called ""the bunch-mode back-propagation algorithm"". !! To economize on the computational cost at every iteration, stochastic gradient descent samples a subset of summand functions at every step. !! The model (e. g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. !! The convergence of stochastic gradient descent has been analyzed using the theories of convex minimization and of stochastic approximation. !! While the basic idea behind stochastic approximation can be traced back to the RobbinsMonro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning. !! Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e. g. differentiable or subdifferentiable)."
stochastic gradient langevin dynamics,"Stochastic gradient Langevin dynamics (SGLD) is an optimization technique composed of characteristics from Stochastic gradient descent, a RobbinsMonro optimization algorithm, and Langevin dynamics, a mathematical extension of molecular dynamics models."
self-balancing binary search tree,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! These operations when designed for a self-balancing binary search tree, contain precautionary measures against boundlessly increasing tree height, so that these abstract data structures receive the attribute ""self-balancing"". !! In computer science, a self-balancing binary search tree (BST) is any node-based binary search tree that automatically keeps its height (maximal number of levels below the root) small in the face of arbitrary item insertions and deletions. !! This is the case for many binary search trees, such as the AVL trees and the redblack trees the latter was called symmetric binary B-tree and was renamed; it can, however, still be confused with the generic concept of self-balancing binary search tree because of the initials. !! Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues."
avl trees,"This is the case for many binary search trees, such as the AVL trees and the redblack trees the latter was called symmetric binary B-tree and was renamed; it can, however, still be confused with the generic concept of self-balancing binary search tree because of the initials. !! 720 times the worst-case height of RB trees, so AVL trees are more rigidly balanced. !! AVL trees can be colored redblack, thus are a subset of RB trees."
abstract data structures,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! :299-302 Binary search trees are also a fundamental data structure used in construction of abstract data structures such as sets, multisets, and associative arrays."
associative arrays,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! They can be used to implement several other common abstract data types, including lists, stacks, queues, associative arrays, and S-expressions, though it is not uncommon to implement those data structures directly without using a linked list as the basis. !! :299-302 Binary search trees are also a fundamental data structure used in construction of abstract data structures such as sets, multisets, and associative arrays."
priority queues,"From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. !! To improve performance, priority queues are typically based on a heap, giving O(log n) performance for inserts and removals, and O(n) to build the heap initially from a set of n elements. !! Stacks and queues are different than priority queues. !! While coders often implement priority queues with heaps, they are conceptually distinct from heaps. !! The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets. !! Self-balancing binary search trees can be used in a natural way to construct and maintain ordered lists, such as priority queues. !! This operation and its O(1) performance is crucial to many applications of priority queues."
mutable ordered lists,"The self-balancing binary search trees provide efficient implementations for mutable ordered lists, and can be used for other abstract data structures such as associative arrays, priority queues and sets."
digital signal,"In electronics, a digital-to-analog converter (DAC, D/A, D2A, or D-to-A) is a system that converts a digital signal into an analog signal. !! Simple digital signals represent information in discrete bands of analog levels. !! In a digital signal, the physical quantity representing the information may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etcetera. !! A digital signal is a signal that represents data as a sequence of discrete values; at any given time it can only take on, at most, one of a finite number of values. !! As a result, digital signals have noise immunity; electronic noise, provided it is not too great, will not affect digital circuits, whereas noise always degrades the operation of analog signals to some degree. !! Digital signals having more than two states are occasionally used; circuitry using such signals is called multivalued logic. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
behavioral software design pattern,The state pattern is a behavioral software design pattern that allows an object to alter its behavior when its internal state changes.
strategy pattern,"The state pattern can be interpreted as a strategy pattern, which is able to switch a strategy through invocations of methods defined in the pattern's interface."
polymorphic association,"Polymorphic association is a term used in discussions of Object-Relational Mapping with respect to the problem of representing in the relational database domain, a relationship from one class to multiple classes."
relational database domain,"Polymorphic association is a term used in discussions of Object-Relational Mapping with respect to the problem of representing in the relational database domain, a relationship from one class to multiple classes."
ordered sets,"In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices."
ram usage,"In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources."
lightweight software,"In computing, lightweight software also called lite program and lightweight application, is a computer program that is designed to have a small memory footprint (RAM usage) and low CPU usage, overall a low usage of system resources."
weight-balanced tree,"A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. !! Join: The function Join is on two weight-balanced trees t1 and t2 and a key k and will return a tree containing all elements in t1, t2 as well as k. It requires k to be greater than all keys in t1 and smaller than all keys in t2. !! Several set operations have been defined on weight-balanced trees: union, intersection and set difference. !! Weight-balanced trees are popular in the functional programming community and are used to implement sets and maps in MIT Scheme, SLIB and implementations of Haskell. !! With the new operations, the implementation of weight-balanced trees can be more efficient and highly-parallelizable."
binary search tree,"AA trees are a variation of the redblack tree, a form of binary search tree which supports efficient addition and deletion of entries. !! The performance of a binary search tree is dependent on the order of insertion of the nodes into the tree; several variations of the binary search tree can be built with guaranteed worst-case performance. !! A weight-balanced tree is a binary search tree that stores the sizes of subtrees in the nodes. !! In computer science, a ternary search tree is a type of trie (sometimes called a prefix tree) where nodes are arranged in a manner similar to a binary search tree, but with up to three children rather than the binary tree's limit of two. !! In computer science, an optimal binary search tree (Optimal BST), sometimes called a weight-balanced binary tree, is a binary search tree which provides the smallest possible search time (or expected search time) for a given sequence of accesses (or access probabilities). !! In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree. !! The binary search tree algorithm was discovered independently by several researchers, including P. F. Windley, Andrew Donald Booth, Andrew Colin, Thomas N. Hibbard, and attributed to Conway Berners-Lee and David Wheeler, in 1960 for storing labeled data in magnetic tapes. !! Tango trees work by partitioning a binary search tree into a set of preferred paths, which are themselves stored in auxiliary trees (so the tango tree is represented as a tree of trees). !! Binary search trees allow binary search for fast lookup, addition, and removal of data items, and can be used to implement dynamic sets and lookup tables. !! The time complexity of operations on the binary search tree is directly proportional to the height of the tree."
function join,"Join: The function Join is on two weight-balanced trees t1 and t2 and a key k and will return a tree containing all elements in t1, t2 as well as k. It requires k to be greater than all keys in t1 and smaller than all keys in t2."
meta learning,"Meta learning is originally described by Donald B. Maudsley (1979) as ""the process by which learners become aware of and increasingly in control of habits of perception, inquiry, learning, and growth that they have internalized"". !! Meta learning can be defined as an awareness and understanding of the phenomenon of learning itself as opposed to subject knowledge. !! Five principles were enunciated to facilitate meta learning. !! Meta learning is a branch of metacognition concerned with learning about one's own learning and learning processes. !! The idea of meta learning was later used by John Biggs (1985) to describe the state of ""being aware of and taking control of one's own learning""."
pooling layers,"The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. !! Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. !! Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. !! This is followed by other layers such as pooling layers, fully connected layers, and normalization layers. !! Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers."
fully connected layers,"This is followed by other layers such as pooling layers, fully connected layers, and normalization layers."
normalization layers,"This is followed by other layers such as pooling layers, fully connected layers, and normalization layers."
traditional convolutional layers,Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers.
neuron clusters,Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer.
feature maps,"Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value."
binary arithmetic,"Fractions in binary arithmetic terminate only if 2 is the only prime factor in the denominator. !! The full title of Leibniz's article is translated into English as the ""Explanation of Binary Arithmetic, which uses only the characters 1 and 0, with some remarks on its usefulness, and on the light it throws on the ancient Chinese figures of Fu Xi"". !! In 1937, Claude Shannon produced his master's thesis at MIT that implemented Boolean algebra and binary arithmetic using electronic relays and switches for the first time in history."
adjacency list,"An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighbouring vertices or edges. !! In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex. !! Each unordered list within an adjacency list describes the set of neighbors of a particular vertex in the graph. !! This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges."
finite graph,"In graph theory and computer science, an adjacency list is a collection of unordered lists used to represent a finite graph. !! In graph theory and computer science, an adjacency matrix is a square matrix used to represent a finite graph."
neighbouring vertices,An adjacency list representation for a graph associates each vertex in the graph with the collection of its neighbouring vertices or edges.
adjacent vertices,"This version of the adjacency list uses more memory than the version in which adjacent vertices are listed directly, but the existence of explicit edge objects allows it extra flexibility in storing additional information about edges."
adjacency list data structure,The main operation performed by the adjacency list data structure is to report a list of the neighbors of a given vertex.
modern decision theory,The origins of robust optimization date back to the establishment of modern decision theory in the 1950s and the use of worst case analysis and Wald's maximin model as a tool for the treatment of severe uncertainty.
maximin model,It was shown that the stability radius model is an instance of Wald's maximin model. !! The origins of robust optimization date back to the establishment of modern decision theory in the 1950s and the use of worst case analysis and Wald's maximin model as a tool for the treatment of severe uncertainty.
classification criteria,There are a number of classification criteria for robust optimization problems/models.
maximin models,Modern robust optimization deals primarily with non-probabilistic models of robustness that are worst case oriented and as such usually deploy Wald's maximin models.
probabilistic models,Modern robust optimization deals primarily with non-probabilistic models of robustness that are worst case oriented and as such usually deploy Wald's maximin models.
software construction,"Software construction is a software engineering discipline. !! In order to account for the unanticipated gaps in the software design, during software construction some design modifications must be made on a smaller or larger scale to flesh out details of the software design. !! Linguistic notations which are distinguished in particular by the use of word-like strings of text to represent complex software constructions, and the combination of such word-like strings into patterns that have a sentence-like syntax."
chi-square automatic interaction detection,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
bonferroni testing,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
decision tree technique,"Chi-square automatic interaction detection (CHAID) is a decision tree technique, based on adjusted significance testing (Bonferroni testing)."
augmented reality,"Commercial augmented reality experiences were first introduced in entertainment and gaming businesses. !! Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory. !! In this way, augmented reality alters one's ongoing perception of a real-world environment, whereas virtual reality completely replaces the user's real-world environment with a simulated one. !! The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. !! Augmented reality is related to two largely synonymous terms: mixed reality and computer-mediated reality. !! (2002), ""Exploring Humanistic Intelligence Through Physiologically Mediated Reality"" (PDF), Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR'02) 0-7695-1781-1/02, IEEE"
compiler optimization technique,Loop splitting is a compiler optimization technique. !! Bounds-checking elimination is a compiler optimization technique that eliminates unneeded bounds checking.
loop splitting,More generalised loop splitting was added in GCC 7. !! Loop splitting is a compiler optimization technique. !! Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
loop peeling,Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
loop body,Loop peeling is a special case of loop splitting which splits any problematic first (or last) few iterations from the loop and performs them outside of the loop body.
generalised loop splitting,More generalised loop splitting was added in GCC 7.
application programming interface,"Interface-based programming defines the application as a collection of components, in which Application Programming Interface (API) calls between components may only be made through abstract interfaces, not concrete classes. !! Portable Distributed Objects (PDO) is an application programming interface (API) for creating object-oriented code that can be executed remotely on a network of computers."
portable distributed objects,Portable Distributed Objects (PDO) is an application programming interface (API) for creating object-oriented code that can be executed remotely on a network of computers.
category theory,"In category theory, a subobject classifier is a special object of a category such that, intuitively, the subobjects of any object X in the category correspond to the morphisms from X to . !! Categorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic."
truth value object,"Therefore, a subobject classifier is also known as a ""truth value object"" and the concept is widely used in the categorical description of logic."
computer network architecture,Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity.
heterogeneous networks,Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity.
delay-tolerant networking,"The Delay-Tolerant Networking Research Group. !! This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking. !! In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym. !! Delay-tolerant networking (DTN) is an approach to computer network architecture that seeks to address the technical issues in heterogeneous networks that may lack continuous network connectivity."
ipn design,"In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym."
terrestrial networks,"In 2002, Kevin Fall started to adapt some of the ideas in the IPN design to terrestrial networks and coined the term delay-tolerant networking and the DTN acronym."
traditional computer networking,"This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking."
classic ad hoc,"This field saw many optimizations on classic ad hoc and delay-tolerant networking algorithms and began to examine factors such as security, reliability, verifiability, and other areas of research that are well understood in traditional computer networking."
hashing problem,Static Hashing is another form of the hashing problem which allows users to perform lookups on a finalized dictionary set (all objects in the dictionary are final and not changing).
static hashing,"Static Hashing is another form of the hashing problem which allows users to perform lookups on a finalized dictionary set (all objects in the dictionary are final and not changing). !! Since static hashing requires that the database, its objects and reference remain the same its applications are limited."
informatics techniques,"Biodiversity informatics is the application of informatics techniques to biodiversity information, such as taxonomy, biogeography or ecology."
biodiversity informatics,"Biodiversity informatics is the application of informatics techniques to biodiversity information, such as taxonomy, biogeography or ecology. !! Biodiversity informatics contrasts with ""bioinformatics"", which is often used synonymously with the computerized handling of data in the specialized area of molecular biology. !! Biodiversity informatics (different but linked to bioinformatics) is the application of information technology methods to the problems of organizing, accessing, visualizing and analyzing primary biodiversity data. !! Biodiversity informatics may also have to cope with managing information from unnamed taxa such as that produced by environmental sampling and sequencing of mixed-field samples. !! Biodiversity informatics is a term that was only coined around 1992 but with rapidly increasing data sets has become useful in numerous studies and applications, such as the construction of taxonomic databases or geographic information systems."
taxonomic databases,"Biodiversity informatics is a term that was only coined around 1992 but with rapidly increasing data sets has become useful in numerous studies and applications, such as the construction of taxonomic databases or geographic information systems."
processing unit,Measurement of the six degrees of freedom is accomplished today through both AC and DC magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit.
tree contraction,"Parallel tree contraction was introduced by Gary L. Miller and John H. Reif, and has subsequently been modified to improve efficiency by X. !! We now show the evaluation can be done with parallel tree contraction. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
parallel solution,"In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms."
parallel graph algorithms,"In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms."
parallel tree contraction,"Parallel tree contraction was introduced by Gary L. Miller and John H. Reif, and has subsequently been modified to improve efficiency by X. !! We now show the evaluation can be done with parallel tree contraction. !! In computer science, parallel tree contraction is a broadly applicable technique for the parallel solution of a large number of tree problems, and is used as an algorithm design technique for the design of a large number of parallel graph algorithms. !! Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
maximal subtree isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
graph isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
tree isomorphism,"Tree contraction has been used in designing many efficient parallel algorithms, including expression evaluation, finding lowest common ancestors, tree isomorphism, graph isomorphism, maximal subtree isomorphism, common subexpression elimination, computing the 3-connected components of a graph, and finding an explicit planar embedding of a planar graphBased on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic."
moravec's paradox,"Explanation of the XKCD comic about Moravec's paradox !! Moravec's paradox is the observation by artificial intelligence and robotics researchers that, contrary to traditional assumptions, reasoning requires very little computation, but sensorimotor and perception skills require enormous computational resources."
basic function,This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition.
eigenmode expansion,"Eigenmode expansion (EME) is a computational electrodynamics modelling technique. !! Eigenmode expansion is a rigorous technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local eigenmodes that exists in the cross section of the device. !! Eigenmode expansion is a linear frequency-domain method. !! Unlike the beam propagation method, which is only valid under the slowly varying envelope approximation, eigenmode expansion provides a rigorous solution to Maxwell's equations."
data encryption standard,RFC4772 : Security Implications of Using the Data Encryption Standard (DES) !! The Data Encryption Standard (DES ) is a symmetric-key algorithm for the encryption of digital data. !! In 1973 NBS solicited private industry for a data encryption standard (DES).
continuous unimodal function,"Successive parabolic interpolation is a technique for finding the extremum (minimum or maximum) of a continuous unimodal function by successively fitting parabolas (polynomials of degree two) to a function of one variable at three unique points or, in general, a function of n variables at 1+n(n+3)/2 points, and at each iteration replacing the ""oldest"" point with the extremum of the fitted parabola."
successive parabolic interpolation,"Successive parabolic interpolation is a technique for finding the extremum (minimum or maximum) of a continuous unimodal function by successively fitting parabolas (polynomials of degree two) to a function of one variable at three unique points or, in general, a function of n variables at 1+n(n+3)/2 points, and at each iteration replacing the ""oldest"" point with the extremum of the fitted parabola. !! Moreover, not requiring the computation or approximation of function derivatives makes successive parabolic interpolation a popular alternative to other methods that do require them (such as gradient descent and Newton's method)."
binary inputs,"A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
boolean function,"In Boolean logic, the majority function (also called the median operator) is the Boolean function that evaluates to false when half or more arguments are false and true otherwise, i. e. the value of the function equals the value of the majority of the inputs. !! A Boolean network consists of a discrete set of boolean variables each of which has a Boolean function (possibly different for each variable) assigned to it which takes inputs from a subset of those variables and output that determines the state of the variable it is assigned to. !! Minimizing (or, equivalently, maximizing) a pseudo-Boolean function is NP-hard. !! The degree of the pseudo-Boolean function is simply the degree of the polynomial in this representation. !! In the mathematical field of combinatorics, a bent function is a special type of Boolean function which is maximally non-linear; it is as different as possible from the set of all linear and affine functions when measured by Hamming distance between truth tables. !! This can easily be seen by formulating, for example, the maximum cut problem as maximizing a pseudo-Boolean function. !! In computer science, a binary decision diagram (BDD) or branching program is a data structure that is used to represent a Boolean function. !! A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
logic gate,"Logic gates are primarily implemented using diodes or transistors acting as electronic switches, but can also be constructed using vacuum tubes, electromagnetic relays (relay logic), fluidic logic, pneumatic logic, optics, molecules, or even mechanical elements. !! Depending on the context, the term may refer to an ideal logic gate, one that has for instance zero rise time and unlimited fan-out, or it may refer to a non-ideal physical device (see Ideal and real op-amps for comparison). !! With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic. !! Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates. !! A logic gate is an idealized model of computation or a physical electronic device implementing a Boolean function, a logical operation performed on one or more binary inputs that produces a single binary output."
boolean functions,"With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic. !! This is an important class of pseudo-boolean functions, because they can be minimized in polynomial time. !! A truth table is a mathematical table used in logicspecifically in connection with Boolean algebra, boolean functions, and propositional calculuswhich sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables."
boolean logic,"In Boolean logic, the majority function (also called the median operator) is the Boolean function that evaluates to false when half or more arguments are false and true otherwise, i. e. the value of the function equals the value of the majority of the inputs. !! With amplification, logic gates can be cascaded in the same way that Boolean functions can be composed, allowing the construction of a physical model of all of Boolean logic, and therefore, all of the algorithms and mathematics that can be described with Boolean logic."
compound logic gates,Compound logic gates AND-OR-Invert (AOI) and OR-AND-Invert (OAI) are often employed in circuit design because their construction using MOSFETs is simpler and more efficient than the sum of the individual gates.
performance analyzer,Performance Analyzer is available as part of Oracle Developer Studio. !! Performance Analyzer is a commercial utility software for software performance analysis for x86 or SPARC machines.
software performance analysis,Performance Analyzer is a commercial utility software for software performance analysis for x86 or SPARC machines.
oracle developer studio,Performance Analyzer is available as part of Oracle Developer Studio.
n log n comparisons,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
input sequences,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
comparison sorting algorithms,"Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted)."
sorting algorithms,"Binary search trees are also efficacious in sorting algorithms and search algorithms. !! From a computational-complexity standpoint, priority queues are congruent to sorting algorithms. !! Among the authors of early sorting algorithms around 1951 was Betty Holberton, who worked on ENIAC and UNIVAC. !! For typical serial sorting algorithms, good behavior is O(n log n), with parallel sort in O(log2 n), and bad behavior is O(n2). !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Comparison sorting algorithms have a fundamental requirement of (n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted). !! In particular, some sorting algorithms are ""in-place""."
randomized algorithms,"Reservoir sampling is a family of randomized algorithms for choosing a simple random sample, without replacement, of k items from a population of unknown size n in a single pass over the items. !! In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator. !! The study of randomized algorithms was spurred by the 1977 discovery of a randomized primality test (i. e. , determining the primality of a number) by Robert M. Solovay and Volker Strassen. !! Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds. !! Computational complexity theory models randomized algorithms as probabilistic Turing machines. !! Randomized algorithms are particularly useful when faced with a malicious ""adversary"" or attacker who deliberately tries to feed a bad input to the algorithm (see worst-case complexity and competitive analysis (online algorithm)) such as in the Prisoner's dilemma. !! This class acts as the randomized equivalent of P, i. e. BPP represents the class of efficient randomized algorithms."
timespace tradeoffs,"Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, timespace tradeoffs, and upper and lower bounds."
information security,"In the fields of physical security and information security, access control (AC) is the selective restriction of access to a place or other resource, while access management describes the process. !! Web application security is a branch of information security that deals specifically with the security of websites, web applications and web services."
significant digits,"The term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can ""float""; that is, it can be placed anywhere relative to the significant digits of the number."
logarithm function,"The value distribution is similar to floating point, but the value-to-representation curve (i. e. , the graph of the logarithm function) is smooth (except at 0)."
binary floating point,"Some simple rational numbers (e. g. , 1/3 and 1/10) cannot be represented exactly in binary floating point, no matter what the precision is."
time delay neural network,"Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function. !! The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! Time delay neural network (TDNN) is a multilayer artificial neural network architecture whose purpose is to 1) classify patterns with shift-invariance, and 2) model context at each layer of the network."
feedforward neural network,"The Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. !! A probabilistic neural network (PNN) is a feedforward neural network, which is widely used in classification and pattern recognition problems."
neural network toolbox,Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function.
time delays,Matlab: The neural network toolbox has explicit functionality designed to produce a time delay neural network give the step size of time delays and an optional training function.
automata theory,"Automata theory is closely related to formal language theory. !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! Automata theory was initially considered a branch of mathematical systems theory, studying the behavior of discrete-parameter systems. !! With the publication of this volume, ""automata theory emerged as a relatively autonomous discipline"". !! Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification. !! To convert a grammar to Chomsky normal form, a sequence of simple transformations is applied in a certain order; this is described in most textbooks on automata theory. !! Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems."
abstract machines,"Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. !! Abstract machines can also be used to model abstract data types, which can be specified in terms of their operational semantics on an abstract machine. !! In the theory of computation, abstract machines are often used in thought experiments regarding computability or to analyze the complexity of algorithms. !! Through the use of abstract machines, it is possible to compute the amount of resources (time, memory, etc. ) !! This application of abstract machines is related to the subject of computational complexity theory. !! Undecidable problems can be related to different topics, such as logic, abstract machines or topology. !! More complex definitions create abstract machines with full instruction sets, registers and models of memory."
formal language theory,"In formal language theory, deterministic context-free languages (DCFL) are a proper subset of context-free languages. !! Automata theory is closely related to formal language theory. !! In formal language theory, computer science and linguistics, the Chomsky hierarchy (also referred to as the ChomskySchtzenberger hierarchy) is a containment hierarchy of classes of formal grammars."
differential calculus,Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems.
sparse graph,"More precisely, it follows from a result of Nash-Williams (1964) that the graphs of arboricity at most a are exactly the (a,a)-sparse graphs. !! The opposite, a graph with only a few edges, is a sparse graph. !! Pseudoforests are exactly the (1,0)-sparse graphs, and the Laman graphs arising in rigidity theory are exactly the (2,3)-tight graphs. !! A Sparse graph code is a code which is represented by a sparse graph. !! However, not every (3,6)-sparse graph is planar. !! Thus trees are exactly the (1,1)-tight graphs, forests are exactly the (1,1)-sparse graphs, and graphs with arboricity k are exactly the (k,k)-sparse graphs."
thus trees,"Thus trees are exactly the (1,1)-tight graphs, forests are exactly the (1,1)-sparse graphs, and graphs with arboricity k are exactly the (k,k)-sparse graphs."
cognitive dimensions,"Cognitive dimensions or cognitive dimensions of notations are design principles for notations, user interfaces and programming languages, described by researcher Thomas R. G. Green and furthered researched with Marian Petre."
binary space partitioning,"Binary space partitioning is a generic process of recursively dividing a scene into two until the partitioning satisfies one or more requirements. !! A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming. !! Binary space partitioning was developed in the context of 3D computer graphics in 1969. !! Binary space partitioning arose from the computer graphics need to rapidly draw three-dimensional scenes composed of polygons. !! In computer science, binary space partitioning (BSP) is a method for recursively subdividing a space into two convex sets by using hyperplanes as partitions."
3d computer graphics,"Binary space partitioning was developed in the context of 3D computer graphics in 1969. !! In 3D computer graphics, ray tracing is a technique for modeling light transport for use in a wide variety of rendering algorithms for generating digital images."
bsp tree,A disadvantage of binary space partitioning is that generating a BSP tree can be time-consuming.
humancomputer interaction,"Interruption science is a branch of human factors psychology and emerged from humancomputer interaction and cognitive psychology. !! In the industrial design field of humancomputer interaction, a user interface (UI) is the space where interactions between humans and machines occur. !! Augmented cognition research generally focuses on tasks and environments where humancomputer interaction and interfaces already exist. !! The field of information visualization has emerged ""from research in humancomputer interaction, computer science, graphics, visual design, psychology, and business methods. !! Ubiquitous computing touches on distributed computing, mobile computing, location computing, mobile networking, sensor networks, humancomputer interaction, context-aware smart home technologies, and artificial intelligence. !! Implicit data collection is used in humancomputer interaction to gather data about the user in an implicit, non-invasive way. !! In humancomputer interaction, MPG stands for ""multi-touch, physics and gestures"", referencing a common method of interacting with computers and various electronic devices. !! Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search at SIGIR06 and Exploratory Search and HCI at CHI07 (in order to meet with the experts in humancomputer interaction). !! Common topics of interaction design include design, humancomputer interaction, and software development. !! In humancomputer interaction, an organic user interface (OUI) is defined as a user interface with a non-flat display. !! Mobile interaction is an aspect of humancomputer interaction that emerged when computers became small enough to enable mobile usage, around the 1990s. !! Proper design of error messages is an important topic in usability and other fields of humancomputer interaction. !! Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics."
predictive model,Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics.
fitts's law,"Fitts's law (often cited as Fitts' law) is a predictive model of human movement primarily used in humancomputer interaction and ergonomics. !! Fitts's law is used to model the act of pointing, either by physically touching an object with a hand or finger, or virtually, by pointing to an object on a computer monitor using a pointing device. !! Both parameters show the linear dependency in Fitts's law. !! Fitts's law has been shown to apply under a variety of conditions; with many different limbs (hands, feet, the lower lip, head-mounted sights), manipulanda (input devices), physical environments (including underwater), and user populations (young, old, special educational needs, and drugged participants). !! The first humancomputer interface application of Fitts's law was by Card, English, and Burr, who used the index of performance (IP), interpreted as 1b, to compare performance of different input devices, with the mouse coming out on top compared to the joystick or directional movement keys."
linear dependency,Both parameters show the linear dependency in Fitts's law.
sigmoid curve,"A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve."
mathematical function,"An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result. !! A minimax approximation algorithm (or L approximation or uniform approximation) is a method to find an approximation of a mathematical function that minimizes maximum error. !! A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve."
sigmoid function,"A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve. !! In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function. !! Other standard sigmoid functions are given in the Examples section. !! Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. !! Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams)."
standard sigmoid functions,Other standard sigmoid functions are given in the Examples section.
gompertz curve,Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams).
ogee curve,Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams).
sigmoid functions,"Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing."
real numbers,"Word embeddings can be obtained using a set of language modeling and feature learning techniques where words or phrases from the vocabulary are mapped to vectors of real numbers. !! Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing."
analog-to-digital converter,"Since the values are added together, the dithering produces results that are more exact than the LSB of the analog-to-digital converter. !! People often produce music on computers using an analog recording and therefore need analog-to-digital converters to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files. !! A Time-stretch analog-to-digital converter (TS-ADC) digitizes a very wide bandwidth analog signal, that cannot be digitized by a conventional electronic ADC, by time-stretching the signal prior to digitization. !! Analog-to-digital converters are integral to 2000s era music reproduction technology and digital audio workstation-based sound recording. !! A digital filter system usually consists of an analog-to-digital converter (ADC) to sample the input signal, followed by a microprocessor and some peripheral components such as memory to store data and filter coefficients etc. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
analog signal,"In electronics, a digital-to-analog converter (DAC, D/A, D2A, or D-to-A) is a system that converts a digital signal into an analog signal. !! In electronics, an analog-to-digital converter (ADC, A/D, or A-to-D) is a system that converts an analog signal, such as a sound picked up by a microphone or light entering a digital camera, into a digital signal."
analog recording,People often produce music on computers using an analog recording and therefore need analog-to-digital converters to create the pulse-code modulation (PCM) data streams that go onto compact discs and digital music files.
subrecursive hierarchies,"How can noncomputable functions be classified into a hierarchy based on their level of noncomputabilityAlthough there is considerable overlap in terms of knowledge and methods, mathematical computability theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages."
memory debugger,"Many memory debuggers require applications to be recompiled with special dynamic memory allocation libraries, whose APIs are mostly compatible with conventional dynamic memory allocation libraries, or else use dynamic linking. !! Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections. !! Memory debuggers work by monitoring memory access, allocations, and deallocation of memory. !! A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows. !! Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
memory leaks,"A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows. !! In contrast to memory leaks, where the leaked memory is never released, the memory consumed by a space leak is released, but later than expected. !! The memory management system must track outstanding allocations to ensure that they do not overlap and that no memory is ever ""lost"" (i. e. that there are no ""memory leaks"")."
buffer overflows,A memory debugger is a debugger for finding software memory problems such as memory leaks and buffer overflows.
managed code,"Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections."
garbage collection,"Garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance. !! Resources other than memory, such as network sockets, database handles, user interaction windows, file and device descriptors, are not typically handled by garbage collection. !! Programs written in languages that have garbage collection, such as managed code, might also need memory debuggers, e. g. for memory leaks due to ""living"" references in collections. !! In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++. !! In computer science, garbage collection (GC) is a form of automatic memory management. !! Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp. !! Garbage collection relieves the programmer from performing manual memory management where the programmer specifies what objects to deallocate and return to the memory system and when to do so."
conventional dynamic memory allocation libraries,"Many memory debuggers require applications to be recompiled with special dynamic memory allocation libraries, whose APIs are mostly compatible with conventional dynamic memory allocation libraries, or else use dynamic linking."
virtual machine,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
memory debuggers,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
special memory allocation libraries,"Some memory debuggers (e. g. Valgrind) work by running the executable in a virtual machine-like environment, monitoring memory access, allocation and deallocation so that no recompilation with special memory allocation libraries is required."
shift cipher,"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques."
caesar cipher,"The Caesar cipher is named after Julius Caesar, who, according to Suetonius, used it with a shift of three (A becoming D when encrypting, and D becoming A when decrypting) to protect messages of military significance. !! In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques. !! The encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenre cipher, and still has modern application in the ROT13 system. !! As with all single-alphabet substitution ciphers, the Caesar cipher is easily broken and in modern practice offers essentially no communications security. !! It is unknown how effective the Caesar cipher was at the time, but it is likely to have been reasonably secure, not least because most of Caesar's enemies would have been illiterate and others would have assumed that the messages were written in an unknown foreign language."
caesar shift,"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques."
vigenre cipher,"The encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenre cipher, and still has modern application in the ROT13 system."
computational materials science,"Computational materials science and engineering uses modeling, simulation, theory, and informatics to understand materials. !! Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics. !! The Gordon Research Conference on Computational Materials Science and Engineering began in 2020. !! One notable sub-field of computational materials science is integrated computational materials engineering (ICME), which seeks to use computational results and methods in conjunction with experiments, with a focus on industrial and commercial application. !! Those dedicated to the field include Computational Materials Science, Modelling and Simulation in Materials Science and Engineering, and npj Computational Materials."
integrated computational materials engineering,"One notable sub-field of computational materials science is integrated computational materials engineering (ICME), which seeks to use computational results and methods in conjunction with experiments, with a focus on industrial and commercial application."
computational engineering,"Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics."
computational physics,"Computational materials science is one sub-discipline of both computational science and computational engineering, containing significant overlap with computational chemistry and computational physics. !! Collision detection is a classic issue of computational geometry and has applications in various computing fields, primarily in computer graphics, computer games, computer simulations, robotics and computational physics. !! Lattice models are also ideal for study by the methods of computational physics, as the discretization of any continuum model automatically turns it into a lattice model."
machine consciousness,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
synthetic consciousness,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
artificial consciousness,"Artificial consciousness concepts are also pondered in the philosophy of artificial intelligence through questions about mind, consciousness, and mental states. !! Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics. !! In his article ""Artificial Consciousness: Utopia or Real Possibility,"" Giorgio Buttazzo says that a common objection to artificial consciousness is that ""Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can no longer be reprogrammed, from rethinking), emotions, or free will. !! As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. !! Artificial Intelligence System (AIS) was a distributed computing project undertaken by Intelligence Realm, Inc. with the long-term goal of simulating the human brain in real time, complete with artificial consciousness and artificial general intelligence. !! The aim of the theory of artificial consciousness is to ""Define that which would have to be synthesized were consciousness to be found in an engineered artifact"" (Aleksander 1995)."
cognitive robotics,"Artificial consciousness (AC), also known as machine consciousness (MC) or synthetic consciousness (Gamez 2008; Reggia 2013), is a field related to artificial intelligence and cognitive robotics."
boolean operators,"Source code that does bit manipulation makes use of the bitwise operations: AND, OR, XOR, NOT, and possibly other operations analogous to the boolean operators; there are also bit shifts and operations to count ones and zeros, find high and low one or zero, set, reset and test bits, extract and insert fields, mask and zero fields, gather and scatter bits to and from specified bit positions or fields. !! Short-circuit evaluation, minimal evaluation, or McCarthy evaluation (after John McCarthy) is the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true."
conditional expression,"In any programming language that implements short-circuit evaluation, the expression x and y is equivalent to the conditional expression if x then y else x, and the expression x or y is equivalent to if x then x else y."
ensemble learning method,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random forests,"Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration. !! The idea of random subspace selection from Ho was also influential in the design of random forests. !! :587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! With respect to other advanced machine learning approaches, such as artificial neural networks, random forests, or genetic programming, learning classifier systems are particularly well suited to problems that require interpretable solutions. !! An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc. ). !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random decision forests,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
decision trees,"Decision tree learning or induction of decision trees is one of the predictive modelling approaches used in statistics, data mining and machine learning. !! Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
training time,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
random forest,"Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration. !! :587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! For classification tasks, the output of the random forest is the class selected by most trees. !! An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc. ). !! Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time."
classification tasks,"For classification tasks, the output of the random forest is the class selected by most trees. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others."
gradient boosted trees,":587588 Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. !! Another useful regularization techniques for gradient boosted trees is to penalize model complexity of the learned model."
automatic memory management,"In computer science, garbage collection (GC) is a form of automatic memory management. !! Memory management within an address space is generally categorized as either manual memory management or automatic memory management. !! The Garbage Collection Handbook: The Art of Automatic Memory Management."
normal discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events."
discriminant function analysis,"In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type. !! Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. !! Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership."
linear discriminant analysis,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events."
comparison sort,"A comparison sort must have an average-case lower bound of (n log n) comparison operations, which is known as linearithmic time. !! Counting sort is not a comparison sort; it uses key values as indexes into an array and the (n log n) lower bound for comparison sorting will not apply. !! A metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. !! Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized). !! There are fundamental limits on the performance of comparison sorts. !! A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a ""less than or equal to"" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list."
comparison sorts,"Comparison sorts may run faster on some lists; many adaptive sorts such as insertion sort run in O(n) time on an already-sorted or nearly-sorted list. !! A metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. !! Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized). !! There are fundamental limits on the performance of comparison sorts. !! Despite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted."
linearithmic time,"A comparison sort must have an average-case lower bound of (n log n) comparison operations, which is known as linearithmic time."
lookup table,"In computer science, a lookup table (LUT) is an array that replaces runtime computation with a simpler array indexing operation. !! This is often beneficial, since succinct data structures find their uses in large data sets, in which case cache misses become much more frequent and the chances of the lookup table being evicted from closer CPU caches becomes higher. !! Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions. !! Perfect hash functions may be used to implement a lookup table with constant worst-case access time. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! It made sense to reduce expensive read operations by a form of manual caching by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items. !! FPGAs also make extensive use of reconfigurable, hardware-implemented, lookup tables to provide programmable hardware functionality."
lookup tables,"Binary search trees allow binary search for fast lookup, addition, and removal of data items, and can be used to implement dynamic sets and lookup tables. !! Lookup tables are also used extensively to validate input values by matching against a list of valid (or invalid) items in an array and, in some programming languages, may include pointer functions (or offsets to labels) to process the matching input. !! FPGAs also make extensive use of reconfigurable, hardware-implemented, lookup tables to provide programmable hardware functionality. !! Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
complex functions,"Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
statistical density functions,"Before the advent of computers, lookup tables of values were used to speed up hand calculations of complex functions, such as in trigonometry, logarithms, and statistical density functions."
dynamic prefetched arrays,It made sense to reduce expensive read operations by a form of manual caching by creating either static lookup tables (embedded in the program) or dynamic prefetched arrays to contain only the most commonly occurring data items.
generic programming,"Generic programming centers around the idea of abstracting from concrete, efficient algorithms to obtain generic algorithms that can be combined with different data representations to produce a wide variety of useful software. !! The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra. !! Generic programming is a style of computer programming in which algorithms are written in terms of types to-be-specified-later that are then instantiated when needed for specific types provided as parameters. !! The term ""generic programming"" was originally coined by David Musser and Alexander Stepanov in a more specific sense than the above, to describe a programming paradigm whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, with generic functions implemented in terms of these concepts, typically using language genericity mechanisms as described above. !! However, in the generic programming approach, each data structure returns a model of an iterator concept (a simple value type that can be dereferenced to retrieve the current value, or changed to point to another value in the sequence) and each algorithm is instead written generically with arguments of such iterators, e. g. a pair of iterators pointing to the beginning and end of the subsequence or range to process."
algebraic theories,"The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra."
abstract algebra,"The ""generic programming"" paradigm is an approach to software decomposition whereby fundamental requirements on types are abstracted from across concrete examples of algorithms and data structures and formalized as concepts, analogously to the abstraction of algebraic theories in abstract algebra."
data visualization,"To communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools. !! Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e. g. , points, lines, or bars) contained in graphics. !! Data visualization (often abbreviated data viz) is an interdisciplinary field that deals with the graphic representation of data. !! Data visualization has its roots in the field of statistics and is therefore generally considered a branch of descriptive Statistics. !! Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses. !! Since the graphic design of the mapping can adversely affect the readability of a chart, mapping is a core competency of Data visualization."
graphic representation,Data visualization (often abbreviated data viz) is an interdisciplinary field that deals with the graphic representation of data.
descriptive statistics,"Data visualization has its roots in the field of statistics and is therefore generally considered a branch of descriptive Statistics. !! In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA)."
information graphics,"To communicate information clearly and efficiently, data visualization uses statistical graphics, plots, information graphics and other tools."
visual objects,"Data visualization refers to the techniques used to communicate data or information by encoding it as visual objects (e. g. , points, lines, or bars) contained in graphics."
graph minor theory,"In this variation of graph minor theory, a graph is always simplified after any edge contraction to eliminate its self-loops and multiple edges."
edge contraction,"In this variation of graph minor theory, a graph is always simplified after any edge contraction to eliminate its self-loops and multiple edges."
rooted tree,"A rooted tree itself has been defined by some authors as a directed graph. !! The various kinds of data structures referred to as trees in computer science have underlying graphs that are trees in graph theory, although such data structures are generally rooted trees. !! A rooted forest may be directed, called a directed rooted forest, either making all its edges point away from the root in each rooted treein which case it is called a branching or out-forestor making all its edges point towards the root in each rooted treein which case it is called an anti-branching or in-forest. !! A rooted tree may be directed, called a directed rooted tree, either making all its edges point away from the rootin which case it is called an arborescence or out-treeor making all its edges point towards the rootin which case it is called an anti-arborescence or in-tree. !! A rooted forest is a disjoint union of rooted trees. !! An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex."
plane tree,An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex.
ordered tree,"Conversely, given an ordered tree, and conventionally drawing the root at the top, then the child vertices in an ordered tree can be drawn left-to-right, yielding an essentially unique planar embedding. !! An ordered tree (or plane tree) is a rooted tree in which an ordering is specified for the children of each vertex."
child vertices,"Conversely, given an ordered tree, and conventionally drawing the root at the top, then the child vertices in an ordered tree can be drawn left-to-right, yielding an essentially unique planar embedding."
serial manipulator,The main advantage of a serial manipulator is a large workspace with respect to the size of the robot and the floor space it occupies. !! For example when a serial manipulator is fully extended it is in what is known as the boundary singularity. !! A singularity is a configuration of a serial manipulator in which the joint parameters no longer completely define the position and orientation of the end-effector. !! Serial manipulators are the most common industrial robots and they are designed as a series of links connected by motor-actuated joints that extend from a base to an end-effector.
serial manipulators,Serial manipulators are the most common industrial robots and they are designed as a series of links connected by motor-actuated joints that extend from a base to an end-effector.
boundary singularity,For example when a serial manipulator is fully extended it is in what is known as the boundary singularity.
javascript computer programming,Lazy inheritance is a design pattern used in JavaScript computer programming.
server message block,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network. !! Server Message Block (SMB) is a communication protocol that Microsoft created for providing shared access to files and printers across nodes on a network. !! [MS-SMB]: Server Message Block (SMB) Protocol. !! Specifies the Server Message Block (SMB) Protocol, which defines extensions to the existing Common Internet File System (CIFS) specification that have been implemented by Microsoft since the publication of the CIFS specification. !! [MS-SMB2]: Server Message Block (SMB) Protocol Versions 2 and 3."
network browsing,"Server Message Block (SMB) enables file sharing, printer sharing, network browsing, and inter-process communication (through named pipes) over a computer network."
decision boundaries,"In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes."
smoothness assumption,"This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes."
clustering algorithms,"This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms. !! As listed above, clustering algorithms can be categorized based on their cluster model. !! Basic mean shift clustering algorithms maintain a set of data points the same size as the input data set. !! ELKI contains k-means (with Lloyd and MacQueen iteration, along with different initializations such as k-means++ initialization) and various more advanced clustering algorithms. !! The following overview will only list the most prominent examples of clustering algorithms, as there are possibly over 100 published clustering algorithms."
generative adversarial networks,"Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy. !! Synthetic media as a field has grown rapidly since the creation of generative adversarial networks, primarily through the rise of deepfakes as well as music synthesis, text generation, human image synthesis, speech synthesis, and more. !! A Style-Based Generator Architecture for Generative Adversarial Networks. !! Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs."
generative models,"They don't necessarily perform better than generative models at classification and regression tasks. !! With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. !! Recently, there has been a trend to build very large deep generative models. !! Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs."
regression tasks,They don't necessarily perform better than generative models at classification and regression tasks.
deep neural networks,"With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks."
Gradient boosting,"The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others. !! Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. !! (This section follows the exposition of gradient boosting by Li. )"
gradient boosting,"The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! Gradient boosting is a machine learning technique used in regression and classification tasks, among others. !! Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean. !! (This section follows the exposition of gradient boosting by Li. )"
optimization algorithm,"What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples. !! The idea of gradient boosting originated in the observation by Leo Breiman that boosting can be interpreted as an optimization algorithm on a suitable cost function. !! Many optimization algorithms need to start from a feasible point. !! In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."
explicit regression gradient boosting algorithms,"Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean."
boosting methods,"Like other boosting methods, gradient boosting combines weak ""learners"" into a single strong learner in an iterative fashion. !! A gradient-boosted trees model is built in a stage-wise fashion as in other boosting methods, but it generalizes the other methods by allowing optimization of an arbitrary differentiable loss function."
bio-inspired computing,"Within computer science, bio-inspired computing relates to artificial intelligence and machine learning. !! Bio-inspired computing uses an evolutionary approach, while traditional A. I. uses a 'creationist' approach. !! Bio-inspired computing, short for biologically inspired computing, is a field of study which seeks to solve computer science problems using models of biology. !! Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning. !! Bio-inspired computing is a major subset of natural computation."
biologically inspired computing,"Bio-inspired computing, short for biologically inspired computing, is a field of study which seeks to solve computer science problems using models of biology."
natural computation,Bio-inspired computing is a major subset of natural computation.
computer learning,Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning.
traditional artificial intelligence,Behavior-based robotics sets itself apart from traditional artificial intelligence by using biological systems as a model. !! Bio-Inspired computing can be distinguished from traditional artificial intelligence by its approach to computer learning.
bit slicing,"Bit slicing, although not called that at the time, was also used in computers before large-scale integrated circuits (LSI, the predecessor to today's VLSI, or very-large-scale integration circuits). !! Bit slicing more or less died out due to the advent of the microprocessor. !! The main advantage was that bit slicing made it economically possible in smaller processors to use bipolar transistors, which switch much faster than NMOS or CMOS transistors. !! In more recent times, the term bit slicing was reused by Matthew Kwan to refer to the technique of using a general-purpose CPU to implement multiple parallel simple virtual machines using general logic instructions to perform single-instruction multiple-data (SIMD) operations. !! Bit slicing is a technique for constructing a processor from modules of processors of smaller bit width, for the purpose of increasing the word length; in theory to make an arbitrary n-bit central processing unit (CPU)."
database tuning,Database tuning describes a group of activities used to optimize and homogenize the performance of a database. !! Database tuning aims to maximize use of system resources to perform work as efficiently and rapidly as possible.
markov random field,"The underlying graph of a Markov random field may be finite or infinite. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. !! In other words, a random field is said to be a Markov random field if it satisfies Markov properties. !! The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model."
undirected graph,"Given an undirected graph with non-negative edge weights and a subset of vertices, usually referred to as terminals, the Steiner tree problem in graphs requires a tree of minimum weight that contains all terminals (but may include additional vertices). !! In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. !! It differs from an ordinary or undirected graph, in that the latter is defined in terms of unordered pairs of vertices, which are usually called edges, links or lines."
undirected graphical model,"In the domain of physics and probability, a Markov random field (MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph."
image processing,"Digital image processing is the use of a digital computer to process digital images through an algorithm. !! Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. !! In image processing, line detection is an algorithm that takes a collection of n edge points and finds all the lines on which these edge points lie. !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. !! Automated sign language translationGesture recognition can be conducted with techniques from computer vision and image processing. !! Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. !! The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased. !! In the domain of artificial intelligence, a Markov random field is used to model various low- to mid-level tasks in image processing and computer vision. !! Mathematical Morphology and Its Applications to Image Processing, J. Serra and P. Soille (Eds. !! Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems."
world wide web,"Web mining is the application of data mining techniques to discover patterns from the World Wide Web. !! Google Images (previously Google Image Search) is a search engine owned by Google that allows users to search the World Wide Web for images. !! Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications. !! In contrast, the World Wide Web is a global collection of documents and other resources, linked by hyperlinks and URIs. !! Tim Berners-Lee founded the World Wide Web Consortium (W3C) which created XML in 1996 and recommended replacing HTML with stricter XHTML. !! A visual search engine is a search engine designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. !! The World Wide Web (WWW), commonly known as the Web, was originally a hypertext document management system accessed over the Internet. !! The terms Internet and World Wide Web are often used without much distinction. !! Web frameworks provide a standard way to build and deploy web applications on the World Wide Web. !! Viewing a web page on the World Wide Web normally begins either by typing the URL of the page into a web browser or by following a hyperlink to that page or resource. !! One particularly common use for certificate authorities is to sign certificates used in HTTPS, the secure browsing protocol for the World Wide Web."
image search tool,"Google's developers worked on developing this further, and they realized that an image search tool was required to answer ""the most popular search query"" they had seen to date: the green Versace dress of Jennifer Lopez worn in February 2000."
software verification method,Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution.
runtime error detection,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably. !! Buffer overflowsRuntime error detection tools can only detect errors in the executed control flow of the application. !! Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution."
software application,"The concept of reverse computation is somewhat simpler than reversible computing in that reverse computation is only required to restore the equivalent state of a software application, rather than support the reversibility of the set of all possible instructions. !! Reverse computation is a software application of the concept of reversible computing. !! Runtime error detection is a software verification method that analyzes a software application as it executes and reports defects that are detected during that execution."
application crashing,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably."
file overwrites,"Runtime error detection can identify defects that manifest themselves only at runtime (for example, file overwrites) and zeroing in on the root causes of the application crashing, running slowly, or behaving unpredictably."
buffer overflowsruntime error detection tools,Buffer overflowsRuntime error detection tools can only detect errors in the executed control flow of the application.
computational intelligence,"The expression computational intelligence (CI) usually refers to the ability of a computer to learn a specific task from data or experimental observation. !! Computational heuristic intelligence (CHI) refers to specialized programming techniques in computational intelligence (also called artificial intelligence, or AI). !! Except those main principles, currently popular approaches include biologically inspired algorithms such as swarm intelligence and artificial immune systems, which can be seen as a part of evolutionary computation, image processing, data mining, natural language processing, and artificial intelligence, which tends to be confused with Computational Intelligence. !! Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. !! Generally, computational intelligence is a set of nature-inspired computational methodologies and approaches to address complex real-world problems to which mathematical or traditional modelling can be useless for a few reasons: the processes might be too complex for mathematical reasoning, it might contain some uncertainties during the process, or the process might simply be stochastic in nature. !! Computational Intelligence therefore provides solutions for such problems."
soft computing,"Even though it is commonly considered a synonym of soft computing, there is still no commonly accepted definition of computational intelligence. !! A complete overview of FML and related applications can be found in the book titled On the power of Fuzzy Markup Language edited by Giovanni Acampora, Chang-Shing Lee, Vincenzo Loia and Mei-Hui Wang, and published by Springer in the series Studies on Fuzziness and Soft Computing."
associative classifier,An associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value.
supervised learning model,An associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value.
graphics system,"Primitives are basic units which a graphics system may combine to create more complex images or models. !! In the field of realistic rendering, Japan's Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. !! The 3D Core Graphics System (or Core) was the first graphical standard to be developed."
cholesky decomposition,"The conjugate gradient method is often implemented as an iterative algorithm, applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. !! For this reason, the LDL decomposition is often called the square-root-free Cholesky decomposition. !! where L is a lower triangular matrix with real and positive diagonal entries, and L* denotes the conjugate transpose of L. Every Hermitian positive-definite matrix (and thus also every real-valued symmetric positive-definite matrix) has a unique Cholesky decomposition. !! Some indefinite matrices for which no Cholesky decomposition exists have an LDL decomposition with negative entries in D: it suffices that the first n1 leading principal minors of A are non-singular. !! When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations. !! A frontal solver builds a LU or Cholesky decomposition of a sparse matrix given as the assembly of element matrices by assembling the matrix and eliminating equations only on a subset of elements at a time. !! In numerical analysis the minimum degree algorithm is an algorithm used to permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, to reduce the number of non-zeros in the Cholesky factor. !! In linear algebra, the Cholesky decomposition or Cholesky factorization (pronounced sh-LES-kee) is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful for efficient numerical solutions, e. g. , Monte Carlo simulations."
multifrontal solver,A multifrontal solver of Duff and Reid is an improvement of the frontal solver that uses several independent fronts at the same time.
fuzzy markup language,"A complete overview of FML and related applications can be found in the book titled On the power of Fuzzy Markup Language edited by Giovanni Acampora, Chang-Shing Lee, Vincenzo Loia and Mei-Hui Wang, and published by Springer in the series Studies on Fuzziness and Soft Computing. !! Fuzzy Markup Language (FML) is a specific purpose markup language based on XML, used for describing the structure and behavior of a fuzzy system independently of the hardware architecture devoted to host and run it. !! Diet assessment based on type-2 fuzzy ontology and fuzzy markup language."
point distribution model,"Point distribution models rely on landmark points. !! The point distribution model concept has been developed by Cootes, Taylor et al. !! The point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes."
c4 model,"C4 model documents the architecture of a software system, by showing multiple points of view that explain the decomposition of a system into containers and components, the relationship between these elements, and, where appropriate, the relation with its users. !! C4 model is a lean graphical notation technique for modelling the architecture of software systems. !! For level 1 to 3, the C4 model uses 5 basic diagramming elements: persons, software systems, containers, components and relationships. !! C4 model relies at this level on existing notations such as Unified Modelling Language (UML), Entity Relation Diagrams (ERD) or diagrams generated by Integrated Development Environments (IDE). !! C4 model was created by the software architect Simon Brown between 2006 and 2011 on the roots of Unified Modelling Language (UML) and the 4+1 architectural view model."
software systems,"Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. !! Software diagnosis (also: software diagnostics) refers to concepts, techniques, and tools that allow for obtaining findings, conclusions, and evaluations about software systems and their implementation, composition, behaviour, and evolution. !! C4 model is a lean graphical notation technique for modelling the architecture of software systems. !! Service-oriented modeling is the discipline of modeling business and software systems, for the purpose of designing and specifying service-oriented business systems within a variety of architectural styles and paradigms, such as application architecture, service-oriented architecture, microservices, and cloud computing. !! For level 1 to 3, the C4 model uses 5 basic diagramming elements: persons, software systems, containers, components and relationships. !! In the context of hardware and software systems, formal verification is the act of proving or disproving the correctness of intended algorithms underlying a system with respect to a certain formal specification or property, using formal methods of mathematics. !! End-User Development can be defined as a set of methods, techniques, and tools that allow users of software systems, who are acting as non-professional software developers, at some point to create, modify or extend a software artifact."
unified modelling language,"C4 model was created by the software architect Simon Brown between 2006 and 2011 on the roots of Unified Modelling Language (UML) and the 4+1 architectural view model. !! C4 model relies at this level on existing notations such as Unified Modelling Language (UML), Entity Relation Diagrams (ERD) or diagrams generated by Integrated Development Environments (IDE)."
software system,"A debugging pattern describes a generic set of steps to rectify or correct a bug within a software system. !! Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. !! In computer science, program optimization, code optimization, or software optimization is the process of modifying a software system to make some aspect of it work more efficiently or use fewer resources. !! C4 model documents the architecture of a software system, by showing multiple points of view that explain the decomposition of a system into containers and components, the relationship between these elements, and, where appropriate, the relation with its users. !! In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. !! A software metric is a standard of measure of a degree to which a software system or process possesses some property. !! In computing, the Distributed Computing Environment (DCE) software system was developed in the early 1990s from the work of the Open Software Foundation (OSF), a consortium (founded in 1988) that included Apollo Computer (part of Hewlett-Packard from 1989), IBM, Digital Equipment Corporation, and others."
memory management unit,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU. !! A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses. !! However, the TLB cache is part of the memory management unit (MMU) and not directly related to the CPU caches."
virtual memory addresses,"A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses."
computer hardware unit,"A memory management unit (MMU), sometimes called paged memory management unit (PMMU), is a computer hardware unit having all memory references passed through itself, primarily performing the translation of virtual memory addresses to physical addresses."
memory allocation,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU. !! For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it."
memory protection,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU."
memory sharing,"It includes the original Sun 1 memory management unit that provides address translation, memory protection, memory sharing and memory allocation for multiple processes running on the CPU."
programming language semantics,"In programming language semantics, normalisation by evaluation (NBE) is a style of obtaining the normal form of terms in the -calculus by appealing to their denotational semantics."
tridiagonal matrix algorithm,"The derivation of the tridiagonal matrix algorithm is a special case of Gaussian elimination. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! This can be done efficiently if both solutions are computed at once, as the forward portion of the pure tridiagonal matrix algorithm can be shared."
gaussian elimination,"The derivation of the tridiagonal matrix algorithm is a special case of Gaussian elimination. !! In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. !! LU decomposition can be viewed as the matrix form of Gaussian elimination."
systems theory,"Systems design could be seen as the application of systems theory to product development. !! Algorithmic art, also known as computer-generated art, is a subset of generative art (generated by an autonomous system) and is related to systems art (influenced by systems theory)."
sorted binary tree,"In computer science, a binary search tree (BST), also called an ordered or sorted binary tree, is a rooted binary tree data structure whose internal nodes each store a key greater than all the keys in the node's left subtree and less than those in its right subtree."
first-order reduction,"A first-order reduction is a reduction where each component is restricted to be in the class FO of problems calculable in first-order logic. !! Many important complexity classes are closed under first-order reductions, and many of the traditional complete problems are first-order complete as well (Immerman 1999 p. 49-50). !! , the first-order reductions are stronger reductions than the logspace reductions. !! In computer science, a first-order reduction is a very strong type of reduction between two computational problems in computational complexity theory."
logspace reductions,", the first-order reductions are stronger reductions than the logspace reductions."
deterministic encryption scheme,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
deterministic encryption,"While deterministic encryption schemes can never be semantically secure, they have some advantages over probabilistic schemes. !! Examples of deterministic encryption algorithms include RSA cryptosystem (without encryption padding), and many block ciphers when used in ECB mode or with a constant initialization vector. !! One primary motivation for the use of deterministic encryption is the efficient searching of encrypted data. !! Deterministic encryption can leak information to an eavesdropper, who may recognize known ciphertexts. !! A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
probabilistic encryption scheme,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm."
encryption algorithm,"A deterministic encryption scheme (as opposed to a probabilistic encryption scheme) is a cryptosystem which always produces the same ciphertext for a given plaintext and key, even over separate executions of the encryption algorithm. !! Dynamic Encryption is a cryptographic principle that enables two parties to change the encryption algorithm for every transaction."
ecb mode,"Examples of deterministic encryption algorithms include RSA cryptosystem (without encryption padding), and many block ciphers when used in ECB mode or with a constant initialization vector."
deterministic encryption schemes,"While deterministic encryption schemes can never be semantically secure, they have some advantages over probabilistic schemes."
encrypted data,One primary motivation for the use of deterministic encryption is the efficient searching of encrypted data.
visual programming languages,"Visual Programming Languages - Snapshots !! Some graphical parsing algorithms have been designed for visual programming languages. !! The following contains a list of notable visual programming languages. !! A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. !! Godot game engine allows game scripts and graphics shaders to be built using node-graph visual programming languages. !! languages of the Microsoft Visual Studio IDE are not visual programming languages: the representation of algorithms etc. !! Parsers for visual programming languages can be implemented using graph grammars."
graphical program elements,"A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output."
spectral methods,"Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. !! Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform. !! Spectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations."
fast fourier transform,"Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
spectral method,"In other words, spectral methods take on a global approach while finite element methods use a local approach. !! Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains. !! Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform. !! Spectral methods can be used to solve ordinary differential equations (ODEs), partial differential equations (PDEs) and eigenvalue problems involving differential equations."
applied mathematics,"Computational mathematics emerged as a distinct part of applied mathematics by the early 1950s. !! Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. !! Matrix multiplication is thus a basic tool of linear algebra, and as such has numerous applications in many areas of mathematics, as well as in applied mathematics, statistics, physics, economics, and engineering. !! Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes. !! Structural analysis employs the fields of applied mechanics, materials science and applied mathematics to compute a structure's deformations, internal forces, stresses, support reactions, accelerations, and stability. !! Polynomial sequences are a topic of interest in enumerative combinatorics and algebraic combinatorics, as well as applied mathematics. !! Spectral methods are a class of techniques used in applied mathematics and scientific computing to numerically solve certain differential equations, potentially involving the use of the fast Fourier transform."
finite element methods,"Spectral methods and finite element methods are closely related and built on the same ideas; the main difference between them is that spectral methods use basis functions that are nonzero over the whole domain, while finite element methods use basis functions that are nonzero only on small subdomains."
exponential convergence,"Partially for this reason, spectral methods have excellent error properties, with the so-called ""exponential convergence"" being the fastest possible, when the solution is smooth."
advanced computing environment,"The Advanced Computing Environment (ACE) was defined by an industry consortium in the early 1990s to be the next generation commodity computing platform, the successor to personal computers based on Intel's 32-bit instruction set architecture."
affective computing,"Using affective computing technology, computers can judge the learners' affection and learning state by recognizing their facial expressions. !! While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. !! Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. !! Affectiva is a company (co-founded by Rosalind Picard and Rana El Kaliouby) directly related to affective computing and aims at investigating solutions and software for facial affect detection. !! Artificial imagination research uses tools and insights from many fields, including computer science, rhetoric, psychology, creative arts, philosophy, neuroscience, affective computing, Artificial Intelligence, cognitive science, linguistics, operations research, creative writing, probability and logic. !! Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects."
gradient boosting algorithms,"Explicit regression gradient boosting algorithms were subsequently developed, by Jerome H. Friedman, simultaneously with the more general functional gradient boosting perspective of Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean."
artificial intelligent agents,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
computational ethics,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
machine ethics,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents. !! Although the definition of ""Machine Ethics"" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI Magazine article ""A Question of Responsibility"":""However, one thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. !! Perhaps what we need is, in fact, a theory and practice of machine ethics, in the spirit of Asimovs three laws of robotics. "" !! Machine ethics should not be confused with computer ethics, which focuses on human use of computers. !! Machine ethics differs from other ethical fields related to engineering and technology."
computational morality,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents."
computer ethics,"Machine ethics should not be confused with computer ethics, which focuses on human use of computers."
intelligent machines,"Although the definition of ""Machine Ethics"" has evolved since, the term was coined by Mitchell Waldrop in the 1987 AI Magazine article ""A Question of Responsibility"":""However, one thing that is apparent from the above discussion is that intelligent machines will embody values, assumptions, and purposes, whether their programmers consciously intend them to or not. !! Oriented energy filters are used to grant sight to intelligent machines and sensors."
monte carlo algorithm,"Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer. !! While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! If there is a procedure for verifying whether the answer given by a Monte Carlo algorithm is correct, and the probability of a correct answer is bounded above zero, then with probability one running the algorithm repeatedly while testing the answers will eventually give a correct answer. !! Two examples of such algorithms are KargerStein algorithm and Monte Carlo algorithm for minimum Feedback arc set. !! In computing, a Monte Carlo algorithm is a randomized algorithm whose output may be incorrect with a certain (typically small) probability."
kargerstein algorithm,Two examples of such algorithms are KargerStein algorithm and Monte Carlo algorithm for minimum Feedback arc set.
las vegas algorithms,Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer.
monte carlo algorithms,"While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! Las Vegas algorithms are a dual of Monte Carlo algorithms that never return an incorrect answer."
deterministic algorithm,"While the answer returned by a deterministic algorithm is always expected to be correct, this is not the case for Monte Carlo algorithms. !! A few routing algorithms do not use a deterministic algorithm to find the best link for a packet to get from its original source to its final destination."
